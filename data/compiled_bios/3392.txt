 Toggle navigation Perceiving Systems Institute Home Autonomous Motion Empirical Inference Haptic Intelligence Modern Magnetic Systems Perceiving Systems Physical Intelligence Autonomous Vision Autonomous Learning Dynamic Locomotion Embodied Vision Intelligent Control Systems Micro, Nano, and Molecular Systems Movement Generation and Control Physics for Inference and Optimization Probabilistic Numerics Rationality Enhancement Statistical Learning Theory Research Overview Virtual Humans and Animals Seeing and Understanding People Scenes, Structure and Motion Behavior, Goals and Action Learning and Inference Beyond Mocap Human Health Datasets and Code Robot Perception Group Holistic Vision Group Completed Publications Code/Data People News & Events News Events Talks Awards Facilities 4D Scanner 4D Face Scanner 4D Foot Scanner 3D Scanner Vicon Motion Capture Inertial Motion Capture Outdoor Motion Capture System Video Capture Computing Espresso Career Why MPI Current Jobs Ph.D Applicants Undergraduate Interns Trials (Participants Wanted) Art (Artist in Residence) About Department Principles Startups Contact Michael Black Director Perceiving Systems Office: N3.019 Max-Planck-Ring 4 72076 Tbingen Germany +49 7071 601 1801 black@tue.mpg.de michael_j_black Find me on Linkedin Find me on Google Scholar Follow me on Twitter Biography Research Students PostDocs Data Code Affiliations Contact Talks Publications Curriculum Vitae [pdf] Conflict of Interest Disclosure This includes any coporate activity within the last 5 years involving more than $5000 where I have a personal or professional interest or any role in which I have corporate responsibility. Corporate research funding (unrestricted): Intel, NVIDIA, Adobe, Facebook, and Amazon. Financial interests (stock): Amazon, Meshcapade. Commercial licensing of MPI technology where I am a co-inventor. Side employment: Amazon (20%, current), Body Labs Inc (20%, 2013-2017). Corporate boards: Body Labs Inc. (2013-2017). Citations Google Scholar citations Research Gate Semantic Scholar DBLP Biography Michael J. Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. in computer science from Yale University (1992). After research at NASA Ames and post-doctoral research at the University of Toronto, he joined the Xerox Palo Alto Research Center in 1993 where he later managed the Image Understanding Area and founded the Digital Video Analysis group. From 2000 to 2010 he was on the faculty of Brown University in the Department of Computer Science (Assoc. Prof. 2000-2004, Prof. 2004-2010). He is a founding director at the Max Planck Institute for Intelligent Systems in Tbingen, Germany, where he leads the Perceiving Systems department. He is also a Distinguished Amazon Scholar,an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University. Black is a foreign member of the Royal Swedish Academy of Sciences. He is a recipient of the 2010 Koenderink Prize for Fundamental Contributions in Computer Vision and the 2013 Helmholtz Prize for work that has stood the test of time. His work has won several paper awards including the IEEE Computer Society Outstanding Paper Award (CVPR'91). His work received Honorable Mention for the Marr Prize in 1999 and 2005. His early work on optical flow has been widely used in Hollywood films including for the Academy-Award-winning effects in What Dreams May Come and The Matrix Reloaded. He has contributed to several influential datasets including the Middlebury Flow dataset , HumanEva , and the Sintel dataset . Black has coauthored over 200 peer-reviewed scientific publications. He is also active in commercializing scientific results, isan inventor on 10 issuedpatents, and has advised multiple startups. He uniquely combines computer vision, graphics, and machine learning to solve problems in the clothing industry.In 2013, heco-foundedBody Labs Inc., which used computer vision, machinelearning, and graphics technology licensed from his lab to commercialize "the body as a digital platform." Body Labs wasacquired by Amazonin 2017. Black's research interests in machine vision include optical flow estimation, 3D shape models, human shape and motion analysis, robust statistical methods, and probabilistic models of the visual world. In computational neuroscience his work focuses on probabilistic models of the neural code and applications of neural decoding in neural prosthetics. Short version Michael Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. from Yale University (1992). After post-doctoral research at the University of Toronto, he worked at Xerox PARC as a member of research staff and area manager. From 2000 to 2010 he was on the faculty of Brown University in the Department of Computer Science (Assoc. Prof. 2000-2004, Prof. 2004-2010). He is one of the founding directors at the Max Planck Institute for Intelligent Systems in Tbingen, Germany, where he leads the Perceiving Systems department.He is also a Distinguished Amazon Scholar,an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University. His work has won several awards including the IEEE Computer Society Outstanding Paper Award (1991), Honorable Mention for the Marr Prize (1999 and 2005), the 2010 Koenderink Prize for Fundamental Contributions in Computer Vision, and the 2013 Helmholtz Prize for work that has stood the test of time. He is a foreign member of the Royal Swedish Academy of Sciences. In 2013 heco-foundedBody Labs Inc., which was acquired by Amazonin 2017. Even shorter version Michael Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. from Yale University (1992). He has held positions at the University of Toronto, Xerox PARC, and Brown Unviversity. He is one of the founding directors at the Max Planck Institute for Intelligent Systems in Tbingen, Germany, where he leads the Perceiving Systems department. He is a Distinguished Amazon Scholar,an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University. His work has won several awards including the IEEE Computer Society Outstanding Paper Award (1991), Honorable Mention for the Marr Prize (1999 and 2005), the 2010 Koenderink Prize, and the 2013 Helmholtz Prize. He is a foreign member of the Royal Swedish Academy of Sciences. In 2013 heco-foundedBody Labs Inc., which was acquired by Amazonin 2017. Head shot [png] Education Yale University, New Haven, CT Ph.D., Computer Science, 1992. Stanford University, Stanford, CA M.S., Computer Science, 1989. The University of British Columbia, Vancouver, BC B.Sc., Honours Computer Science, 1985. Selected Awards/Honors Alumni Research Award University of British Columbia, Department of Computer Science, 2018. Royal Swedish Academy of Sciences Foreign member, Class for Engineering Sciences, since June 2015. 2013 Helmholtz Prize for the paper: Black, M. J., and Anandan, P., "A framework for the robust estimation of optical flow,''IEEE International Conference on Computer Vision, ICCV , pages 231-236, Berlin, Germany. May 1993. 2010 Koenderink Prize for Fundamental Contributions in Computer Vision, with Sidenbladh, H.and Fleet, D. J. for the paper"Stochastic tracking of 3D human figures using 2D image motion,''European Conference on Computer Vision, 2000. Best Paper Award, Eurographics 2017, for the paper"Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs", by von Marcard, T.,Rosenhahn, B., Black, M. J.,Pons-Moll, G. "Dataset Award" at the Eurographics Symposium on Geometry Processing 2016 , with F. Bogo, J. Romero, and M. Loper , for the paper "FAUST: Dataset and evaluation for 3Dmesh registration," CVPR 2014. Best Paper Award, International Conference on 3D Vision (3DV), 2015, with A.O.Ulusoy andA.Geiger , for the paper "Towards Probabilistic Volumetric Reconstruction using Ray Potentials." Best Paper Award, INI-Graphics Net, 2008, First Prize Winner of Category Research, with S. Roth for the paper "Steerable random fields." Best Paper Award, Fourth International Conference onArticulated Motion and Deformable Objects (AMDO-e 2006),with L. Sigal for the paper "Predicting 3D people from 2D pictures.'' Marr Prize, Honorable Mention , Int. Conf. on ComputerVision, ICCV-2005, Beijing, China, Oct. 2005with S. Roth for the paper "On the spatial statistics of optical flow.'' Marr Prize, Honorable Mention , Int. Conf. on ComputerVision, ICCV-99, Corfu, Greece, Sept. 1999with D. J. Fleet for the paper "Probabilistic detection and tracking of motion discontinuities.'' IEEE Computer Society, Outstanding Paper Award, Conference on Computer Vision and Pattern Recognition,Maui, Hawaii, June 1991with P. Anandan for the paper "Robust dynamic motion estimation over time.'' Commendation and Chief's Award , Henrico County Division of Police, County of Henrico, Virginia, April 19, 2007. University of Maryland, Invention of the Year, 1995 ,"Tracking and Recognizing Facial Expressions,''with Y. Yacoob. University of Toronto, Computer Science Students' Union Teaching Award for 1992-1993. Employment and Positions Held Max Planck Institute for Intelligent Systems Tbingen, Germany Director, 1/11 - present Managing Director, 2/13 - 6/15, 3/18 - 11/18 Amazon Tbingen, Germany Distinguished Amazon Scholar, 11/17 - present Eberhard Karls Universitt Tbingen, Faculty of Science, Department of Computer Science Tbingen, Germany Honorarprofessor, 05/22/12 - present B ody Labs Inc. New York, NY, USA Co-founder, Science Advisor, Member of the Board, 01/13- 10/2017 ETH Zrich, Dept. of Information Technology and Electrical Engineering Zrich, Switzerland Visting Professor, 04/2014 - 04/2016 Stanford University, Electrical Engineering Stanford, CA Visiting Professor, 5/11-4/12, 7/12-7/13 Brown University, Department of Computer Science, Providence, RI Adjunct Professor (Research), 1/11-present Professor, 7/04-12/10 Associate Professor, 7/00-6/04 My research addressedthe problem of estimating and explaining motion in image sequences. I developed methods detecting and tracking 2D and3D human motion including the introduction of particle filtering for 3D human tracking and belief propagation for 3D human pose estimation. I worked onprobabilistic models of images include thehigh-order Field of Experts model. I worked on 3D human shape estimation from images and video and developed applications of this technology. I also developed mathematical models for decoding neural signals. This included the first uses of particle filtering and Kalman filtering for decoding motor cortical neural activity and the first point-and-click cortical neural brain-machine-interface for people with paralysis. Xerox Palo Alto Research Center, Palo Alto, CA Area Manager, Image Understanding Area, 1/96-7/00 Member of Research Staff, 9/93-12/95 Research includedmodeling image changes (motion, illumination, specularity, occlusion, etc.) in video as a mixtureof causes. I developed methods ofmotion explanation; that is, the extraction of mid-level or high-level concepts from motion. This included the modeling and recognition of motion "features"(occlusion boundaries, moving bars, etc.), human facial expressions and gestures, and motion "texture"(plants, fire, water, etc.). I applied these methods to problems invideo indexing, motion for video annotation, teleconferencing, and gestural user interfaces. Other research includedrobust learning of image-based models, regularization with transparency, anisotropic diffusion, and the recovery of multiple shapes from transparent textures. University of Toronto, Toronto, Ontario Assistant Professor, Department of Computer Science, (8/92 - 9/93). Research included the application of mixture models to optical flow, detection and tracking of surface discontinuities using motion information, and robust surface recovery in dynamic environments. Yale University, (9/89-8/92) New Haven, CT Research Assistant, Department of Computer Science. Research in the recovery of optical flow, incremental estimation, temporal continuity, applications of robust statistics to optical flow, the relationship between robust statistics and line processes, the early detection of motion discontinuities, and the role of representation in computer vision. NASA Ames Research Center, (6/90-8/92) Moffett Field, CA Visiting Researcher, Aerospace Human Factors Research Division. Developed motion estimation algorithms in the context of an autonomous Mars landing and nap-of-the-earth helicopter flight and studied the psychophysical implications of a temporal continuity assumption. Advanced Decision Systems, (12/86-6/89) Mountain View, CA Computer Scientist, Image Understanding Group. Research on spatial reasoning for robotic vehicle route planning and terrain analysis. Vision research including perceptual grouping, object-based translational motion processing, the integration of vision and control for an autonomous vehicle, object modeling using generalized cylinders, and the development of an object-oriented vision environment. GTE Government Systems, (6/85-12/86) Mountain View, CA Engineer, Artificial Intelligence Group. Developed expert systems for multi-source data fusion and fault location. Miscellaneous, ('78-'85) Summer undergraduate researcher at UBC; park ranger's assistant; volunteer firefighter, busboy; and probably my worst job: cleaning dog kennels. Research Interests I am interested in motion. What does motion tell us about the structure of the world and how can we compute this from video? How do humans and animals move? How does the brain control complex movement? My work combines computervision, graphics and neuroscience to develop new models and algorithms to capture and analyze the motion of the world. My Computer Vision research addresses: the estimation of scene structure and physical properties from video; articulated human motion pose estimation and tracking; the estimation of human body shape from images and video; the representation and detection of motion discontinuities; the estimation of optical flow ; vision as inverse graphics. My Graphicsresearch addresses: virtual humans; next-generation motion capture; articulated and non-rigid shape representation; human and animal shape and motion capture; human animation, AR/VR; capture and animation of clothing. My Computational Neuroscience research addresses: modeling the neural control of reaching and grasping; novel neural decoding algorithms; neural prostheses and cortical brain-machine interfaces; markless animalmotion capture. I also work on industrial applications in Fashion Science : Body scanning and measurement; clothing sizing; cloth capture and modeling; virtual try-on. What is maybe unique about my work is the combination of the these themes. For example I study human motion from the inside (decoding neural activity in paralyzed humans) and the outside (with novel motion capture techniques). Current PhD students: O mid Taheri , MPI for Intelligent Systems, Tbingen Qianli Ma , Int. Max Planck Research School, Intelligent Systems, Tbingen Vassilis Choutas , MPI-ETH Center for Learning Systems, Co-supervised with Luc van Gool Ahmed Osman , Int. Max Planck Research School, Intelligent Systems, Tbingen Mohamed Hassan , MPI for Intelligent Systems, Tbingen Partha Ghosh , Int. Max Planck Research School, Intelligent Systems, Tbingen SoubhikSanyal , Int. Max Planck Research School, Intelligent Systems, Tbingen Daniel Cudeiro , MPI-ETH Center for Learning Systems, Co-supervised with Luc van Gool Nadine Regg , MPI-ETH Center for Learning Systems, Co-supervised with Konrad Schindler Eric Price , MPI for Intelligent Systems, Tbingen Yinghao Huang , MPI for Intelligent Systems, Tbingen Anurag Ranjan , MPI for Intelligent Systems, Tbingen Graduated PhD students: Jonas Wulff , Postdoctoral Researcher, MIT CSAIL Thesis: Model-based Optical Flow: Layers, Learning, and Geometry , University of Tbingen, April 2018 Matthew Loper , Amazon, Thesis: Human Shape Estimation using Statistical Body Models , University of Tbingen, May 2017 Silvia Zuffi , Research Scientist, IMATI-CNR, Institute for Applied Mathematics and Information Technologies, Milan Italy Thesis: Shape Models of the Human Body for Distributed Inference , Brown University, May 2015 Aggeliki Tsoli , Post doctoral researcher, FORTH Institute, Crete, Thesis: Modeling the Human body in 3D: Data Registration and Human Shape Representation , Department of Computer Science, Brown University, May 2014 Oren Freifeld , Assistant Professor, Dept. of Computer Science, Ben-Gurion Univ., Israel, Thesis: Statistics on Manifolds with Applications to Modeling Shape Deformations , Division of Applied Mathematics, Brown University, August 2013 Peng Guan , Senior Software Engineer, Google, Thesis: Virtual Human Bodies with Clothing and Hair: From Images to Animation , Department of Computer Science, Brown University, December 2012 Deqing Sun , Senior Research Scientist, NVIDIA Research, Thesis: From Pixels to Layers: Joint Motion Estimationand Segmentation , Department of Computer Science, Brown University, July 2012 Alexandru Balan , Xbox Incubation Researcher, Microsoft Thesis: Detailed Human Shape and Pose from Images , Department of Computer Science, Brown University, May 2010 Leonid Sigal, Associate Professor of Computer Science, Univ. of British Columbia (UBC) Thesis: Continuous-state graphical models for object localization, pose estimation and tracking Department of Computer Science, Brown University, May 2008 Stefan Roth , Professor, Dept. of Computer Science, TU Darmstadt Thesis: High-order Markov random fields for low-level vision . Dept. of Computer Science, Brown University, May 2007 Winner of the Joukowsky Family Foundation Outstanding Dissertation Award Frank Wood , Associate Professor of Computer Science, Univ. of British Columbia (UBC) Thesis: Nonparametric Bayesian modeling of neural data. Department of Computer Science, Brown University Hulya Yalcin , Assistant Professor, Department of Electronics and Communications Engineering, Istanbul Technical University, Turkey Thesis: Implicit models of moving and static surfaces , Division of Engineering, Brown University, May 2004 Wei Wu , Associate Professor, Dept. of Statistics, Florida State Thesis: Statistical models of neural coding in motor cortex, Division of Applied Math, Brown University. Co-supervised with David Mumford. May 2004. Fernando De la Torre , Research Associate Professor, CMU and Facebook, Thesis: Robust subspace learning for computer vision, La Salle School of Engineering. Universitat Ramon Llull, Barcelona, Spain. Jan. 2002 Hedvig Kjellstrom (nee Sidenbladh) , Professorof Comptuer Science, KTH, Sweden Thesis: Probabilistic Tracking and Reconstruction of 3D Human Motion in Monocular Video Sequences. Dept. of Numerical Analysis and Computer Science, KTH, Stockholm, Sweden 2001 Shanon Ju Thesis: Estimating image motion in layers: The Skin and Bones model. University of Toronto. Jan. 1999 Post doctoral researchers: Siyu Tang , MPI for Intelligent Systems, Jan.2017 - present Dimitris Tzionas , MPI for Intelligent Systems, Oct.2016- present Timo Bolkart , MPI for Intelligent Systems, Oct.2016- present Aamir Ahmad , MPI for Intelligent Systems, Sept.2016- present Sergi Pujades , MPI for Intelligent Systems, Jan.2016- present Former post doctoral Researchers: Alejandra Quiros-Ramirez , MPI for Intelligent Systems, May2015- Nov. 2017. Laura Sevilla , Lecturer Reader in Image and Vision Computing, University of Edinburgh. Ali Osman Ulusoy , (jointly with A. Geiger), Microsoft. Naejin Kong ,Samsung, Korea. Federica Bogo , Microsoft Research, Cambridge. Gerard Pons-Moll, Research Scientist,MPI for Informatik. Ijaz Akhter , postdoctoral researcher,Australia Nationa University, Canbera. Silvia Zuffi , R esearch Scientist, IMATI-CNR, Institute for Applied Mathematics(Milano) . Si Yong Yeo , Scientist, A*Star, Singapore. Cristina Garcia Cifuentes , Amazon. Chaohui Wang , A ssistant Professor, Laboratoire d'Informatique Gaspard Monge, Universit Paris-Est, Paris, France Sren Hauberg , Associate Professor, Technical University of Denmark(DTU), Copenhagen. Hueihan Jhuang , industry, Taiwan. Javier Romero , Amazon. Gregrory Shakhnarovich , Associate Professor, Toyota Technological Institute at Chicago. Sung-Phil Kim , Associate Professor, School of Design and Human Engineering, UNIST, Korea. Ronan Fablet , Professor, Telecom Bretange. Datasets and evalautions I belive that computer vision is advanced by careful evaluation and comparison. Consequently I have been involved in building several public datasets and evaluation websites. FAUST human scan dataset and registration benchmark 3D human scans of multiple people in multiple poses with accurate ground truth correspondences: FAUST site . MPI-Sintel optical flow dataset and evaluation Optical flow benchmark based on the animate film Sintel: MPI-Sintel site . JHMDB: Joint-annotated Human Motion Database Annotated videos for action recognition: JHMDB site . Middlebury Optical Flow Benchmark Image sequences, ground truth flow, and evaluation are all available on the Middlebury Flow site . HumanEva Multi-camera imagery with ground truth 3D human pose: HumanEvasite . Lee Walking Sequence Multi-camera imagery with ground truth 3D human pose. This predates HumanEva and the imagery is grayscale only. There is also software for partical filter tracking on the site. http://cs.brown.edu/~ls/Software/index.html Archival Image Sequences My old Brown site has several image sequences used in my older publications. These include some classic sequences such as Yosemite, the Pepsi can, the SRI tree sequence, and the Flower Garden sequence. Data Optical Flow Code (C and Matlab): 1. The most recent and most accurate optical flow code in Matlab Download This code is descrbed in A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles behind Them Sun, D., Roth, S., and Black, M.J. International Journal of Computer Vision (IJCV) , 106(2):115-137, 2014. (pdf) Secrets of optical flow estimation and their principles Sun, D., Roth, S., and Black, M. J., IEEE Conf. on Computer Vision and Pattern Recog ., CVPR, June 2010. (pdf) This method implements many of the currently best known techniques for accurate optical flow and was once ranked #1 on the Middlebury evaluation (June 2010). The software is made available for research pupropses. Please read the copyright statement and contact me for commerical licensing. 2. Matlab implmentation of the Black and Anandan dense optical flow method The Matlab flow code is easier to use and more accurate than the original C code. The objective function being optimized is the same but the Matlab version uses more modern optimization methods: Matlab implementation of Black and Anandan robust dense optical flow algorithm The method in 1 above is more accurate and also implements Black and Anandan plus much more. 3. Original Black and Anandan method implemented in C The optical flow software here has been used by a number of graphics companies to make special effects for movies. This software is provided for research purposes only; any sale or use for commercial purposes is strictly prohibited. Contact me for the password to download the software, stating that it is for research purposes. Please contact me if you wish to use this code for commercial purpose. If you are a commercial enterprise and would like assistance in using optical flow in your application, please contact me at my consulting address black@opticalflow.com . This is EXPERIMENTAL software. It is provided to illustrate some ideas in the robust estimation of optical flow. Use at your own risk. No warranty is implied by this distribution. Copyright notice. There are two versions available. First, the original C code implementing the robust flow methods described in Black and Anandan '96: Area-based optical flow: robust affine regression. Dense optical flow: robust regularization. Reference: The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields , Black, M. J. and Anandan, P., Computer Vision and Image Understanding, CVIU , 63(1), pp. 75-104, Jan. 1996. (pdf), (pdf from publisher) Robust Principal Component Analysis (PCA) Software is from the ICCV'2001 paper with Fernando De la Torre. De la Torre, F. and Black, M. J., Robust principal component analysis for computer vision, to appear: Int. Conf. on Computer Vision, ICCV-2001, Vancouver, BC. (postscript, 1.0MB)(pdf, 0.36MB), (abstract) Software, demos, and data. Human motion tracking The code below provides a simple Matlab implementation of the Bayesian 3D person tracking system described in ECCV'00 and ICCV'01. It is too slow to be used to track the entire body but can be used to track various limbs and provides a basis for people who want to understand the methods better and extend them. Learning image statistics for Bayesian tracking, Sidenbladh, H. and Black, M. J., Int. Conf. on Computer Vision, ICCV-2001, Vancouver, BC, Vol. II, pp. 709-716. (postscript, 2.8MB)(pdf, 0.38MB), (abstract) Stochastic tracking of 3D human figures using 2D image motion, Sidenbladh, H., Black, M. J., and Fleet, D.J., European Conference on Computer Vision , D. Vernon (Ed.), Springer Verlag, LNCS 1843, Dublin, Ireland, pp. 702-718 June 2000. (postscript)(pdf), (abstract) Software . (Note: if you uncompress and untar this on a PC using Winzip, the path names may be lost which will cause Matlab to fail when you load the .mat files. Instead uncompress/untar using gunzip and tar.) Werner Reichardt Center for Integrative Neuroscience, Eberhard Karls Universitt Tbingen, Member since 2011. Bernstein Center for Computational Neuroscience , Tbingen, since Jan. 2011. Brown Institute for Brain Science (BIBS), Member How to reach me: email: black@tue.mpg.de Skype: michael_j_black Phone:+49 7071 601-1801 FAX:+497071 601-1802 Mailing address Michael J. Black Max Planck Institute for Intelligent Systems Spemannstrasse 41 72076 Tbingen Germany For more information including our address and directions, see the department CONTACT page. I receive more email than I can read, let alone respond to. I apologize if you do not get a response. If you do not hear from me, consider the following: If you need something that is time sensitive (letter of reference, paper review, etc.), please contact or cc my assistant, Melanie Feldhofer (melanie.feldhofer@is.mpg.de) If you have asked me to do something (like review a paper or grant proposal), and I haven't responded saying that I can do it, then I have not agreed to do it. If you are seeking a job or want to be a PhD student,visit the CAREER page Applications for jobs or graduate school should be sent to ps-apply@tuebingen.mpg.de My assistant reads mail sent to me at black@is.mpg.de. If you have something particularly private, you can email me at black@cs.brown.edu and only I will read it. Overview talks Estimating Human Motion: Past, Present, and Future. (with full bibliography) 40 Years DAGM - Invited Talks, GCPR 2018 (pdf) What is optical flow for? On prediction, persistence and structure. Workshop on What is Optical Flow For? ECCV, Munich, Sept. 2018. ( ppt 76MB) The Future and Generative Models: A Case Study of Human Bodies in Motion. 2-hour course given at the Int. Computer Vision Summer School, July 2016. ( ppt 1.5GB) On building digital humans. An overview of our work on 3D body shape, based on a series of talks during 2015. ( ppt 1GB) Keynotes On building digital humans , Animation Studies Summer School , Tbiungen, 2015. How and why to learn a 3D model of the human body, 12th IEEE Int. Conf. on Advanced Video andSignal-based Surveillance, AVSS , Karlsruhe, Germany, Aug. 2015. 4D capture, modeling, and animation of human soft-tissue motion, Kinovis Inaugural Workshop , INRIA Rhone-Alps,Grenoble, 2015. Visions of motor control: From motion capture to thecortical control of movement , Int. Conf. on Robotics and Automation (ICRA) ,Karlsruhe, Germany, May 2013. Modernizing Muybridge: From 3D models of the body to decoding the brain , Keynote, Svenska Sllskapet fr Automatiserad Bildanalys (Swedish Society for Automated Image Analysis, SSBA) , KTH, Stockholm,March 2012. On modeling bodies and brains: From 3D models to decoding the brain , Keynote, Vision, Modeling and Visualization Workshop , October 4-6, 2011, Berlin. Human activity understanding: Observing the body and the brain Keynote, International Workshop on Human Activity Understanding from 3D Data, Colorado Spring, June 24, 2011. Invited Conference, Workshop, and Summer SchoolTalks Learning to be a Digital Human 9th Int. Workshop on Human Behavior Understanding . at ECCV, Sept. 2018. ( pptx ) On Building Digital Humans Shape Analysis and Learning by Geometry and Machine Inst. for Pure and Applied Mathematics (IPAM), UCLA, Feb. 2016. ( pptx ) Estimating 3D human pose and shape using differentiable rendering Inverse Rendering Workshop, ICCV, Santiago Chile, Dec. 2015. ( pptx ) On building digital humans , Computational Vision Summer School (CVSS), Black Forest, Germany, 2015, Machine Learning Summer School , Tbingen, July 2015. The Mathematics of Body Shape -- The Secret Lives ofTriangles in Hollywood , Science Notes, WAHRnehmung ,Tbingen,May 7, 2015. ( youtube ) How to build adigital human, FMX,Conference on Animation, Effects, Games and Transmedia ,Stuttgart, Germany, May 05-08, 2015. Video segmentation: What should the answer be?, First Int.Workshop on Video Segmentation, withECCV'14, Zrich, Sept.2014. Grassmann averages for scalable robust PCA 4th Int.Workshop on Computer Vision , Alghero, Italy, May 2014. Optical flow: The "good parts" version , ETH/MPI Research Network on Learning Systems, Summer School ,Zrich, June 2014,. Optical flow: The "good parts" version , Machine Learning Summer School (MLSS) , Tbiungen, 2013. ( youtube ) MPI-Sintel: From animation to evaluation of optical flow , ECCV 2012 Workshop onUnsolved Problems in Optical Flow and Stereo Estimation , Oct. 12,Florence Italy. [pdf] Modernizing Muybridge: From 3D models of the human body todecoding the brain, Sensory Coding & Natural Environment, IST Austrial, Sept. 2012. A naturalistic film for optical flow evaluation , At the intersection of Vision, Graphics, Learning and Sensing - Representations and Applications Workshop , Cambridge, May 2012. Thinking about movement: Decoding the brain to restore lost function , Inaugural Symposium: New Perspectives in Integrative Neuroscience , Hertie Institute, Tbingen, May 2012. On modeling and estimating human body shape , Rank Prize Symposium on Machine Learning and Computer Vision , Grasmere, UK, March 26-29, 2012. Modernizing Muybridge: From 3D models of the body to decoding the brain , Bernstein Cluster C Symposium,Bayesian Inference: From Spikes to Behaviour, Tbingen, December 9-10, 2011. From Muybridge to a brain-computer interface: A computational investigation of animal movement , Technion Computer Engineering (TCE) Inaugural Conference , Haifa, Israel, June 1-5, 2011. Invited Talks: Colloquia and Seminars The Mathematics of Body Shape, CIN-MPI Body Perception seminar, Tbingen, July 2012. Modernizing Muybridge: From 3D models of the body to decoding the brain, Gatsby Computational Neuroscience Unit, Univ. College, London, Jan. 2012. Modernizing Muybridge: From 3D models of the body to decoding the brain, Oxford University , Robotics Research Group Seminar, Jan. 2012. Modeling bodies and brains: From computer vision to neural prostheses, Wilhelm Schickhard Institute for Computer Sciences, Eberhard Karls Universit y, Tbingen, Germany, July 2011. Other Talks From Scans to Avatars. Using Multi-Viewpoint, High Precision 3D Surface Imaging to create Realistic Deformable Models of the Body, Jointly with Chris Lane, CEO 3dMD LLC. 3rd International Conference and Exhibition on3D Body Scanning Technologies, Lugano, Switzerland, 16-17 October 2012. Computing Optical Flow, Computational Vision Summer School 2012 , Freudenstadt-Lauterbad (Black Forest), June-July 2012 Seeing machines, Paul-Peter Ewald Kolloquium , MPI for Ingelligent Systems, Stuttgart, July 1, 2011. 300 results ( View BibTeX file of all listed publications ) 2019 The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from 3D Measurements Pujades, S. , Mohler, B. , Thaler, A. , Tesch, J. , Mahmood, N. , Hesse, N., Blthoff, H. H., Black, M. J. IEEE Transactions on Visualization and Computer Graphics , 2019 (article) Abstract Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating The Virtual Caliper, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned. [BibTex] Share 2019 Pujades, S. , Mohler, B. , Thaler, A. , Tesch, J. , Mahmood, N. , Hesse, N., Blthoff, H. H., Black, M. J. The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from 3D Measurements IEEE Transactions on Visualization and Computer Graphics , 2019 (article) [BibTex] Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders Ghosh, P. , Losalka, A., Black, M. J. In Proc. AAAI , 2019 (inproceedings) Abstract Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success till now. Two distinct categories of samples against which deep neural networks are vulnerable, ``adversarial samples" and ``fooling samples", have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can defend against them both under a unified framework. Our model has the form of a variational autoencoder with a Gaussian mixture prior on the latent variable, such that each mixture component corresponds to a single class. We show how selective classification can be performed using this model, thereby causing the adversarial objective to entail a conflict. The proposed method leads to the rejection of adversarial samples instead of misclassification, while maintaining high precision and recall on test data. It also inherently provides a way of learning a selective classifier in a semi-supervised scenario, which can similarly resist adversarial attacks. We further show how one can reclassify the detected adversarial samples by iterative optimization. link (url) Project Page [BibTex] Share Ghosh, P. , Losalka, A., Black, M. J. Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders In Proc. AAAI , 2019 (inproceedings) link (url) Project Page [BibTex] 2018 Customized Multi-Person Tracker Ma, L., Tang, S. , Black, M. J. , Gool, L. V. In Computer Vision ACCV 2018 , Springer International Publishing, December 2018 (inproceedings) PDF Project Page [BibTex] Share 2018 Ma, L., Tang, S. , Black, M. J. , Gool, L. V. Customized Multi-Person Tracker In Computer Vision ACCV 2018 , Springer International Publishing, December 2018 (inproceedings) PDF Project Page [BibTex] Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time Huang, Y. , Kaufmann, M., Aksan, E., Black, M. J. , Hilliges, O., Pons-Moll, G. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 37, pages: 185:1-185:15, ACM, November 2018, Two first authors contributed equally (article) Abstract We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user's body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330,000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes. data code pdf preprint video DOI Project Page [BibTex] Share Huang, Y. , Kaufmann, M., Aksan, E., Black, M. J. , Hilliges, O., Pons-Moll, G. Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 37, pages: 185:1-185:15, ACM, November 2018, Two first authors contributed equally (article) data code pdf preprint video DOI Project Page [BibTex] On the Integration of Optical Flow and Action Recognition Sevilla-Lara, L. , Liao, Y. , Guney, F., Jampani, V. , Geiger, A. , Black, M. J. In German Conference on Pattern Recognition (GCPR) , October 2018 (inproceedings) Abstract Most of the top performing action recognition methods use optical flow as a "black box" input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities. arXiv [BibTex] Share Sevilla-Lara, L. , Liao, Y. , Guney, F., Jampani, V. , Geiger, A. , Black, M. J. On the Integration of Optical Flow and Action Recognition In German Conference on Pattern Recognition (GCPR) , October 2018 (inproceedings) arXiv [BibTex] Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles Price, E. , Lawless, G. , Ludwig, R. , Martinovic, I. , Buelthoff, H. H., Black, M. J. , Ahmad, A. IEEE Robotics and Automation Letters , Robotics and Automation Letters , 3(4):3193-3200, IEEE, October 2018, Also accepted and presented in the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). (article) Abstract Multi-camera tracking of humans and animals in outdoor environments is a relevant and challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, online, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs and active selection of most informative regions of image. We demonstrate the efficiency of our approach through simulations with up to 16 robots and real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. ROS-based source code is provided for the benefit of the community. Published Version link (url) DOI [BibTex] Share Price, E. , Lawless, G. , Ludwig, R. , Martinovic, I. , Buelthoff, H. H., Black, M. J. , Ahmad, A. Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles IEEE Robotics and Automation Letters , Robotics and Automation Letters , 3(4):3193-3200, IEEE, October 2018, Also accepted and presented in the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). (article) Published Version link (url) DOI [BibTex] Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation Wulff, J. , Black, M. J. In German Conference on Pattern Recognition (GCPR) , October 2018 (inproceedings) Abstract The difficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic n losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then fine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation as a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fi ne-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fi elds. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow. pdf arXiv Project Page [BibTex] Share Wulff, J. , Black, M. J. Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation In German Conference on Pattern Recognition (GCPR) , October 2018 (inproceedings) pdf arXiv Project Page [BibTex] Generating 3D Faces using Convolutional Mesh Autoencoders Ranjan, A. , Bolkart, T. , Sanyal, S. , Black, M. J. In European Conference on Computer Vision (ECCV) , Lecture Notes in Computer Science, vol 11207, pages: 725-741, Springer, Cham, September 2018 (inproceedings) Abstract Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/. code paper supplementary link (url) DOI Project Page Project Page [BibTex] Share Ranjan, A. , Bolkart, T. , Sanyal, S. , Black, M. J. Generating 3D Faces using Convolutional Mesh Autoencoders In European Conference on Computer Vision (ECCV) , Lecture Notes in Computer Science, vol 11207, pages: 725-741, Springer, Cham, September 2018 (inproceedings) code paper supplementary link (url) DOI Project Page Project Page [BibTex] Learning Human Optical Flow Ranjan, A. , Romero, J. , Black, M. J. In 29th British Machine Vision Conference , September 2018 (inproceedings) Abstract The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research. video code pdf link (url) Project Page Project Page [BibTex] Share Ranjan, A. , Romero, J. , Black, M. J. Learning Human Optical Flow In 29th British Machine Vision Conference , September 2018 (inproceedings) video code pdf link (url) Project Page Project Page [BibTex] Unsupervised Learning of Multi-Frame Optical Flow with Occlusions Janai, J. , Gney, F. , Ranjan, A. , Black, M. J. , Geiger, A. In European Conference on Computer Vision (ECCV) , Lecture Notes in Computer Science, vol 11220, pages: 713-731, Springer, Cham, September 2018 (inproceedings) pdf suppmat DOI Project Page [BibTex] Share Janai, J. , Gney, F. , Ranjan, A. , Black, M. J. , Geiger, A. Unsupervised Learning of Multi-Frame Optical Flow with Occlusions In European Conference on Computer Vision (ECCV) , Lecture Notes in Computer Science, vol 11220, pages: 713-731, Springer, Cham, September 2018 (inproceedings) pdf suppmat DOI Project Page [BibTex] Learning an Infant Body Model from RGB-D Data for Accurate Full Body Motion Analysis Hesse, N., Pujades, S. , Romero, J. , Black, M. J. , Bodensteiner, C., Arens, M., Hofmann, U. G., Tacke, U., Hadders-Algra, M., Weinberger, R., Muller-Felber, W., Schroeder, A. S. In Int. Conf. on Medical Image Computing and Computer Assisted Intervention (MICCAI) , September 2018 (inproceedings) Abstract Infant motion analysis enables early detection of neurodevelopmental disorders like cerebral palsy (CP). Diagnosis, however, is challenging, requiring expert human judgement. An automated solution would be beneficial but requires the accurate capture of 3D full-body movements. To that end, we develop a non-intrusive, low-cost, lightweight acquisition system that captures the shape and motion of infants. Going beyond work on modeling adult body shape, we learn a 3D Skinned Multi-Infant Linear body model (SMIL) from noisy, low-quality, and incomplete RGB-D data. We demonstrate the capture of shape and motion with 37 infants in a clinical environment. Quantitative experiments show that SMIL faithfully represents the data and properly factorizes the shape and pose of the infants. With a case study based on general movement assessment (GMA), we demonstrate that SMIL captures enough information to allow medical assessment. SMIL provides a new tool and a step towards a fully automatic system for GMA. pdf Project page video extended arXiv version Project Page [BibTex] Share Hesse, N., Pujades, S. , Romero, J. , Black, M. J. , Bodensteiner, C., Arens, M., Hofmann, U. G., Tacke, U., Hadders-Algra, M., Weinberger, R., Muller-Felber, W., Schroeder, A. S. Learning an Infant Body Model from RGB-D Data for Accurate Full Body Motion Analysis In Int. Conf. on Medical Image Computing and Computer Assisted Intervention (MICCAI) , September 2018 (inproceedings) pdf Project page video extended arXiv version Project Page [BibTex] Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera Marcard, T. V., Henschel, R., Black, M. J. , Rosenhahn, B., Pons-Moll, G. In European Conference on Computer Vision (ECCV) , Lecture Notes in Computer Science, vol 11214, pages: 614-631, Springer, Cham, September 2018 (inproceedings) Abstract In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW ), a new dataset consisting of more than 51; 000 frames with accurate 3D pose in challenging sequences, including walking in the city, going up-stairs, having co ffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW. pdf SupMat data project DOI Project Page [BibTex] Share Marcard, T. V., Henschel, R., Black, M. J. , Rosenhahn, B., Pons-Moll, G. Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera In European Conference on Computer Vision (ECCV) , Lecture Notes in Computer Science, vol 11214, pages: 614-631, Springer, Cham, September 2018 (inproceedings) pdf SupMat data project DOI Project Page [BibTex] Visual Perception and Evaluation of Photo-Realistic Self-Avatars From 3D Body Scans in Males and Females Thaler, A. , Piryankova, I., Stefanucci, J. K., Pujades, S. , de la Rosa, S., Streuber, S. , Romero, J. , Black, M. J. , Mohler, B. J. Frontiers in ICT , 5, pages: 1-14, September 2018 (article) Abstract The creation or streaming of photo-realistic self-avatars is important for virtual reality applications that aim for perception and action to replicate real world experience. The appearance and recognition of a digital self-avatar may be especially important for applications related to telepresence, embodied virtual reality, or immersive games. We investigated gender differences in the use of visual cues (shape, texture) of a self-avatar for estimating body weight and evaluating avatar appearance. A full-body scanner was used to capture each participant's body geometry and color information and a set of 3D virtual avatars with realistic weight variations was created based on a statistical body model. Additionally, a second set of avatars was created with an average underlying body shape matched to each participants height and weight. In four sets of psychophysical experiments, the influence of visual cues on the accuracy of body weight estimation and the sensitivity to weight changes was assessed by manipulating body shape (own, average) and texture (own photo-realistic, checkerboard). The avatars were presented on a large-screen display, and participants responded to whether the avatar's weight corresponded to their own weight. Participants also adjusted the avatar's weight to their desired weight and evaluated the avatar's appearance with regard to similarity to their own body, uncanniness, and their willingness to accept it as a digital representation of the self. The results of the psychophysical experiments revealed no gender difference in the accuracy of estimating body weight in avatars. However, males accepted a larger weight range of the avatars as corresponding to their own. In terms of the ideal body weight, females but not males desired a thinner body. With regard to the evaluation of avatar appearance, the questionnaire responses suggest that own photo-realistic texture was more important to males for higher similarity ratings, while own body shape seemed to be more important to females. These results argue for gender-specific considerations when creating self-avatars. pdf DOI [BibTex] Share Thaler, A. , Piryankova, I., Stefanucci, J. K., Pujades, S. , de la Rosa, S., Streuber, S. , Romero, J. , Black, M. J. , Mohler, B. J. Visual Perception and Evaluation of Photo-Realistic Self-Avatars From 3D Body Scans in Males and Females Frontiers in ICT , 5, pages: 1-14, September 2018 (article) pdf DOI [BibTex] Statistical Modelling of Fingertip Deformations and Contact Forces during Tactile Interaction Gueorguiev, D. , Tzionas, D. , Pacchierotti, C., Black, M. J. , Kuchenbecker, K. J. Extended abstract presented at the Hand, Brain and Technology conference (HBT), Ascona, Switzerland, August 2018 (misc) Abstract Little is known about the shape and properties of the human finger during haptic interaction, even though these are essential parameters for controlling wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning (3D over time) and modelling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution while simultaneously recording the interfacial forces at the contact. Preliminary results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion and proximal/distal bending, deformations that cannot be captured by imaging of the contact area alone. Therefore, we are currently capturing a dataset that will enable us to create a statistical model of the fingers deformations and predict the contact forces induced by tactile interaction with objects. This technique could improve current methods for tactile rendering in wearable haptic devices, which rely on general physical modelling of the skins compliance, by developing an accurate model of the variations in finger properties across the human population. The availability of such a model will also enable a more realistic simulation of virtual finger behaviour in virtual reality (VR) environments, as well as the ability to accurately model a specific users finger from lower resolution data. It may also be relevant for inferring the physical properties of the underlying tissue from observing the surface mesh deformations, as previously shown for body tissues. Project Page [BibTex] Share Gueorguiev, D. , Tzionas, D. , Pacchierotti, C., Black, M. J. , Kuchenbecker, K. J. Statistical Modelling of Fingertip Deformations and Contact Forces during Tactile Interaction Extended abstract presented at the Hand, Brain and Technology conference (HBT), Ascona, Switzerland, August 2018 (misc) Project Page [BibTex] Decentralized MPC based Obstacle Avoidance for Multi-Robot Target Tracking Scenarios Tallamraju, R. , Rajappa, S., Black, M. J. , Karlapalem, K., Ahmad, A. 2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR) , pages: 1-8, IEEE, August 2018 (conference) Abstract In this work, we consider the problem of decentralized multi-robot target tracking and obstacle avoidance in dynamic environments. Each robot executes a local motion planning algorithm which is based on model predictive control (MPC). The planner is designed as a quadratic program, subject to constraints on robot dynamics and obstacle avoidance. Repulsive potential field functions are employed to avoid obstacles. The novelty of our approach lies in embedding these non-linear potential field functions as constraints within a convex optimization framework. Our method convexifies nonconvex constraints and dependencies, by replacing them as pre-computed external input forces in robot dynamics. The proposed algorithm additionally incorporates different methods to avoid field local minima problems associated with using potential field functions in planning. The motion planner does not enforce predefined trajectories or any formation geometry on the robots and is a comprehensive solution for cooperative obstacle avoidance in the context of multi-robot target tracking. We perform simulation studies for different scenarios to showcase the convergence and efficacy of the proposed algorithm. Published Version link (url) DOI [BibTex] Share Tallamraju, R. , Rajappa, S., Black, M. J. , Karlapalem, K., Ahmad, A. Decentralized MPC based Obstacle Avoidance for Multi-Robot Target Tracking Scenarios 2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR) , pages: 1-8, IEEE, August 2018 (conference) Published Version link (url) DOI [BibTex] Robust Physics-based Motion Retargeting with Realistic Body Shapes Borno, M. A., Righetti, L. , Black, M. J. , Delp, S. L., Fiume, E., Romero, J. Computer Graphics Forum , 37, pages: 6:1-12, July 2018 (article) Abstract Motion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically-based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of motions from a single mocap sequence by simply varying the characters. In an interactive environment, successful retargeting requires adapting the motion to unexpected external forces. We achieve robustness to such forces using a novel LQR-tree formulation. We show that the simulated motions look appropriate to each characters anatomy and their actions are robust to perturbations. pdf video Project Page Project Page [BibTex] Share Borno, M. A., Righetti, L. , Black, M. J. , Delp, S. L., Fiume, E., Romero, J. Robust Physics-based Motion Retargeting with Realistic Body Shapes Computer Graphics Forum , 37, pages: 6:1-12, July 2018 (article) pdf video Project Page Project Page [BibTex] Method and Apparatus for Estimating Body Shape Black, M. J. , Balan, A., Weiss, A. , Sigal, L., Loper, M. , St Clair, T. June 2018, U.S.~Patent 10,002,460 (misc) Abstract A system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping. Google Patents Project Page [BibTex] Share Black, M. J. , Balan, A., Weiss, A. , Sigal, L., Loper, M. , St Clair, T. Method and Apparatus for Estimating Body Shape June 2018, U.S.~Patent 10,002,460 (misc) Google Patents Project Page [BibTex] Adversarial Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation Ranjan, A. , Jampani, V. , Kim, K., Sun, D. , Wulff, J. , Black, M. J. May 2018 (article) Abstract We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled and, consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other by exploiting known geometric constraints. In order to model geometric constraints, we introduce Adversarial Collaboration, a framework that facilitates competition and collaboration between neural networks. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. Adversarial Collaboration works much like expectation-maximization but with neural networks that act as adversaries, competing to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state of the art results amongst unsupervised methods. pdf link (url) Project Page Project Page [BibTex] Share Ranjan, A. , Jampani, V. , Kim, K., Sun, D. , Wulff, J. , Black, M. J. Adversarial Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation May 2018 (article) pdf link (url) Project Page Project Page [BibTex] Assessing body image in anorexia nervosa using biometric self-avatars in virtual reality: Attitudinal components rather than visual body size estimation are distorted Mlbert, S. C. , Thaler, A. , Mohler, B. J. , Streuber, S. , Romero, J. , Black, M. J. , Zipfel, S., Karnath, H., Giel, K. E. Psychological Medicine , 48(4):642-653, March 2018 (article) Abstract Background: Body image disturbance (BID) is a core symptom of anorexia nervosa (AN), but as yet distinctive features of BID are unknown. The present study aimed at disentangling perceptual and attitudinal components of BID in AN. Methods: We investigated n=24 women with AN and n=24 controls. Based on a 3D body scan, we created realistic virtual 3D bodies (avatars) for each participant that were varied through a range of 20% of the participants' weights. Avatars were presented in a virtual reality mirror scenario. Using different psychophysical tasks, participants identified and adjusted their actual and their desired body weight. To test for general perceptual biases in estimating body weight, a second experiment investigated perception of weight and shape matched avatars with another identity. Results: Women with AN and controls underestimated their weight, with a trend that women with AN underestimated more. The average desired body of controls had normal weight while the average desired weight of women with AN corresponded to extreme AN (DSM-5). Correlation analyses revealed that desired body weight, but not accuracy of weight estimation, was associated with eating disorder symptoms. In the second experiment, both groups estimated accurately while the most attractive body was similar to Experiment 1. Conclusions: Our results contradict the widespread assumption that patients with AN overestimate their body weight due to visual distortions. Rather, they illustrate that BID might be driven by distorted attitudes with regard to the desired body. Clinical interventions should aim at helping patients with AN to change their desired weight. doi pdf DOI Project Page [BibTex] Share Mlbert, S. C. , Thaler, A. , Mohler, B. J. , Streuber, S. , Romero, J. , Black, M. J. , Zipfel, S., Karnath, H., Giel, K. E. Assessing body image in anorexia nervosa using biometric self-avatars in virtual reality: Attitudinal components rather than visual body size estimation are distorted Psychological Medicine , 48(4):642-653, March 2018 (article) doi pdf DOI Project Page [BibTex] Towards a Statistical Model of Fingertip Contact Deformations from 4D Data Gueorguiev, D. , Tzionas, D. , Pacchierotti, C., Black, M. J. , Kuchenbecker, K. J. Work-in-progress paper (3 pages) presented at the IEEE Haptics Symposium, San Francisco, USA, March 2018 (misc) Abstract Little is known about the shape and properties of the human finger during haptic interaction even though this knowledge is essential to control wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning and modeling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution. The results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion of about 0.2 cm and proximal/distal bending of about 30, deformations that cannot be captured by imaging of the contact area alone. This project constitutes a first step towards an accurate statistical model of the fingers behavior during haptic interaction. link (url) Project Page [BibTex] Share Gueorguiev, D. , Tzionas, D. , Pacchierotti, C., Black, M. J. , Kuchenbecker, K. J. Towards a Statistical Model of Fingertip Contact Deformations from 4D Data Work-in-progress paper (3 pages) presented at the IEEE Haptics Symposium, San Francisco, USA, March 2018 (misc) link (url) Project Page [BibTex] Body size estimation of self and others in females varying in BMI Thaler, A. , Geuss, M. N., Mlbert, S. C. , Giel, K. E., Streuber, S. , Romero, J. , Black, M. J. , Mohler, B. J. PLoS ONE , 13(2), Febuary 2018 (article) Abstract Previous literature suggests that a disturbed ability to accurately identify own body size may contribute to overweight. Here, we investigated the influence of personal body size, indexed by body mass index (BMI), on body size estimation in a non-clinical population of females varying in BMI. We attempted to disentangle general biases in body size estimates and attitudinal influences by manipulating whether participants believed the body stimuli (personalized avatars with realistic weight variations) represented their own body or that of another person. Our results show that the accuracy of own body size estimation is predicted by personal BMI, such that participants with lower BMI underestimated their body size and participants with higher BMI overestimated their body size. Further, participants with higher BMI were less likely to notice the same percentage of weight gain than participants with lower BMI. Importantly, these results were only apparent when participants were judging a virtual body that was their own identity (Experiment 1), but not when they estimated the size of a body with another identity and the same underlying body shape (Experiment 2a). The different influences of BMI on accuracy of body size estimation and sensitivity to weight change for self and other identity suggests that effects of BMI on visual body size estimation are self-specific and not generalizable to other bodies. pdf DOI Project Page [BibTex] Share Thaler, A. , Geuss, M. N., Mlbert, S. C. , Giel, K. E., Streuber, S. , Romero, J. , Black, M. J. , Mohler, B. J. Body size estimation of self and others in females varying in BMI PLoS ONE , 13(2), Febuary 2018 (article) pdf DOI Project Page [BibTex] Co-Registration Simultaneous Alignment and Modeling of Articulated 3D Shapes Black, M. , Hirshberg, D. , Loper, M. , Rachlin, E. , Weiss, A. Febuary 2018, U.S.~Patent 9,898,848 (misc) Abstract Present application refers to a method, a model generation unit and a computer program (product) for generating trained models (M) of moving persons, based on physically measured person scan data (S). The approach is based on a common template (T) for the respective person and on the measured person scan data (S) in different shapes and different poses. Scan data are measured with a 3D laser scanner. A generic personal model is used for co-registering a set of person scan data (S) aligning the template (T) to the set of person scans (S) while simultaneously training the generic personal model to become a trained person model (M) by constraining the generic person model to be scan-specific, person-specific and pose-specific and providing the trained model (M), based on the co registering of the measured object scan data (S). text [BibTex] Share Black, M. , Hirshberg, D. , Loper, M. , Rachlin, E. , Weiss, A. Co-Registration Simultaneous Alignment and Modeling of Articulated 3D Shapes Febuary 2018, U.S.~Patent 9,898,848 (misc) text [BibTex] End-to-end Recovery of Human Shape and Pose Kanazawa, A. , Black, M. J. , Jacobs, D. W., Malik, J. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE Computer Society, 2018 (inproceedings) Abstract We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation. pdf code project video Project Page [BibTex] Share Kanazawa, A. , Black, M. J. , Jacobs, D. W., Malik, J. End-to-end Recovery of Human Shape and Pose In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE Computer Society, 2018 (inproceedings) pdf code project video Project Page [BibTex] End-to-end Learning for Graph Decomposition Song, J., Andres, B., Black, M. , Hilliges, O., Tang, S. arXiv:1812.09737 , 2018 (article) Abstract We propose a novel end-to-end trainable framework for the graph decomposition problem. The minimum cost mul- ticut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimiza- tion problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels of the initial graph and the hard con- straints are introduced in the CRF as high-order potentials. The parameters of a standard Neural Network and the fully differentiable CRF are optimized in an end-to-end manner. Furthermore, our method utilizes the cycle constraints as meta-supervisory signals during the learning of the deep feature representations by taking the dependencies between the output random variables into account. We present analy- ses of the end-to-end learned representations, showing the impact of the joint training, on the task of clustering images of MNIST. We also validate the effectiveness of our approach both for the feature learning and the final clustering on the challenging task of real-world multi-person pose estimation paper.pdf Video link (url) [BibTex] Share Song, J., Andres, B., Black, M. , Hilliges, O., Tang, S. End-to-end Learning for Graph Decomposition arXiv:1812.09737 , 2018 (article) paper.pdf Video link (url) [BibTex] Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images Zuffi, S. , Kanazawa, A. , Black, M. J. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE Computer Society, 2018 (inproceedings) Abstract Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames. pdf code/data 3D models Project Page [BibTex] Share Zuffi, S. , Kanazawa, A. , Black, M. J. Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE Computer Society, 2018 (inproceedings) pdf code/data 3D models Project Page [BibTex] 2017 Learning a model of facial shape and expression from 4D scans Li, T. , Bolkart, T. , Black, M. J. , Li, H., Romero, J. ACM Transactions on Graphics , 36(6):194:1-194:17, November 2017, Two first authors contributed equally (article) Abstract The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression from 4D face sequences in the D3DFACS dataset along with additional 4D sequences.We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de). data/model video paper supplemental Project Page [BibTex] Share 2017 Li, T. , Bolkart, T. , Black, M. J. , Li, H., Romero, J. Learning a model of facial shape and expression from 4D scans ACM Transactions on Graphics , 36(6):194:1-194:17, November 2017, Two first authors contributed equally (article) data/model video paper supplemental Project Page [BibTex] Investigating Body Image Disturbance in Anorexia Nervosa Using Novel Biometric Figure Rating Scales: A Pilot Study Mlbert, S. C. , Thaler, A. , Streuber, S. , Black, M. J. , Karnath, H., Zipfel, S., Mohler, B. , Giel, K. E. European Eating Disorders Review , 25(6):607-612, November 2017 (article) Abstract This study uses novel biometric figure rating scales (FRS) spanning body mass index (BMI) 13.8 to 32.2 kg/m2 and BMI 18 to 42 kg/m2. The aims of the study were (i) to compare FRS body weight dissatisfaction and perceptual distortion of women with anorexia nervosa (AN) to a community sample; (ii) how FRS parameters are associated with questionnaire body dissatisfaction, eating disorder symptoms and appearance comparison habits; and (iii) whether the weight spectrum of the FRS matters. Women with AN (n=24) and a community sample of women (n=104) selected their current and ideal body on the FRS and completed additional questionnaires. Women with AN accurately picked the body that aligned best with their actual weight in both FRS. Controls underestimated their BMI in the FRS 1432 and were accurate in the FRS 1842. In both FRS, women with AN desired a body close to their actual BMI and controls desired a thinner body. Our observations suggest that body image disturbance in AN is unlikely to be characterized by a visual perceptual disturbance, but rather by an idealization of underweight in conjunction with high body dissatisfaction. The weight spectrum of FRS can influence the accuracy of BMI estimation. publisher DOI Project Page [BibTex] Share Mlbert, S. C. , Thaler, A. , Streuber, S. , Black, M. J. , Karnath, H., Zipfel, S., Mohler, B. , Giel, K. E. Investigating Body Image Disturbance in Anorexia Nervosa Using Novel Biometric Figure Rating Scales: A Pilot Study European Eating Disorders Review , 25(6):607-612, November 2017 (article) publisher DOI Project Page [BibTex] Embodied Hands: Modeling and Capturing Hands and Bodies Together Romero, J. , Tzionas, D. , Black, M. J. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 36(6):245:1-245:17, 245:1245:17, ACM, November 2017 (article) Abstract Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes at http://mano.is.tue.mpg.de. website youtube paper suppl video link (url) DOI Project Page [BibTex] Share Romero, J. , Tzionas, D. , Black, M. J. Embodied Hands: Modeling and Capturing Hands and Bodies Together ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) , 36(6):245:1-245:17, 245:1245:17, ACM, November 2017 (article) website youtube paper suppl video link (url) DOI Project Page [BibTex] Parameterized Model of 2D Articulated Human Shape Black, M. J. , Freifeld, O. , Weiss, A., Loper, M. , Guan, P. September 2017, U.S.~Patent 9,761,060 (misc) Abstract Disclosed are computer-readable devices, systems and methods for generating a model of a clothed body. The method includes generating a model of an unclothed human body, the model capturing a shape or a pose of the unclothed human body, determining two-dimensional contours associated with the model, and computing deformations by aligning a contour of a clothed human body with a contour of the unclothed human body. Based on the two-dimensional contours and the deformations, the method includes generating a first two-dimensional model of the unclothed human body, the first two-dimensional model factoring the deformations of the unclothed human body into one or more of a shape variation component, a viewpoint change, and a pose variation and learning an eigen-clothing model using principal component analysis applied to the deformations, wherein the eigen-clothing model classifies different types of clothing, to yield a second two-dimensional model of a clothed human body. Google Patents [BibTex] Share Black, M. J. , Freifeld, O. , Weiss, A., Loper, M. , Guan, P. Parameterized Model of 2D Articulated Human Shape September 2017, U.S.~Patent 9,761,060 (misc) Google Patents [BibTex] Effects of animation retargeting on perceived action outcomes Kenny, S., Mahmood, N. , Honda, C., Black, M. J. , Troje, N. F. Proceedings of the ACM Symposium on Applied Perception (SAP17) , pages: 2:1-2:7, September 2017 (conference) Abstract The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person's movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. Based on a set of 67 markers, we estimated both the kinematics of the actions as well as the performer's individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. In a virtual reality environment, observers rated the perceived weight or thrown distance of the objects. They were also asked to explicitly discriminate between consistent and hybrid stimuli. Observers were unable to accomplish the latter, but hybridization of shape and motion influenced their judgements of action outcome in systematic ways. Inconsistencies between shape and motion were assimilated into an altered perception of the action outcome. pdf DOI [BibTex] Share Kenny, S., Mahmood, N. , Honda, C., Black, M. J. , Troje, N. F. Effects of animation retargeting on perceived action outcomes Proceedings of the ACM Symposium on Applied Perception (SAP17) , pages: 2:1-2:7, September 2017 (conference) pdf DOI [BibTex] Dynamic FAUST: Registering Human Bodies in Motion Bogo, F. , Romero, J. , Pons-Moll, G. , Black, M. J. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract While the ready availability of 3D scan data has influenced research throughout computer vision, less attention has focused on 4D data; that is 3D scans of moving nonrigid objects, captured over time. To be useful for vision research, such 4D scans need to be registered, or aligned, to a common topology. Consequently, extending mesh registration methods to 4D is important. Unfortunately, no ground-truth datasets are available for quantitative evaluation and comparison of 4D registration methods. To address this we create a novel dataset of high-resolution 4D scans of human subjects in motion, captured at 60 fps. We propose a new mesh registration method that uses both 3D geometry and texture information to register all scans in a sequence to a common reference topology. The approach exploits consistency in texture over both short and long time intervals and deals with temporal offsets between shape and texture capture. We show how using geometry alone results in significant errors in alignment when the motions are fast and non-rigid. We evaluate the accuracy of our registration and provide a dataset of 40,000 raw and aligned meshes. Dynamic FAUST extends the popular FAUST dataset to dynamic 4D data, and is available for research purposes at http://dfaust.is.tue.mpg.de. pdf video Project Page Project Page Project Page [BibTex] Share Bogo, F. , Romero, J. , Pons-Moll, G. , Black, M. J. Dynamic FAUST: Registering Human Bodies in Motion In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) pdf video Project Page Project Page Project Page [BibTex] Learning from Synthetic Humans Varol, G. , Romero, J. , Martin, X., Mahmood, N. , Black, M. J. , Laptev, I., Schmid, C. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract Estimating human pose, shape, and motion from images and videos are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL (Synthetic hUmans foR REAL tasks): a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new dataset open up new possibilities for advancing person analysis using cheap and large-scale synthetic data. arXiv project data Project Page Project Page [BibTex] Share Varol, G. , Romero, J. , Martin, X., Mahmood, N. , Black, M. J. , Laptev, I., Schmid, C. Learning from Synthetic Humans In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) arXiv project data Project Page Project Page [BibTex] On human motion prediction using recurrent neural networks Martinez, J. , Black, M. J. , Romero, J. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction. arXiv Project Page [BibTex] Share Martinez, J. , Black, M. J. , Romero, J. On human motion prediction using recurrent neural networks In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) arXiv Project Page [BibTex] Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data Janai, J. , Gney, F. , Wulff, J. , Black, M. , Geiger, A. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , pages: 1406-1416, IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract Existing optical flow datasets are limited in size and variability due to the difficulty of capturing dense ground truth. In this paper, we tackle this problem by tracking pixels through densely sampled space-time volumes recorded with a high-speed video camera. Our model exploits the linearity of small motions and reasons about occlusions from multiple frames. Using our technique, we are able to establish accurate reference flow fields outside the laboratory in natural environments. Besides, we show how our predictions can be used to augment the input images with realistic motion blur. We demonstrate the quality of the produced flow fields on synthetic and real-world datasets. Finally, we collect a novel challenging optical flow dataset by applying our technique on data from a high-speed camera and analyze the performance of the state-of-the-art in optical flow under various levels of motion blur. pdf suppmat Project page Video DOI Project Page [BibTex] Share Janai, J. , Gney, F. , Wulff, J. , Black, M. , Geiger, A. Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , pages: 1406-1416, IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) pdf suppmat Project page Video DOI Project Page [BibTex] Optical Flow in Mostly Rigid Scenes Wulff, J. , Sevilla-Lara, L. , Black, M. J. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , pages: 6911-6920, IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPISintel and KITTI-2015 benchmarks. pdf SupMat video code Project Page [BibTex] Share Wulff, J. , Sevilla-Lara, L. , Black, M. J. Optical Flow in Mostly Rigid Scenes In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , pages: 6911-6920, IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) pdf SupMat video code Project Page [BibTex] Detailed, accurate, human shape estimation from clothed 3D scan sequences Zhang, C. , Pujades, S. , Black, M. , Pons-Moll, G. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE Computer Society, Washington, DC, USA, July 2017, Spotlight (inproceedings) Abstract We address the problem of estimating human body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited statistical models of body shape produce overly smooth shapes lacking personalized details. In this paper we contribute a new approach to recover not only an approximate shape of the person, but also their detailed shape. Our approach allows the estimated shape to deviate from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available a new high quality 4D dataset that enables quantitative evaluation. Our method outperforms the previous state of the art, both qualitatively and quantitatively. arxiv_preprint video dataset pdf supplemental Project Page [BibTex] Share Zhang, C. , Pujades, S. , Black, M. , Pons-Moll, G. Detailed, accurate, human shape estimation from clothed 3D scan sequences In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE Computer Society, Washington, DC, USA, July 2017, Spotlight (inproceedings) arxiv_preprint video dataset pdf supplemental Project Page [BibTex] 3D Menagerie: Modeling the 3D Shape and Pose of Animals Zuffi, S. , Kanazawa, A. , Jacobs, D., Black, M. J. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , pages: 5524-5532, IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract There has been significant work on learning realistic, articulated, 3D models of the human body. In contrast, there are few such models of animals, despite many applications. The main challenge is that animals are much less cooperative than humans. The best human body models are learned from thousands of 3D scans of people in specific poses, which is infeasible with live animals. Consequently, we learn our model from a small set of 3D scans of toy figurines in arbitrary poses. We employ a novel part-based shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and refine the registrations and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the registration to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and fit to data. We demonstrate generalization by fitting it to images of real animals including species not seen in training. pdf video Project Page [BibTex] Share Zuffi, S. , Kanazawa, A. , Jacobs, D., Black, M. J. 3D Menagerie: Modeling the 3D Shape and Pose of Animals In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , pages: 5524-5532, IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) pdf video Project Page [BibTex] Optical Flow Estimation using a Spatial Pyramid Network Ranjan, A. , Black, M. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small (< 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning. pdf SupMat project/code [BibTex] Share Ranjan, A. , Black, M. Optical Flow Estimation using a Spatial Pyramid Network In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) pdf SupMat project/code [BibTex] Semantic Multi-view Stereo: Jointly Estimating Objects and Voxels Ulusoy, A. O. , Black, M. J. , Geiger, A. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract Dense 3D reconstruction from RGB images is a highly ill-posed problem due to occlusions, textureless or reflective surfaces, as well as other challenges. We propose object-level shape priors to address these ambiguities. Towards this goal, we formulate a probabilistic model that integrates multi-view image evidence with 3D shape information from multiple objects. Inference in this model yields a dense 3D reconstruction of the scene as well as the existence and precise 3D pose of the objects in it. Our approach is able to recover fine details not captured in the input shapes while defaulting to the input models in occluded regions where image evidence is weak. Due to its probabilistic nature, the approach is able to cope with the approximate geometry of the 3D models as well as input shapes that are not present in the scene. We evaluate the approach quantitatively on several challenging indoor and outdoor datasets. YouTube pdf suppmat Project Page [BibTex] Share Ulusoy, A. O. , Black, M. J. , Geiger, A. Semantic Multi-view Stereo: Jointly Estimating Objects and Voxels In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) YouTube pdf suppmat Project Page [BibTex] Deep representation learning for human motion prediction and classification Btepage, J. , Black, M. , Kragic, D., Kjellstrm, H. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction. arXiv Project Page [BibTex] Share Btepage, J. , Black, M. , Kragic, D., Kjellstrm, H. Deep representation learning for human motion prediction and classification In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) arXiv Project Page [BibTex] Unite the People: Closing the Loop Between 3D and 2D Human Representations Lassner, C. , Romero, J. , Kiefel, M. , Bogo, F. , Black, M. J. , Gehler, P. V. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) Abstract 3D models provide a common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits in-the-wild. However, depending on the level of detail, it can be hard to impossible to acquire labeled data for training 2D estimators on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits for multiple human pose datasets. Human annotators solely sort good and bad fits. This procedure leads to an initial dataset, UP-3D, with rich annotations. With a comprehensive set of experiments, we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail: our models predict 31 segments and 91 landmark locations on the body. Using the 91 landmark pose estimator, we present state-of-the art results for 3D human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure. We show that UP-3D can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. The data, code and models are available for research purposes. arXiv project/code/data Project Page [BibTex] Share Lassner, C. , Romero, J. , Kiefel, M. , Bogo, F. , Black, M. J. , Gehler, P. V. Unite the People: Closing the Loop Between 3D and 2D Human Representations In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017 , IEEE, Piscataway, NJ, USA, July 2017 (inproceedings) arXiv project/code/data Project Page [BibTex] Method for providing a three dimensional body model Loper, M. , Mahmood, N. , Black, M. July 2017, U.S.~Patent 9,710,964 B2. (misc) Abstract A method for providing a three-dimensional body model which may be applied for an animation, based on a moving body, wherein the method comprises providing a parametric three-dimensional body model, which allows shape and pose variations; applying a standard set of body markers; optimizing the set of body markers by generating an additional set of body markers and applying the same for providing 3D coordinate marker signals for capturing shape and pose of the body and dynamics of soft tissue; and automatically providing an animation by processing the 3D coordinate marker signals in order to provide a personalized three-dimensional body model, based on estimated shape and an estimated pose of the body by means of predicted marker locations. Google Patents MoSh Project [BibTex] Share Loper, M. , Mahmood, N. , Black, M. Method for providing a three dimensional body model July 2017, U.S.~Patent 9,710,964 B2. (misc) Google Patents MoSh Project [BibTex] System and method for simulating realistic clothing Black, M. J. , Guan, P. June 2017, U.S.~Patent 9,679,409 B2 (misc) Abstract Systems, methods, and computer-readable storage media for simulating realistic clothing. The system generates a clothing deformation model for a clothing type, wherein the clothing deformation model factors a change of clothing shape due to rigid limb rotation, pose-independent body shape, and pose-dependent deformations. Next, the system generates a custom-shaped garment for a given body by mapping, via the clothing deformation model, body shape parameters to clothing shape parameters. The system then automatically dresses the given body with the custom- shaped garment. Google Patents pdf [BibTex] Share Black, M. J. , Guan, P. System and method for simulating realistic clothing June 2017, U.S.~Patent 9,679,409 B2 (misc) Google Patents pdf [BibTex] Appealing Avatars from 3D Body Scans: Perceptual Effects of Stylization Fleming, R., Mohler, B. J. , Romero, J. , Black, M. J. , Breidt, M. In Computer Vision, Imaging and Computer Graphics Theory and Applications: 11th International Joint Conference, VISIGRAPP 2016, Rome, Italy, February 27 29, 2016, Revised Selected Papers , pages: 175-196, Springer International Publishing, 2017 (inbook) Abstract Using styles derived from existing popular character designs, we present a novel automatic stylization technique for body shape and colour information based on a statistical 3D model of human bodies. We investigate whether such stylized body shapes result in increased perceived appeal with two different experiments: One focuses on body shape alone, the other investigates the additional role of surface colour and lighting. Our results consistently show that the most appealing avatar is a partially stylized one. Importantly, avatars with high stylization or no stylization at all were rated to have the least appeal. The inclusion of colour information and improvements to render quality had no significant effect on the overall perceived appeal of the avatars, and we observe that the body shape primarily drives the change in appeal ratings. For body scans with colour information, we found that a partially stylized avatar was perceived as most appealing. publisher site pdf DOI [BibTex] Share Fleming, R., Mohler, B. J. , Romero, J. , Black, M. J. , Breidt, M. Appealing Avatars from 3D Body Scans: Perceptual Effects of Stylization In Computer Vision, Imaging and Computer Graphics Theory and Applications: 11th International Joint Conference, VISIGRAPP 2016, Rome, Italy, February 27 29, 2016, Revised Selected Papers , pages: 175-196, Springer International Publishing, 2017 (inbook) publisher site pdf DOI [BibTex] Data-Driven Physics for Human Soft Tissue Animation Kim, M. , Pons-Moll, G. , Pujades, S. , Bang, S., Kim, J., Black, M. J. , Lee, S. ACM Transactions on Graphics, (Proc. SIGGRAPH) , 36(4):54:1-54:12, 2017 (article) Abstract Data driven models of human poses and soft-tissue deformations can produce very realistic results, but they only model the visible surface of the human body and cannot create skin deformation due to interactions with the environment. Physical simulations can generalize to external forces, but their parameters are difficult to control. In this paper, we present a layered volumetric human body model learned from data. Our model is composed of a data-driven inner layer and a physics-based external layer. The inner layer is driven with a volumetric statistical body model (VSMPL). The soft tissue layer consists of a tetrahedral mesh that is driven using the finite element method (FEM). Model parameters, namely the segmentation of the body into layers and the soft tissue elasticity, are learned directly from 4D registrations of humans exhibiting soft tissue deformations. The learned two layer model is a realistic full-body avatar that generalizes to novel motions and external forces. Experiments show that the resulting avatars produce realistic results on held out sequences and react to external forces. Moreover, the model supports the retargeting of physical properties from one avatar when they share the same topology. video paper link (url) Project Page [BibTex] Share Kim, M. , Pons-Moll, G. , Pujades, S. , Bang, S., Kim, J., Black, M. J. , Lee, S. Data-Driven Physics for Human Soft Tissue Animation ACM Transactions on Graphics, (Proc. SIGGRAPH) , 36(4):54:1-54:12, 2017 (article) video paper link (url) Project Page [BibTex] Towards Accurate Marker-less Human Shape and Pose Estimation over Time Huang, Y. , Bogo, F. , Lassner, C. , Kanazawa, A. , Gehler, P. V. , Romero, J. , Akhter, I. , Black, M. J. In International Conference on 3D Vision (3DV) , pages: 421-430, 2017 (inproceedings) Abstract Existing markerless motion capture methods often assume known backgrounds, static cameras, and sequence specific motion priors, limiting their application scenarios. Here we present a fully automatic method that, given multiview videos, estimates 3D human pose and body shape. We take the recently proposed SMPLify method [12] as the base method and extend it in several ways. First we fit a 3D human body model to 2D features detected in multi-view images. Second, we use a CNN method to segment the person in each image and fit the 3D body model to the contours, further improving accuracy. Third we utilize a generic and robust DCT temporal prior to handle the left and right side swapping issue sometimes introduced by the 2D pose estimator. Validation on standard benchmarks shows our results are comparable to the state of the art and also provide a realistic 3D shape avatar. We also demonstrate accurate results on HumanEva and on challenging monocular sequences of dancing from YouTube. Code pdf DOI Project Page [BibTex] Share Huang, Y. , Bogo, F. , Lassner, C. , Kanazawa, A. , Gehler, P. V. , Romero, J. , Akhter, I. , Black, M. J. Towards Accurate Marker-less Human Shape and Pose Estimation over Time In International Conference on 3D Vision (3DV) , pages: 421-430, 2017 (inproceedings) Code pdf DOI Project Page [BibTex] Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs (Best Paper, Eurographics 2017) Marcard, T. V., Rosenhahn, B., Black, M. , Pons-Moll, G. Computer Graphics Forum 36(2), Proceedings of the 38th Annual Conference of the European Association for Computer Graphics (Eurographics) , pages: 349-360 , 2017 (article) Abstract We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables motion capture using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall video pdf Project Page [BibTex] Share Marcard, T. V., Rosenhahn, B., Black, M. , Pons-Moll, G. Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs Computer Graphics Forum 36(2), Proceedings of the 38th Annual Conference of the European Association for Computer Graphics (Eurographics) , pages: 349-360 , 2017 (article) video pdf Project Page [BibTex] ClothCap: Seamless 4D Clothing Capture and Retargeting Pons-Moll, G. , Pujades, S. , Hu, S., Black, M. ACM Transactions on Graphics, (Proc. SIGGRAPH) , 36(4):73:1-73:15, ACM, New York, NY, USA, 2017, Two first authors contributed equally (article) Abstract Designing and simulating realistic clothing is challenging and, while several methods have addressed the capture of clothing from 3D scans, previous methods have been limited to single garments and simple motions, lack detail, or require specialized texture patterns. Here we address the problem of capturing regular clothing on fully dressed people in motion. People typically wear multiple pieces of clothing at a time. To estimate the shape of such clothing, track it over time, and render it believably, each garment must be segmented from the others and the body. Our ClothCap approach uses a new multi-part 3D model of clothed bodies, automatically segments each piece of clothing, estimates the naked body shape and pose under the clothing, and tracks the 3D deformations of the clothing over time. We estimate the garments and their motion from 4D scans; that is, high-resolution 3D scans of the subject in motion at 60 fps. The model allows us to capture a clothed person in motion, extract their clothing, and retarget the clothing to new body shapes. ClothCap provides a step towards virtual try-on with a technology for capturing, modeling, and analyzing clothing in motion. video project_page paper link (url) DOI Project Page Project Page [BibTex] Share Pons-Moll, G. , Pujades, S. , Hu, S., Black, M. ClothCap: Seamless 4D Clothing Capture and Retargeting ACM Transactions on Graphics, (Proc. SIGGRAPH) , 36(4):73:1-73:15, ACM, New York, NY, USA, 2017, Two first authors contributed equally (article) video project_page paper link (url) DOI Project Page Project Page [BibTex] 2016 Creating body shapes from verbal descriptions by linking similarity spaces Hill, M. Q. , Streuber, S. , Hahn, C. A., Black, M. J. , OToole, A. J. Psychological Science , 27(11):1486-1497, November 2016, (article) Abstract Brief verbal descriptions of bodies (e.g. curvy, long-legged) can elicit vivid mental images. The ease with which we create these mental images belies the complexity of three-dimensional body shapes. We explored the relationship between body shapes and body descriptions and show that a small number of words can be used to generate categorically accurate representations of three-dimensional bodies. The dimensions of body shape variation that emerged in a language-based similarity space were related to major dimensions of variation computed directly from three-dimensional laser scans of 2094 bodies. This allowed us to generate three-dimensional models of people in the shape space using only their coordinates on analogous dimensions in the language-based description space. Human descriptions of photographed bodies and their corresponding models matched closely. The natural mapping between the spaces illustrates the role of language as a concise code for body shape, capturing perceptually salient global and local body features. pdf [BibTex] Share 2016 Hill, M. Q. , Streuber, S. , Hahn, C. A., Black, M. J. , OToole, A. J. Creating body shapes from verbal descriptions by linking similarity spaces Psychological Science , 27(11):1486-1497, November 2016, (article) pdf [BibTex] Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image Bogo, F. , Kanazawa, A. , Lassner, C. , Gehler, P. , Romero, J. , Black, M. J. In Computer Vision ECCV 2016 , pages: 561-578, Lecture Notes in Computer Science, Springer International Publishing, October 2016 (inproceedings) Abstract We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we fi rst use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fi t it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art. pdf Video Sup Mat video Code Project Project Page [BibTex] Share Bogo, F. , Kanazawa, A. , Lassner, C. , Gehler, P. , Romero, J. , Black, M. J. Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image In Computer Vision ECCV 2016 , pages: 561-578, Lecture Notes in Computer Science, Springer International Publishing, October 2016 (inproceedings) pdf Video Sup Mat video Code Project Project Page [BibTex] Previous 1 2 3 4 5 6 Next Our goal is to understand the principles of Perception, Action and Learning in autonomous systems that successfully interact with complex environments and to use this understanding to design future systems Latest News Michael Black receives UBC Computer Science Department Alumni Research Award 2018 29 November 2018 Siyu Tang receives DAGM MVTec 2018 Dissertation Award 11 October 2018 Body perception research using virtual reality is this avatar really me? 10 October 2018 Realistic Avatars for the Virtual Zoo 20 June 2018 Links Cyber Valley IMPRS-IS Center for Learning Systems Computer Science at Max Planck linkedIn Twitter YouTube FaceBook Intranet (internal) Confluence Wiki (internal) Contact Us Max Planck Institute for Intelligent Systems Max-Planck-Ring 4 72076 Tbingen Heisenbergstr. 3 70569 Stuttgart Germany info@is.mpg.de For website questions and technical problems please contact: web@is.mpg.de 2019 Max-Planck-Gesellschaft - Imprint | Privacy Policy Sign In We use cookies to improve your website experience. Find out more about our cookies and how to disable them . By continuing, you consent to our use of cookies. Continue 
