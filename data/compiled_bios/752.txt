 Toggle navigation Home News Awards Publications Talks Visits Service Teaching Students Manohar Kaul I received a Ph.D. at the Data-intensive Systems group in Aarhus University, Denmark. I worked under the supervision of Prof. Christian S. Jensen. Prior to my Masters at Uppsala University in Sweden, I worked for nearly 9 years in the I.T industry, primarily at ORACLE Utilities as a Senior Systems Architect. My current research interests include Applied Algebraic Toppology, Topological Data Analysis (TDA), Scalable Machine Learning, and Spatial Databases. Contact Assistant Professor Dept. of Computer Science IIT Hyderabad India mkaul (at) iith (dot) ac (dot) in DBLP , Google Scholar News 12-May-2018: " Solving Partial Assignment Problems using Random Clique Complexes " with Charu and Deepak accepted at ICML 2018 26-Mar-2017: Paper on Fault-Tolerance accepted at TKDE 2017 09-Jul-2016: Awarded VLDB 2016 Travel Fellowship 21-Dec-2015: Paper on Distributed Graph Fault-Tolerance accepted at ICDE 2016 01-Nov-2014: Moved to TU Berlin at DIMA headed by Prof. Volker Markl 07-Jun-2013: Was awarded the Best Paper Award at MDM 2013 . Honors and Awards VLDB 2016 Travel Fellowship MDM 2013 Best Paper Award Outstanding Academic Achievement - Australian Computer Society (ACS) 2000 Most Outstanding Advanced Features - Industrial Project Award Golden Key Society Honor - 1999 Publications Solving Partial Assignment Problems using Random Clique Complexes Charu Sharma, Deepak Nathani, Manohar Kaul International Conference on Machine Learning ( ICML ), 2018 (accepted) [pdf] [abstract] We present an alternate formulation of the partial assignment problem as matching random clique complexes , that are higher-order analogues of random graphs, designed to provide a set of invariants that better detect higher-order structure. The proposed method creates random clique adjacency matrices for each k-skeleton of the random clique complexes and matches them, taking into account each point as the affine combination of its geometric neighborhood. We justify our solution theoretically, by analyzing the algorithms runtime and storage complexity along with the asymptotic behavior of the quadratic assignment problem (QAP) that is associated with the underlying random adjacency matrices. Accuracy experiments on both synthetic and real-world datasets containing severe occlusions and distortions, demonstrate the robustness of our approach. We outperform diverse matching algorithm by a significant margin. On Fault Tolerance for Distributed Iterative Dataflow Processing Chen Xu, Markus Holzemer, Manohar Kaul, Juan Soto, Volker Markl Transactions on Knowledge and Data Engineering ( TKDE ), 2017 [pdf] [abstract] Large-scale graph and machine learning analytics widely employ distributed iterative processing. Typically, these analytics are a part of a comprehensive workflow, which includes data preparation, model building, and model evaluation. General-purpose distributed dataflow frameworks execute all steps of such workflows holistically. This holistic view enables these systems to reason about and automatically optimize the entire pipeline. Here, graph and machine learning analytics are known to incur a long runtime since they require multiple passes over the data until convergence is reached. Thus, fault tolerance and a fast-recovery from any intermittent failure is critical for efficient analysis. In this paper, we propose novel fault-tolerant mechanisms for graph and machine learning analytics that run on distributed dataflow systems. We seek to reduce checkpointing costs and shorten failure recovery times. For graph processing, rather than writing checkpoints that block downstream operators, our mechanism writes checkpoints in an unblocking manner that does not break pipelined tasks. In contrast to the conventional approach for unblocking checkpointing (e.g., that manage checkpoints independently for immutable datasets), we inject the checkpoints of mutable datasets into the iterative dataflow itself. Hence, our mechanism is iteration-aware by design. This simplifies the system architecture and facilitates coordinating checkpoint creation during iterative graph processing. Moreover, we are able to rapidly rebound, via confined recovery, by exploiting the fact that log files exist locally on healthy nodes and managing to avoid a complete recomputation from scratch. In addition, we propose replica recovery for machine learning algorithms, whereby we employ a broadcast variable that enables us to quickly recover without having to introduce any checkpoints. In order to evaluate our fault tolerance strategies, we conduct both a theoretical study and empirical experimental analyses using Apache Flink and discover that they outperform blocking checkpointing and complete recovery Elementary, dear Watson! Manohar Kaul Conference on Innovative Data Systems Research ( CIDR ), 2017 (vision abstract) [pdf] [abstract] In "The Sign of Four", Sherlock Holmes makes some very startling inferences about Watsons whereabouts by informing Watson that he was at the Wigmore Street Post Office earlier that morning. On further inquiry by Watson, Holmes explains that he combined (joined) the observation that Watsons shoes had a reddish mud on their instep", with the fact that just opposite the Wigmore Post Office the pavement was freshly dug up to expose a similar reddish mud and it was so positioned that it would be challenging to enter the office without treading into the mud". Contrary to the popular belief that this is deduction, this method of reasoning is actually an example of abductive inference. Abduction begins from the facts observed and then seeks the simplest hypothesis (Occams razor") that best explains the facts, while deduction finds data to support a hypothesis. Given the big data challenge that we presently face, is it then possible to utilize an abductive model (effect to cause reasoning) to find the best explanation for the observations, as opposed to the traditional method of forming hypotheses and testing them with observations (cause to effect reasoning)? Can our databases extend our understanding of this data automatically by inferring explanations from incomplete observations? Sharing Hash Codes for Multiple Purposes Wikor Pronobis, Danny Panknin, Johannes Kirschnick, Vignesh Srinivasan, Wojciech Samek, Volker Markl, Manohar Kaul, Klaus-Robert Mller, Shinichi Nakajima ArXiv.org, 2016 [pdf] [abstract] Locality sensitive hashing (LSH) is a powerful tool for sublinear-time approximate nearest neighbor search, and a variety of hashing schemes have been proposed for different similarity measures. However, hash codes significantly depend on the similarity, which prohibits users from adjusting the similarity at query time. In this paper, we propose multiple purpose LSH (mp-LSH) which shares the hash codes for different similarities. By using vector/code augmentation and cover tree techniques, our mp-LSH supports L2, cosine, and inner product similarities, and their corresponding weighted sums, where the weights can be adjusted at query time. It also allows us to modify the importance of pre-defined groups of features. Thus, mp-LSH enables us, for example, to retrieve similar items to a query with the user preference taken into account, to find a similar material to a query with some properties (stability, utility, etc.) optimized, and to turn on or off a part of multi-modal information (brightness, color, audio, text, etc.) in image/video retrieval. We theoretically and empirically analyze the performance of three variants of mp-LSH, and demonstrate their usefulness on several real-world data sets. Improving Data Quality by Leveraging Statistical Relational Learning Larysa Visengeriyeva, Alan Akbik, Manohar Kaul International Conference on Information Quality ( ICIQ ), 2016 [pdf] [abstract] Digitally collected data suffers from many data quality issues, such as duplicate, incorrect, or incomplete data. A common approach for counteracting these issues is to formulate a set of data cleaning rules to identify and repair incorrect, duplicate and missing data. Data cleaning systems must be able to treat data quality rules holistically, to incorporate heterogeneous constraints within a single routine, and to automate data curation. We propose an approach to data cleaning based on statistical relational learning (SRL). We argue that a formalism - Markov logic - is a natural fit for modeling data quality rules. Our approach allows for the usage of probabilistic joint inference over interleaved data cleaning rules to improve data quality. Furthermore, it obliterates the need to specify the order of rule execution. We describe how data quality rules expressed as formulas in first-order logic directly translate into the predictive model in our SRL framework. Efficient Fault-tolerance for Iterative Graph Processing on Distributed Dataflow Systems Chen Xu, Markus Holzemer, Manohar Kaul, Volker Markl IEEE International Conference on Data Engineering ( ICDE ), 2016 [pdf] [abstract] Real-world graph processing applications in many instances require combining the graph data with tabular data or making the graph processing a part of a larger analytics pipeline. General-purpose distributed dataflow frameworks typically execute such pipelines while analyzing the entire pipeline in a holistic manner to further optimize the processing. A majority of big graph processing algorithms are iterative in nature and incur a long runtime, therefore it becomes all the more necessary to tolerate and recover quickly from any intermittent failures. In this work, we propose an efficient fault-tolerance mechanism for iterative graph processing on distributed data-flow systems with the objective to reduce the checkpointing cost and failure recovery time. Rather than writing checkpoints that block downstream operators, we write checkpoints in an unblocking manner. Also, in comparison to the typical unblocking checkpointing approach of managing checkpoints independently, we inject the checkpoints into the dataflow itself. It not only inherits the advantage of a low execution latency without breaking the pipelined tasks, but simplifies the system design to coordinate the checkpoint writing. Further, we achieve speedier recovery, i.e., confined recovery, by using the local log files on each node to avoid a complete re- computation from scratch. Our theoretical studies as well as experimental analysis on Flink give further insight into our fault- tolerance strategies and show that they are more efficient than blocking checkpointing and complete recovery for iterative graph processing on dataflow systems. New Lower and Upper Bounds for Shortest Distance Queries on Terrains Manohar Kaul, Raymond Chi-Wing Wong, Christian S. Jensen Proceedings of Very Large Databases ( PVLDB ), 2015 [pdf] [abstract] The increasing availability of massive and accurate laser data enables the processing of spatial queries on terrains. As shortest-path computation, an integral element of query processing, is inherently expensive on terrains, a key approach to enabling efficient query processing is to reduce the need for exact shortest-path computation in query processing. We develop new lower and upper bounds on terrain shortest distances that are provably tighter than any existing bounds. Unlike existing bounds, the new bounds do not rely on the quality of the triangulation. We show how use of the new bounds speeds up query processing by reducing the need for exact distance computations. Speedups of of nearly an order of magnitude are demonstrated empirically for well-known spatial queries. R-Apriori : An Efficient Apriori based Algorithm on Spark Sanjay Rathee, Manohar Kaul, Arti Kashyap PIKM , Ph.D. student workshop at CIKM , 2015 [pdf] [abstract] Association rule mining remains a very popular and effective method to extract meaningful information from large datasets. It tries to find possible associations between items in large transaction based datasets. In order to create these associations, frequent patterns have to be generated. The "Apriori" algorithm along with its set of improved variants, which were one of the earliest proposed frequent pattern generation algorithms still remain a preferred choice due to their ease of implementation and natural tendency to be parallelized. While many efficient single-machine methods for Apriori exist, the massive amount of data available these days is far beyond the capacity of a single machine. Hence, there is a need to scale across multiple machines to meet the demands of this ever- growing data. MapReduce is a popular fault-tolerant framework for distributed applications. Nevertheless, heavy disk I/O at each MapReduce operation hinders the implementation of efficient iterative data mining algorithms, such as Apriori, on MapReduce platforms. A newly proposed in-memory distributed dataflow platform called Spark overcomes the disk I/O bottlenecks in MapReduce. Therefore, Spark presents an ideal platform for distributed Apriori. However, in the implementation of Apriori, the most computationally expensive task is the generation of candidate sets having all possible pairs for singleton frequent items and comparing each pair with every transaction record. Here, we propose a new approach which dramatically reduces this computational complexity by eliminating the candidate generation step and avoiding costly comparisons. We conduct in-depth experiments to gain insight into the effectiveness, efficiency and scalability of our approach. Our studies show that our approach outperforms the classical Apriori and state-of-the-art on Spark by many times for different datasets. Terrain-Toolkit: A Multi-Functional Tool for Terrain Data (Demo) Qi Wang, Manohar Kaul, Cheng Long, Raymond Chi-Wing Wong Proceedings of Very Large Databases ( PVLDB ), Sep 1-5 2014, Hangzhou, China [pdf] [abstract] Terrain data is becoming increasingly popular both in industry and in academia. Many tools have been developed for visualizing terrain data. However, we find that (1) they usually accept very few data formats of terrain data only; (2) they do not support terrain simplification well which, as will be shown, is used heavily for query processing in spatial databases; and (3) they do not provide the surface distance operator which is fundamental for many applications based on terrain data. Motivated by this, we developed a tool called Terrain-Toolkit for terrain data which accepts a comprehensive set of data formats, supports terrain simplification and provides the surface distance operator. Multi-Cost Optimal Route Planning Under Time-Varying Uncertainty Bin Yang, Chenjuan Guo, Christian S. Jensen, Manohar Kaul, Shuo Shang IEEE International Conference on Data Engineering ( ICDE ), Mar 31 - Apr 4 2014, Chicago, USA [pdf] [abstract] Different uses of a road network call for the consideration of different travel costs: in route planning, travel time and distance are typically considered, and green house gas (GHG) emissions are increasingly being considered. Further, costs such as travel time and GHG emissions are time-dependent and uncertain. To support such uses, we propose techniques that enable the construction of a multi-cost, time-dependent, uncertain graph (MTUG) model of a road network based on GPS data from vehicles that traversed the road network. Based on the MTUG, we define optimal routes that consider multiple costs and time-dependent uncertainty, and we propose efficient algorithms to retrieve optimal routes for a given source-destination pair and a start time. Empirical studies with three road networks in Denmark and a substantial GPS data set offer insight into the design properties of the MTUG and the efficiency of the optimal route algorithms. Finding Shortest Paths on Terrains by Killing Two Birds with One Stone Manohar Kaul, Raymond Chi-Wing Wong, Bin Yang, Christian S. Jensen Proceedings of Very Large Databases ( PVLDB ), Sep 1-5 2014, Hangzhou, China [pdf] [abstract] With the increasing availability of terrain data, e.g., from aerial laser scans, the management of such data is attracting increasing attention in both industry and academia. In particular, spatial queries, e.g., k-nearest neighbor and reverse nearest neighbor queries, in Euclidean and spatial network spaces are being extended to terrains. Such queries all rely on an important operation, that of finding shortest surface distances. However, shortest surface distance computation is very time consuming. We propose techniques that enable efficient computation of lower and upper bounds of the shortest surface distance, which enable faster query processing by eliminating expensive distance computations. Empirical studies show that our bounds are much tighter than the best-known bounds in many cases and that they enable speedups of up to 43 times for some well-known spatial queries. Using Incomplete Information for Complete Weight Annotation of Road Networks Bin Yang, Manohar Kaul, Christian S. Jensen Transactions on Knowledge and Data Engineering ( TKDE ) [pdf] Building Accurate 3D Spatial Networks to Enable Next Generation Intelligent Transportation Systems ( Best Paper Award ) Manohar Kaul, Bin Yang, Christian S. Jensen IEEE Proceedings of International Conference on Mobile Data Management ( MDM ), June 3-6 2013, Milan, Italy [pdf] [abstract] The use of accurate 3D spatial network models can enable substantial improvements in vehicle routing. Notably, such models enable eco-routing, which reduces the environmental impact of transportation. We propose a novel filtering and lifting framework that augments a standard 2D spatial network model with elevation information extracted from massive aerial laser scan data and thus yields an accurate 3D model. We present a filtering technique that is capable of pruning irrelevant laser scan points in a single pass, but assumes that the 2D network fits in internal memory and that the points are appropriately sorted. We also provide an external-memory filtering technique that makes no such assumptions. During lifting, a triangulated irregular network (TIN) surface is constructed from the remaining points. The 2D network is projected onto the TIN, and a 3D network is constructed by means of interpolation. We report on a large-scale empirical study that offers insight into the accuracy, efficiency, and scalability properties of the framework. EcoMark: Evaluating Models of Vehicular Environmental Impact Chenjuan Guo, Yu Ma, Bin Yang, Christian S. Jensen, Manohar Kaul Proceedings of ACM SIGSPATIAL GIS , Nov 7-9, 2012, Redondo Beach, CA, USA [pdf] [abstract] The reduction of greenhouse gas (GHG) emissions from transportation is essential for achieving politically agreed upon emissions reduction targets that aim to combat global climate change. So-called eco-routing and eco-driving are able to substantially reduce GHG emissions caused by vehicular transportation. To enable these, it is necessary to be able to reliably quantify the emissions of vehicles as they travel in a spatial network. Thus, a number of models have been proposed that aim to quantify the emissions of a vehicle based on GPS data from the vehicle and a 3D model of the spatial network the vehicle travels in. We develop an evaluation framework, called EcoMark, for such environmental impact models. In addition, we survey all eleven state-of-the-art impact models known to us. To gain insight into the capabilities of the models and to understand the effectiveness of the EcoMark, we apply the framework to all models. Frequent Route Based Continuous Moving Object Location and Density Prediction on Road Networks Gyozo Gidofalvi, Manohar Kaul, Christian Borgelt, Torben Bach Pedersen Proceedings of ACM SIGSPATIAL GIS , Nov 1-4, 2011, Chicago, IL, USA [pdf] [abstract] Emerging trends in urban mobility have accelerated the need for effective traffic prediction and management systems. The present paper proposes a novel approach to using continuously streaming moving object trajectories for traffic prediction and management. The approach continuously performs three functions for streams of moving object positions in road networks: 1) management of current evolving trajec tories, 2) incremental mining of closed frequent routes, and 3) prediction of near-future locations and densities based on 1) and 2). The approach is empirically evaluated on a large real-world data set of moving object trajectories, originating from a fleet of taxis, illustrating that detailed closed frequent routes can be efficiently discovered and used for prediction. Intelligent Packet Shaper to avoid Network Congestion for Improved Streaming Video Quality at Clients Manohar Kaul, R. Khosla, Y. Mitsukura Proceedings of IEEE International Symposium on Computational Intelligence in Robotics and Automation ( CIRA ), July 16-20, 2003, Kobe, Japan [pdf] [abstract] This paper proposes a traffic shaping algorithm based on neural networks, which adapts to a network over which streaming video is being transmitted. The purpose of this intelligent shaper is to eradicate traffic congestion and improve end-user video quality. It posseses the capability to predict, to a very high level of accuracy, a state of congestion based upon the training data collected about the network's behaviour. Talks Jun 2018: Solving Partial Assignment Problems using Random Clique Complexes at Topology Seminar, Shinshu University, Japan. Sep 2017: Works using Geometry and Algebraic Topology at IBM Research, New Delhi, India. Sep 2016: New Lower and Upper Distance Bounds for Shortest Distance Queries on Terrains at VLDB 2016, New Delhi, India. May 2016: Moving Object Databases at Big Data Workshop held at IIT Mandi, India. Nov 2015: Distance Bounds for Spatial Queries on Polyhedral Surfaces at Department of Computer Science and Engineering, IIT Bombay, India Apr 2015: Scalable Machine Learning Library Related Research at Amazon, Berlin, Germany. Apr 2014: Finding Better Distance Bounds for Shortest Surface Paths on 3D Terrains at Department of Computer Science and Engineering, Chalmers University of Technology, Sweden. June 2013: Building Accurate 3D Spatial Networks to Enable Next Generation Intelligent Transportation Systems at IEEE International Conference on Mobile Data Management (MDM) 2013, Milan, Italy. Visiting Positions May'18 - Jun'18: Visiting Foreign Researcher to Dept. of Mathematics, Shinshu University, Matsumoto, Japan. May17 Sep17: Visiting Foreign Researcher to Institute of Statistical Mathematics (ISM), Tokyo, Japan. Aug13 Mar14: Visiting Ph.D. to Department of Computer Science and Engineering (CSE), HKUST, Hong Kong. Service Reviewer: TKDE'2017, MDM'2018, NIPS'2018 External Reviewer: CIKM 2013, ICDE 2014, EDBT 2014. Session Chair: Conformal Prediction for Reliable Machine Learning (CPRML) 2015. Teaching CS3563: Introduction to DBMS-II @IITH ID1303: Introduction to Programming in C @IITH, @IIT-Bhilai CS3010: Database Theory @IITH Indexing Spatial Data (Elective Course @ IIT-H) Big Data Analytics Seminar (@ TU Berlin) Contract-Based Programming in Q2 (@ Aarhus) Web Technology in Q3 (Double TA) (@ Aarhus) Students Ph.D (in progress): Charu Sharma Masters (completed): Pratik Shukla (@ Qualcomm), Thesis: "Text Clustering using Graph Kernels" Varun Mishra (@ Flikart), Thesis: "Document Simplicial Complexes" 
