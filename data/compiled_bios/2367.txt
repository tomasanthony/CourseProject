I graduated in 1977 with First Class Honours in Pure Mathematics and Physics from the University of Exeter. Following that I studied for a PhD in Geomagnetism underGeraint Rosser at the same university.During my career I have worked at three internationally leading independent research laboratories:I greatly enjoyed my seven years at the National Physical Laboratory, working with some dedicated and highly talented scientists. At the end of those seven years I had achieved an order of magnitude improvement of the repeatability of the josephson voltage standard, with also a significantly improved ease of use and reduction in liquid helium consumption when performing the measurements. However, although quantum metrology is a fascinating area, I could not see any potential for significant advances in the underlying physics, so I went for a change.I started by working for two years at the University of Surrey within their Formal Methods of Software Engineering Group. What I actually did was more in the line of automated theorem proving, spending my time working with Ron Knott on using Prolog to automatically generate behaviours implied by system specifications written in set theory and formal logic. Specification verification tools are way ahead if that now, but it was a fun introduction to the challenges of building automated reasoning systems. Incidentally, Ron is still maintaining the longest running web site on recreational mathematics.I moved on to the Imperial Cancer Research Fund (now Cancer Research UK) in 1989 to work on automated decision making and diagnosis for health care. We were typically looking at non-classical logics and symbolic reasoning in order to make the "arguments" supporting candidate decisions explicit. This lead to some of the earliest published work on computational models of argumentation; an area of research that has grown significantly since those days. Our own work defined a category theoretic semantics for our model of argumentation. We had very extensive collaborations across the research community in models of decision making and reasoning under uncertainty, and my introductory (and very successful at the time) text book with Dominic Clark onRepresenting Uncertainty, anAI Approachspun out of that.After seven years at ICRF it was, of course, time to move on again. I took up a post at Philips Research Laboratories, Redhill, to work as a Principal Scientist within Paul Gough's Software Engineering and Applications research group. Sounds a bit like a change of topic, but was not in fact. Machine automation and reasoning were still key here: "robots" to automate the testing of consumer products; semantic reasoning to automatically generate links between entities in hierarchies of documents; bayesian models for software quality assessment. The last of these initiated a long standing collaboration between myself, and Norman Fenton and Martin Neil at City University and Agena Ltd.2001 saw me moving to a joint appointment with Philips and the University of Surrey, with the aim of strengthening collaboration on computer science research between the two organisations. Sadly, the end of 2001 saw the beginning of the end of Philips Research's once world class research laboratory at Redhill. Promotion to Senior Principal Scientist was a short lived pleasure, but a climax to a wonderful time at Philips. As the research laboratory was gradually wound down, so I progressively moved to full time at the University of Surrey from the beginning of 2003.Of course, a big difference between industry-focused research laboratories and an academic department is that in academia one is not constrained to work within defined remits of a specific industry sector (although both NPL and Philips Research at their best did indeed support a level of blue sky research). So the move to academia did of course give me more freedom in my trajectory. Despite the recent high-profile successes in AI, I believe there are some major challenges. Two key ones for me are: explainability/accountability of AI supported decision making; and, bias.Addressing the former issue has been a major influencer of the approaches I have taken. Going back to our work on toxicological risk prediction with Lhasa Ltd, for the pharmaceutical industry an ability to classify novel chemical compounds according to potential risk classes had little value unless the reasons ("arguments") for thoseclassifications were made explicit to the analysts. Moving on to more recent work, my work with Nick Ryman-Tubb during his PhD and subsequently is showing that 20 years' research on fraud detection in academia has not resulted in any significant advances when assessed according to industrially relevant metrics [Ryman-Tubb, Krause and Garn, 2018]. We have also shown <<citation pending>>that unless there are significant changes to the regulatory environment, the financial sector will simply continue to consider the current level of fraud as an acceptable cost of business - with no account being taken that fraud is in effect a multi-billion dollar industry that is funding terrorism and organised crime. The research challenge is to build AI/data analytic tools that help us gain understanding of the fraud vectors themselves, and also explicate the real impact of this level of crime to help facilitate change to the status quo.The issue of bias is pernicious in AI. The problem here is that, obviously, the models are only as good as the data they are trained on. Never mind the irritation of an e-commerce recommending you products that are way off topic for your specific personality. More critical is the potential risk for a health care advisor to judge a health care scenario on the basis of a data set that has been obtained from a predominantly middle class western population. Within that population there will be ethnic groups that will have had little representation within the training sets. Even worse, there may be pressure to apply such a model on a global basis to populations that have absolutely no representation in the training of the model.This is a real challenge in academic research as it is so hard to spend the length of time needed to collate, gain regulatory approval and quality check high quality data on a global basis. However, we are making progress on this and for the last two or three years I have been supporting Lilian Tang's work on diabetic retinopathy in collating extensive retinal image sets from disparate regions across the globe [Lutfiah Al Turk, Paul Krause, Su Wang, Hend Alsawadi, Abdulrahman Zaid Alshamrani, Tunde Peto, Andrew Bastawrous, and Hongying Lilian Tang, Automated Progression Analysis Across Three Nations, to be submitted].Deep Learning has had a lot of successes.There is so much hype around it that it almost seems that the public eye is equating deep learning with AI. But simply thinking that throwing massive data sets at smart algorithms will generate knowledge is a big mistake. Our group's work on diabetic retinopathy, the projects mentioned above, and others, support the conviction that we must still support human-machine co-creation of knowledge. Big data sets, and smart algorithms are necessary, but they are not sufficient for the generation of machines that demonstrate "intelligent" problem solving. Now, working collectively within the NICE group in the Department of Computer Science here at Surrey, we are building out a suite of capabilities that will continue to help us build machines that cancommunicate and collaboratewith human beings to solve complex problems. Consider, for example, the urgent need to gain deeper understanding of the links between biodiversity and the provision of ecosystem services.WithAlirezaTamaddoni Nezhad, we have capability for mining ecological networks using Inductive Logic Programming and Meta-heuristic reasoning. ThroughSotiris Moschoyiannis, we have an emerging toolset for the identification ofthe system levers or drivers which have high influence on the overall system behaviour.Yunpeng Linow strengthens our Bayesian analytic capability; still a powerful method for ensuring assumptions are correctly captured when reasoning with complex data sets.Andr Grning gives further support on techniques for driving dynamical systems into preferred states, perhaps using reinforcement learning to identify interventions to transition a degraded ecological network into one offering enhanced functionality and resilience.I fundamentally do not believe we can learn how to maintain a human presence in the stunningly beautiful biosphere that we are blessed with, without a richer suite of tools from AI -- and ones with which we can co-create knowledge and understanding. We won't survive on this world without them. But, of course, the world may well survive without us...03.02.16Read moreThe MindBeat software was launched in June 2012 as part of an electronic theatre production of Peter Handke's 'Offending the Audience'. The five Surrey academics played the parts remotely by feeding Handke's text onto the Mindbeat website as part of a three-day durational performance. An open audience was then invited to interact with the play's five voices by sitting at one of five iMac stations set up in the studio space. These five computer monitors showed the text broken down into coloured square patterns. The audience could open the text by clicking onto the coloured squares, which would reveal the short text or beat. They could then add a comment or thought to the original text. The audience's participation produced almost 500 additional beats, creating an alternative version to the Handke script. The Mindbeat software visualised this ideation as a complex pattern of coloured squares.The installation featured generative video and generative electronic music played live throughout the entire three-days. Using the colour and shape patterns of the ideas-exchange as their score, the musicians shared the software visualisation as a basis for their durational sonic improvisation.The ability to correctly detect the location and derive the contextual information where a concept begins to drift is essential in the study of domains with changing context. This paper proposes a Top-down learning method with the incorporation of a learning accuracy mechanism to efficiently detect and manage context changes within a large dataset. With the utilisation of simple search operators to perform convergent search and JBNC with a graphical viewer to derive context information, the identified hidden context are shown with the location of the disjoint points, the contextual attributes that contribute to the concept drift, the graphical output of the true relationships between these attributes and the Boolean characterisation which is the context.
