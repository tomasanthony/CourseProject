 Photo by: Dave Burbank Kenneth P. Birman N. Rama Rao Professor of Computer Science 435 Gates Hall, Cornell University Ithaca, New York 14853 W: 607-255-9199; M: 607-227-0894; F: 607-255-9143 Email: ken@cs.cornell.edu CV: Jan 2019 Follow @KenBirman . My blog is here . Current Research ( full publications list ). Derecho. This project looks at ways of leveraging remote DMA (RDMA) technologies to move large data objects at wire speeds, and modern storage technologies to persist data. Recently we ported Derecho to run over TCP too (not quite as fast, but we still set records). The download site is https://github.com/Derecho-Project . The basic idea behind Derecho is to build the whole system as a compositional structure in which individual subsystems (data movement, higher-level stability detection, membership) all run over streaming data flows that are as asynchronous as possible. Yet we also wanted to offer Paxos-based guarantees. The trick to doing all this turned out to center on reimplementing one of the first Paxos-capable protocols I ever created: virtual synchrony and a version of the virtual synchrony "abcast" protocol, both of which date back to 1987 (in fact we had an even earlier version in 1985). So I've worked on this one protocol, on and off, for 31+ years! Derecho is a "retargetting" of that early protocol, structured to optimize for high-speed data streams yet without changing the underlying logic. Interestingly, with this restructuing, we can prove that Derecho is an optimal version of Paxos, using a theory by Shraer and Keidar. The 1987 work was very far from optimal in that sense. But optimal or not, I doubt that Derecho will be the last word on Paxos! The best way to learn about the system is to read our paper here (the paper has been accepted by ACM Transactions on Computer Systems, and will appear in early 2019). Slides from a recent talk are here . The student leads on this work are Sagar Jha, Matt Milano and Edward Tremel. My colleagues Weijia Song and Robbert van Renesse have also been big contributors. Freeze Frame File System. We also have a cool new file system for real-time applications. It soaks up updates from time-synchronized sensors or other sources and captures the data into an in-memory data structure. Then you can read this data at any instant in time that you wish, with very good temporal precision and also with logical consistency guarantees. We attached our new file system to Spark and it actually outperforms the Berkeley Spark+HDFS setup, and we think we can also support the widely popular Kafka API, which would make the solution immediately useful to two large user communities. So our vision is that perhaps you capture real-time data from some kind of IoT setting, or from smart cars, or the smart grid. Then you flag "events of interest" for instant analysis, and the analytic program (running on Hadoop or on MPI) might want to visit snapshots of the past -- maybe lots of them, at fine temporal resolution. We can materialize that data instantly for you, on demand, replicate it onto a cluster of nodes running Spark, and give all sorts of guarantees... a very cool new option! The plan is for FFFS to merge into Derecho: FFFSv2 will simple "be" Derecho, but will be fully back-compatible with FFFSv1. The two lead authors on this work are Weijia Song and Theo Gkountouvas. Cloud computing teaching tool: Vsync . Before we built Derecho, my main focus was on a C# version of the same kind of system, called Vsync. But Derecho is so much faster than these days, we focus entirely on it. Vsync would be a super teaching tool, though, because it is very well documented and has training videos and other learner-friendly resources. We plan to bring those online in coming months for Derecho, too. For a little while we called this system Isis2, until the terrorists preempted that name. Using all of this technology for IoT applications, notably the Smart Electric Power Grid. Under a grant from the Department of Energy NODES program and with further support from NSF and DARPA, were applying our insights concerning high assurance cloud computing and Vsync to the challenge of controlling the smart power grid. We're currently working to transition the GridCloud system (we built it using Vsync but also incorporating best-of-breed electric power technologies from our colleagues at Washington State University in Pullman) for experimental use by the New England and New York ISOs and NYPA, a major TO. Beyond the smart grid, we believe that this style of embedded cloud-hosted real-time system could have many other applications. Some of the more exciting components of GridCloud include CloudMake, a new management tool, and the real-time indexed file system mentioned above, for capturing high-rate IoT data streams. Theo Gkountovas leads on CloudMake, and he and Weijia Song have collaborated on the file system. Teaching: My fall course, cs6465 , is a PhD-oriented class that looks at "edge computing" in modern datacenters (the term refers to the idea that the cloud might make decisions or initiate actions as soon as data reaches a cloud datacenter node, rather than being stored and processed later in batches).The push to shift AI/ML from the core to the edge is changing the game for systems infrastructure, and this exposes a wide range of research opportunities both in AI/ML for real-time and in systems. Examples arise in smart highways and homes, smart power grids. A recent interest at Cornell is digital agriculture, and we may drill down on that just to have a focus on something concrete and novel. My spring 2019 course, cs5412 , will be an MEng-oriented treatment of cloud computing and smart IoT systems, again with an emphasis on edge issues (we do look at big data issues too, but the edge creates demanding response deadlines and real-time consistency puzzles and we spend a lot of time on those). The web page for spring 2019 isn't yet available, but should go online in November 2018. The course will cover similar topics to what we looked at in spring 2018, except that through a deal with Microsoft, we might have people working on projects that focus on aspects of "digital agriculture", where people build apps and infrastructure to assist farmers in doing smarter farming, for example by selecting the perfect type of seeds for a particular plot (and varying the choice throughout the field), or by applying only the exact amount of fertilizer actually needed, or just the minimal amount of pesticides and only where the insects are causing damage. You can look for "Farmbeats" to learn more about this Microsoft product area. Naturally, because Farmbeats runs on Azure, we'll mostly be focused on Azure for examples from real cloud platforms. The head of the Farmbeats project will be a guest speaker early in the semester. Please note that both courses are open without any restrictions for graduate students at the right level, but both require permission for students from outside CS. Just reach out by email if you need permission and we can talk about your background and whether you could do well in these classes. Don't just assume you won't be allowed to enroll... Video links: An updated version of a talk I gave at SmartMemory on our new RDMA research (Derecho and the Freeze Frame File System) can be found here . SOSP '15 History Day talk on fault-tolerance and consistency, the CATOCS controversy, and the modern-day CAP conjecture. My video is here and an accompanying essay is here . Robbert van Renesse and me discussing how we got into this area of research: here . Instruction videos about learning to use the Vsync library (previously known as Isis2): On the documentation page here or on YouTube (tagged "Vsync"). I'm planning to revise these into a series of short 10-minute segments. We also plan to do a set of short segments on using Derecho. My Textbook (last revised in 2012): Guide to Reliable Distributed Systems: Building High-Assurance Applications and Cloud-Hosted Services. Click here to get to my spring 2018 course, which has slide sets and other materials associated with the book. You are welcome to use these in your own courses if you like. The 2018 slide set is quite new and was one of the outcomes of my 2016-2017 sabbatical during which I visited widely and hopefully, came home with an updated appreciation of the contemporary perspectives seen in industry. But this means that by now, I've departed significantly from the treatment in the book; earlier slide sets that are closer to the book treatment can be found in http://www.cs.cornell.edu/courses/cs5412/XXXXsp, where XXXX would be the year: 2012, 2014, 2014, 2015, 2016. There was no 2013 or 2017 offering. The course is not an easy one to teach because the material evolves at a breathtaking pace, which is why I keep revising the slides. Natually, this also means that the book is already out of date. I don't have the time to revise it, right now. Older work. I've really worked in Cloud Computing for most of my career, although it obviously wasn't called cloud computing in the early days. As a result, our papers in this area date back to 1985. Some examples of mission-critical systems on which my software was used in the past include the New York Stock Exchange and Swiss Exchange, the French Air Traffic Control system, the AEGIS warship and a wide range of applications in settings like factory process control and telephony. In fact, every stock quote or trade on the NYSE from 1995 until early 2006 was reported to the overhead trading consoles through software I personally implemented - a cool (but also scary) image, for me at least! During the ten years this system was running, many computers crashed during the trading day, and many network problems have occurred - but the design we developed and implemented has managed to reconfigure itself automatically and kept the overall system up, without exception. They didn't have a single trading disruption during the entire period. As far as I know, the other organizations listed above have similar stories to report. Today, these kinds of ideas are gaining "mainstream" status. For example, IBM's Websphere 6.0 product includes a multicast layer used to replicate data and other runtime state for high-availability web service applications and web sites. Although IBM developed its own implementation of this technology, we've been told by the developers that the architecture was based on Cornell's Horus and Ensemble systems, described more fully below. The CORBA architecture includes a fault-tolerance mechanism based on some of the same ideas. And we've also worked with Microsoft on the technology at the core of the next generation of that company's clustering product. So, you'll find Cornell's research not just on these web pages, but also on web sites worldwide and in some of the world's most ambitious data centers and high availability computing systems. In fact we still have very active dialogs with many of these companies: Cisco, IBM, Intel, Microsoft, Amazon, and others. An example of a more recent dialog is this: a few years ago worked with Cisco to invent a new continuous availability option for their core Internet routers, the CRS-1 series. You can read about this work here . My group often works with vendors and industry researchers. We maintain a very active dialog with the US government and military on research challenges emerging from a future generation communication systems now being planned by organizations like the Air Force and the Navy. We've even worked on new ways of controlling the electric power grid, but not in time to head off the big blackout in 2003! Looking to the future, we are focused on needs arising in financial systems, large-scale military systems, and even health-care networks. (In this connection, I should perhaps mention that although we do get research support from the government and the US military, none of our research is classified or even sensitive, and all of it focuses on widely used commercial standards and platforms. Most of our software is released for free, under open source licenses.) I'm just one of many members of a group in this area at Cornell. My closest colleagues and co-leaders of the group are Robbert van Renesse and Hakim Weatherspoon. But the systems group is very strong and broad right now, and the three of us have great relationships and collaborations with many other systems faculty here at Cornell (both in the systems area within CS, but also folks in ECE where we have great ties, MAE, IS, and down in New York City, where a few faculty are members of our fast-growing New York City Technology "outpost" on Roosveldt Island. Four generations of reliable distributed systems research! Overall, our group has developed three generations of technology and is now working on a fourth generation system: The Isis Toolkit, developed mostly during 1987-1993, the Horus system, developed starting in 1990 until around 1995, the Ensemble system, 1995-1999. Right now we're developing a number of new systems including Isis2, Gradient, and the reliable TCP solution mentioned above, and working with others to integrate those solutions into settings where reliability, security, consistency and scalability are make-or-break requirements. Older Research web pages: Live Objects, Quicksilver, Maelstrom, Ricochet and Tempest projects Ensemble project Horus project Isis Toolkit (really old stuff! This is from the very first version of Isis). A collection of papers on Isis, edited by myself with Robbert van Renesse, may still be available -- it was called Reliable Distributed Computing with the Isis Toolkit and was in the IEEE Press Computer Science series. Graduate Studies in Computer Science at Cornell: At this time of the year, we get large numbers of inquiries about our PhD program. I want to recommend that people interested in the program not contact faculty members like me directly with routine questions like "can your research group fund me". As you'll see from the web page, Cornell does admissions by means of a committee, so individual faculty members don't normally play a role. This is different from many other schools -- I realize that at many places, each faculty member admits people into her/his own group. But at Cornell, we admit you first, then you come here, and then you affiliate with a research group after a while. Funding is absolutely guaranteed for people in the MS/PhD program during the whole time they are at Cornell. On the other hand, students in the MEng program generally need to pay their own way. Obviously, some people have more direct, specific questions, and there is no problem sending those to me or to anyone else. But as for the generic "can I join your research group?" the answer is that while I welcome people into the group if they demonstrate good ideas and talent in my area, until you are here and take my graduate course and spend time talking with me and my colleagues, how can we know if the match is good? And most such inquiries are from people who haven't yet figured out quite how many good projects are underway at Cornell. Perhaps, on arrival, you'll take Andrew Myer's course in language based security and will realize this is your passion. So at Cornell, we urge you to take time to find out what areas we cover and who is here, to take some courses, and only then affiliate with a research group. But please knock on my door any time you like! I'm more than happy to talk to any student in the department about anything we're doing here! 
