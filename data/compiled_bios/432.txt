Home Papers Research Teaching Students CV Shafiq Rayhan Joty Assistant Professor NTU Natural Language Processing Group Office: Block N4, 02c-79 School of Computer Science and Engineering Nanyang Technological University, Singapore Email Phone Google Scholar Github Senior Manager Salesforce AI Research Email Phone Short Biography Assistant Professor, Nanyang Technological University (NTU), Singapore [Jul'17 - ] Senior Manager, Salesforce AI Research, Singapore [Feb'19 - ] Research Scientist, Qatar Computing Research Institute (QCRI), Doha [Jan'14 - Jul'17] PhD in Computer Science, University of British Columbia (UBC), Vancouver [Sep'08 - Dec'13] Research Interests Natural Language Processing Discourse Processing Machine Translation Image-Text Translation Question Answering Text Summarization Sentiment Analysis Machine Learning Probabilistic Graphical Models Deep Learning Reinforcement Learning Representation Learning News 5 Feb 19 I am looking for a postdoc researcher to work in natural language processing, deep learning, and machine learning. 5 Feb 19 Will serve as an area chair for ACL-2019 and IJCAI-2019 5 Feb 19 Will give a tutorial on "Discourse Processing and Its Applications" at ACL-2019 10 Jan 19 Gave two tutorials: one on discourse at ICDM-18 and the other on NLP for Conversations at COLING-18. 16 Oct 18 Check out our Speech Act Recognition demo (now only accessible inside NTU network). 6 Sep 18 Check out our new Malay - English Neural MT demo (now only accessible inside NTU network). Selected Papers This list contains only journal papers. For full list of papers see the Papers tab. Speech Act Modeling of Written Asynchronous Conversations: A Neural CRF Approach Shafiq Joty, and Tasnim Mohiuddin. In Computational Linguistics (Special Issue on Language in Social Media, Exploiting discourse and other contextual information) : pages 859 - 894, 2018. PDF Abstract Speech Act Modeling of Written Asynchronous Conversations: A Neural CRF Approach Participants in asynchronous conversations (e.g., forums, emails) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, which is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings) using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models. BibTex Speech Act Modeling of Written Asynchronous Conversations: A Neural CRF Approach @article{joty-cl-si-18, abstract = {Participants in asynchronous conversations (e.g., forums, emails) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, which is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations (e.g., meetings) using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTM-RNNs provide better task-specific representations, (ii) conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, (iii) adversarial training gives better domain-invariant representations, and (iv) the global CRF model improves over local models.}, author = {Shafiq Joty and Tasnim Mohiuddin}, journal = {Computational Linguistics (Special Issue on Language in Social Media, Exploiting discourse and other contextual information)}, link = {https://www.mitpressjournals.org/doi/pdf/10.1162/coli_a_00339}, number = {4}, pages = {859 -- 894}, publisher = {MIT Press}, title = {Speech Act Modeling of Written Asynchronous Conversations: A Neural CRF Approach}, volume = {44}, year = {2018} } Phrase-Based Attentions Phi Xuan, and Shafiq Joty. In arXiv (* not peer reviewed) 2018. PDF Abstract Phrase-Based Attentions Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data. BibTex Phrase-Based Attentions @article{nguyen-joty-iclr-19, abstract = {Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.}, author = {Phi Xuan Nguyen and Shafiq Joty}, issue = {}, journal = {arXiv (* not peer reviewed)}, link = {https://arxiv.org/abs/1810.03444}, pages = {}, publisher = {arXiv.org}, title = {Phrase-Based Attentions}, year = {2018} } Machine translation evaluation with neural networks Francisco Guzmn, Shafiq Joty, Llus Mrquez, and Preslav Nakov. In Computer Speech & Language (Special Issue on Deep Learning for Machine Translation) : pages 180-200, 2017. PDF Abstract Machine translation evaluation with neural networks Abstract We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the \WMT\ Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an \MT\ evaluation metric that correlates with human judgments, and is on par with the state of the art. BibTex Machine translation evaluation with neural networks @article{guzman-joty-marquez-nakov-csl-16, abstract = {Abstract We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the \{WMT\} Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic embeddings, and sentence-based representations modeled with convolutional and recurrent neural networks. In summary, the proposed framework is flexible and generalizable, allows for efficient learning and scoring, and provides an \{MT\} evaluation metric that correlates with human judgments, and is on par with the state of the art.}, author = {Guzm\'{a}n, Francisco and Joty, Shafiq and Mrquez, Llus and Nakov, Preslav}, doi = {http://dx.doi.org/10.1016/j.csl.2016.12.005}, issn = {0885-2308}, journal = {Computer Speech & Language (Special Issue on Deep Learning for Machine Translation)}, link = {http://www.sciencedirect.com/science/article/pii/S0885230816301693}, pages = {180--200}, title = {Machine translation evaluation with neural networks}, volume = {45:C}, year = {2017} } Domain Adaptation Using Neural Network Joint Model Shafiq Joty, Nadir Durrani, Hassan Sajjad, and Ahmed Abdelali. In Computer Speech & Language (Special Issue on Deep Learning for Machine Translation) : pages 161-179, 2017. PDF Abstract Domain Adaptation Using Neural Network Joint Model We explore neural joint models for the task of domain adaptation in machine translation in two ways: (i) we apply state-of-the-art domain adaptation techniques, such as mixture modelling and data selection using the recently proposed Neural Network Joint Model (NNJM) (Devlin et al., 2014); (ii) we propose two novel approaches to perform adaptation through instance weighting and weight readjustment in the NNJM framework. In our first approach, we propose a pair of models called Neural Domain Adaptation Models (NDAM) that minimizes the cross entropy by regularizing the loss function with respect to in-domain (and optionally to out-domain) model. In the second approach, we present a set of Neural Fusion Models (NFM) that combines the in- and the out-domain models by readjusting their parameters based on the in-domain data. We evaluated our models on the standard task of translating English-to-German and Arabic-to-English TED talks. The NDAM models achieved better perplexities and modest BLEU improvements compared to the baseline NNJM, trained either on in-domain or on a concatenation of in- and out-domain data. On the other hand, the NFM models obtained significant improvements of up to +0.9 and +0.7 BLEU points, respectively. We also demonstrate improvements over existing adaptation methods such as instance weighting, phrasetable fill-up, linear and log-linear interpolations. BibTex Domain Adaptation Using Neural Network Joint Model @article{joty-durrani-sajjad-abdelali-csl-17, abstract = {We explore neural joint models for the task of domain adaptation in machine translation in two ways: (i) we apply state-of-the-art domain adaptation techniques, such as mixture modelling and data selection using the recently proposed Neural Network Joint Model (NNJM) (Devlin et al., 2014); (ii) we propose two novel approaches to perform adaptation through instance weighting and weight readjustment in the NNJM framework. In our first approach, we propose a pair of models called Neural Domain Adaptation Models (NDAM) that minimizes the cross entropy by regularizing the loss function with respect to in-domain (and optionally to out-domain) model. In the second approach, we present a set of Neural Fusion Models (NFM) that combines the in- and the out-domain models by readjusting their parameters based on the in-domain data. We evaluated our models on the standard task of translating English-to-German and Arabic-to-English TED talks. The NDAM models achieved better perplexities and modest BLEU improvements compared to the baseline NNJM, trained either on in-domain or on a concatenation of in- and out-domain data. On the other hand, the NFM models obtained significant improvements of up to +0.9 and +0.7 BLEU points, respectively. We also demonstrate improvements over existing adaptation methods such as instance weighting, phrasetable fill-up, linear and log-linear interpolations.}, author = {Shafiq Joty and Nadir Durrani and Hassan Sajjad and Ahmed Abdelali}, doi = {https://doi.org/10.1016/j.csl.2016.12.006}, issn = {0885-2308}, journal = {Computer Speech & Language (Special Issue on Deep Learning for Machine Translation)}, link = {http://www.sciencedirect.com/science/article/pii/S0885230816301474}, pages = {161-179}, publisher = {Elsevier}, title = {Domain Adaptation Using Neural Network Joint Model}, volume = {45:C}, year = {2017} } Discourse Structure in Machine Translation Evaluation Shafiq Joty, Francisco Guzmn, Llus Mrquez, and Preslav Nakov. In Computational Linguistics : pages 683-722, 2017. PDF Abstract Discourse Structure in Machine Translation Evaluation In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all- subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment- and at the system-level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DISCOTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular, we show that (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality. BibTex Discourse Structure in Machine Translation Evaluation @article{joty-guzman-marquez-nakov-cl-17, abstract = {In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all- subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment- and at the system-level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DISCOTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular, we show that (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality.}, author = {Shafiq Joty and Guzm\'{a}n, Francisco and Mrquez, Llus and Preslav Nakov}, journal = {Computational Linguistics}, link = {http://www.mitpressjournals.org/doi/pdfplus/10.1162/COLI_a_00298}, pages = {683--722}, publisher = {MIT Press}, title = {Discourse Structure in Machine Translation Evaluation}, volume = {43:4}, year = {2017} } Sleep Quality Prediction From Wearable Data Using Deep Learning Aarti Sathyanarayana, Shafiq Joty, Luis Fernandez-Luque, Ferda Ofli, Jaideep Srivastava, Ahmed Elmagarmid, Shahrad Taheri, and Teresa Arora. In JMIR mHealth and uHealth (JMU) 2016. PDF Abstract Sleep Quality Prediction From Wearable Data Using Deep Learning BACKGROUND: The importance of sleep is paramount to health. Insufficient sleep can reduce physical, emotional, and mental well-being and can lead to a multitude of health complications among people with chronic conditions. Physical activity and sleep are highly interrelated health behaviors. Our physical activity during the day (ie, awake time) influences our quality of sleep, and vice versa. The current popularity of wearables for tracking physical activity and sleep, including actigraphy devices, can foster the development of new advanced data analytics. This can help to develop new electronic health (eHealth) applications and provide more insights into sleep science. OBJECTIVE: The objective of this study was to evaluate the feasibility of predicting sleep quality (ie, poor or adequate sleep efficiency) given the physical activity wearable data during awake time. In this study, we focused on predicting good or poor sleep efficiency as an indicator of sleep quality. METHODS: Actigraphy sensors are wearable medical devices used to study sleep and physical activity patterns. The dataset used in our experiments contained the complete actigraphy data from a subset of 92 adolescents over 1 full week. Physical activity data during awake time was used to create predictive models for sleep quality, in particular, poor or good sleep efficiency. The physical activity data from sleep time was used for the evaluation. We compared the predictive performance of traditional logistic regression with more advanced deep learning methods: multilayer perceptron (MLP), convolutional neural network (CNN), simple Elman-type recurrent neural network (RNN), long short-term memory (LSTM-RNN), and a time-batched version of LSTM-RNN (TB-LSTM). RESULTS: Deep learning models were able to predict the quality of sleep (ie, poor or good sleep efficiency) based on wearable data from awake periods. More specifically, the deep learning methods performed better than traditional logistic regression. CNN had the highest specificity and sensitivity, and an overall area under the receiver operating characteristic (ROC) curve (AUC) of 0.9449, which was 46% better as compared with traditional logistic regression (0.6463). CONCLUSIONS: Deep learning methods can predict the quality of sleep based on actigraphy data from awake periods. These predictive models can be an important tool for sleep research and to improve eHealth solutions for sleep. BibTex Sleep Quality Prediction From Wearable Data Using Deep Learning @article{sathyanarayana-et-al-jmu-16, abstract = {BACKGROUND: The importance of sleep is paramount to health. Insufficient sleep can reduce physical, emotional, and mental well-being and can lead to a multitude of health complications among people with chronic conditions. Physical activity and sleep are highly interrelated health behaviors. Our physical activity during the day (ie, awake time) influences our quality of sleep, and vice versa. The current popularity of wearables for tracking physical activity and sleep, including actigraphy devices, can foster the development of new advanced data analytics. This can help to develop new electronic health (eHealth) applications and provide more insights into sleep science. OBJECTIVE: The objective of this study was to evaluate the feasibility of predicting sleep quality (ie, poor or adequate sleep efficiency) given the physical activity wearable data during awake time. In this study, we focused on predicting good or poor sleep efficiency as an indicator of sleep quality. METHODS: Actigraphy sensors are wearable medical devices used to study sleep and physical activity patterns. The dataset used in our experiments contained the complete actigraphy data from a subset of 92 adolescents over 1 full week. Physical activity data during awake time was used to create predictive models for sleep quality, in particular, poor or good sleep efficiency. The physical activity data from sleep time was used for the evaluation. We compared the predictive performance of traditional logistic regression with more advanced deep learning methods: multilayer perceptron (MLP), convolutional neural network (CNN), simple Elman-type recurrent neural network (RNN), long short-term memory (LSTM-RNN), and a time-batched version of LSTM-RNN (TB-LSTM). RESULTS: Deep learning models were able to predict the quality of sleep (ie, poor or good sleep efficiency) based on wearable data from awake periods. More specifically, the deep learning methods performed better than traditional logistic regression. CNN had the highest specificity and sensitivity, and an overall area under the receiver operating characteristic (ROC) curve (AUC) of 0.9449, which was 46% better as compared with traditional logistic regression (0.6463). CONCLUSIONS: Deep learning methods can predict the quality of sleep based on actigraphy data from awake periods. These predictive models can be an important tool for sleep research and to improve eHealth solutions for sleep.}, author = {Aarti Sathyanarayana and Shafiq Joty and Luis Fernandez-Luque and Ferda Ofli and Jaideep Srivastava and Ahmed Elmagarmid and Shahrad Taheri and Teresa Arora}, doi = {10.2196/mhealth.6562}, journal = {JMIR mHealth and uHealth (JMU)}, link = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5116102/}, number = {e125}, pmid = {27815231}, title = {Sleep Quality Prediction From Wearable Data Using Deep Learning}, volume = {4(4)}, year = {2016} } CODRA: A Novel Discriminative Framework for Rhetorical Analysis Shafiq Joty, Giuseppe Carenini, and Raymond T Ng. In Computational Linguistics : pages 385-435, 2015. PDF Abstract CODRA: A Novel Discriminative Framework for Rhetorical Analysis Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence structure. In this article, we present CODRA a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse. CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. We present two approaches to combine these two stages of parsing effectively. By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further. BibTex CODRA: A Novel Discriminative Framework for Rhetorical Analysis @article{joty-carenini-ng-cl-15, abstract = {Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence structure. In this article, we present CODRA a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse. CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. We present two approaches to combine these two stages of parsing effectively. By conducting a series of empirical evaluations over two different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art, often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated by CODRA can potentially improve the accuracy even further.}, author = {Joty, Shafiq and Carenini, Giuseppe and Ng, Raymond T}, journal = {Computational Linguistics}, link = {papers/joty-carenini-ng-cl-15}, pages = {385-435}, publisher = {MIT Press}, title = {CODRA: A Novel Discriminative Framework for Rhetorical Analysis}, volume = {41:3}, year = {2015} } Topic Segmentation and Labeling in Asynchronous Conversations Shafiq Joty, Giuseppe Carenini, and Raymond Ng. In Journal of Artificial Intelligence Research : pages 521-573, 2013. PDF Abstract Topic Segmentation and Labeling in Asynchronous Conversations Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations. BibTex Topic Segmentation and Labeling in Asynchronous Conversations @article{joty-carenini-ng-jair-13, abstract = {Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.}, author = {Shafiq Joty and Giuseppe Carenini and Raymond Ng}, journal = {Journal of Artificial Intelligence Research}, link = {https://www.jair.org/media/3940/live-3940-7166-jair.pdf}, pages = {521--573}, title = {Topic Segmentation and Labeling in Asynchronous Conversations}, volume = {47}, year = {2013} } Complex Question Answering: Unsupervised Learning Approaches and Experiments Yllias Chali, Shafiq Joty, and Sadid Hasan. In Journal of Artificial Intelligence Research : pages 1-47, 2009. PDF Abstract Complex Question Answering: Unsupervised Learning Approaches and Experiments Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features. BibTex Complex Question Answering: Unsupervised Learning Approaches and Experiments @article{chali-joty-hasan-jair-09, abstract = {Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.}, acmid = {1641504}, address = {USA}, author = {Chali, Yllias and Joty, Shafiq and Hasan, Sadid}, issn = {1076-9757}, issue_date = {2009}, journal = {Journal of Artificial Intelligence Research}, link = {https://www.jair.org/media/2784/live-2784-4446-jair.pdf}, month = {may}, number = {1}, numpages = {47}, pages = {1--47}, publisher = {AI Access Foundation}, title = {Complex Question Answering: Unsupervised Learning Approaches and Experiments}, volume = {35}, year = {2009} } Last updated February 18, 2019. Created with git , jekyll , bootstrap , and sublime text . Website template available at github fork from Adam Lopez's site .
