Andreas Moshovos Home Students Current Alumni Teaching Research Overview Publications Contact Service Tools Site Tools Media Manager Sitemap Page Tools Show pagesource Old revisions Backlinks Back to top I teach at the Department of Electrical and Computer Engineering at the University of Toronto where I have the privilege to work with a team of very talented graduate students . I have also taught at the Northwestern University, USA, the University of Athens, Greece, the Hellenic Open University, Greece, and as a invited professor at the cole polytechnique fdrale de Lausanne, Switzerland. I received a Bachelors and a Master's Degree from the University of Crete, Greece and a Ph.D. from the University of Wisconsin-Madison. My research interests lie primarily in the design of performance-, energy-, and/or cost-optimized computing engines for various applications domains. Most of my work thus far has been on high-performance general-purpose systems.My current work emphasizes highly-specialized computing engines for Deep Learning . I will also be serving as the Director of the newly formed National Sciences and Engineering Research Council Strategic Partnership Network on Machine Learning Hardware Acceleration (NSERC COHESA), a partnership of 19 Researchers across 7 Universities involving 8 Industrial Partners. For the work I have done with my students and collaborators, I have been awarded the ACM SIGARCH Maurice Wilkes mid-career award , a National Research Foundation CAREER Award, two IBM Faculty Partnership awards, a Semiconductor Research Innovation award, an IEEE Top Picks in Computer Architecture Research, and a MICRO conference Hall of Fame award. I have served as the Program Chair for the ACM/IEEE International Symposium on Microarchitecture and the ACM/IEEE International Symposium on Performance Analysis of Systems and Software. I am also Fellow of the ACM. Deep Learning Acceleration Value-Based Acceleration: We are developing, designing and demonstrating a novel class of hardware accelerators for Deep Learning networks whose key feature is that they are value-based . Conventional accelerators rely mostly on the structure of computation, that is, which calculations are performed and how they communicate. Value-based accelerators further boost performance by taking advantage of expected properties in the runtime calculated value stream, such as, dynamically redundant or ineffectual computations, or the distribution of values, or even their bit content. In short, our accelerator designs, reduce the amount of work that needs to be performed for existing neural models and do so transparently to the model designer. Why are we pursuing these designs? Because Deep Learning is transforming our world by leaps and bounds. One of the three drivers behind Deep Learning success is the computing hardware that enabled its first practical applications. While algorithmic improvements will allow Deep Learning to evolve, much hinges on hardwares ability to keep delivering ever higher performance and data processing storage and processing capability. As Dennard scaling has seized, the only viable way to do so is by architecture specialization. The figure below highlights the potential and hence motivation for some of the methods we have developed: A: remove zero Activations, W: remove zero weights, Ap: use dynamic per group precision for activations, Ae: skip ineffectual terms after Booth Encoding the Activations. We also have designs that exploit Weight precision (see LOOM below) and yet to be released designs that exploit further properties :) This IEEE MICRO and IEEE Computer articles present our rationale and summarize some of our designs. The most recently publicly disclosed design is Bit-Tactical that targets but does not require sparse networks. The tables below summarize some key characteristics of some of our designs: Laconic Our most recent design, Laconic , targets the effectual bit content of both activations and weights. The potential reduction in work is consistently two orders of magnitude over processing full 16b values. As all our other designs, Laconic does not sacrifice accuracy. The graph below shows work reduction and equivalently performance improvement potential. And here are the performance improvements achieved by different configurations of Laconic vs. a DaDianNao configuration with 1 tile, processing 8 filters and 16 products per filter. The subscript is the number of wires used in the Weight Memory interface. The baseline uses filters x weights x bits = 8x16x16 = 2K wires.
