 Boris Grot Home Home Publications CV Contact boris.grot@ed.ac.uk +44 131 650 5123 University of Edinburgh School of Informatics Informatics Forum 2.04A 10 Crichton Street Edinburgh EH8 9AB, UK Boris Grot , Reader (Associate Professor) School of Informatics University of Edinburgh Research Interests: System architectures for datacenters Processor architecture and microarchitecture Memory systems and interconnection networks Systems with quality-of-service guarantees About: I am an Associate Professor (Reader, in UK parlance) in the School of Informatics at the University of Edinburgh. My work focuses on improving the efficiency of large-scale datacenters (think Google or Facebook) through improvements to server processor architectures, memory systems, and interconnects. To understand why this is important, read below . Previously, I was a post-doctoral researcher at the Parallel Systems Architecture Lab at EPFL, working on Scale-Out Processors and other fun projects. I did my PhD in Computer Science at The University of Texas at Austin. My thesis focused on scalability and quality-of-service in on-chip networks of highly-integrated processor chips. Why datacenters (and my research) matter? As mobile computing and cyber-physical systems displace traditional forms of computing, datacenters will shoulder the data-crunching burden. There are three reasons for the growing reliance on datacenters. First, mobile and embedded systems are inherently constrained in their processing capabilities due to limitations of battery technology, low thermal ceilings, and form factor considerations. Second, the important applications of today (e.g., search, social networking, business analytics) draw on enormous volumes of data and have massive processing requirements that are well beyond the reach of individual servers. Last, businesses of all sizes are increasingly moving their applications to the cloud for reasons of scalability, resiliency, and operational efficiency. A modern datacenter is a football-field sized installation that houses tens of thousands of servers, draws 5-20 MW of power, and costs over $100 million to deploy. A 2010 study estimated the global datacenter energy footprint at 1.3% of the world-wide usage. This number is widely expected to grow considerably in the coming decade due to the rapid pace of deployment of new datacenters and the developing world coming online. Energy scalability of datacenters is an important problem with major economic and environmental implications. As the semiconductor industry inches toward the physical limits of voltage scaling, improvements in energy-efficiency of future chips will require much more effort than in the past. The same is true for conventional memories and networking technologies used in datacenters. The time is right to re-invent server architectures by specializing processor chips, memory hierarchy, and networks for tomorrows datacenter computing. Current PhD openings: If you are a student interested in working on exciting problems at the cross-roads of architecture, systems, and big data, please read this . Recent Jan 2019: Stretch is nominated for the Best Paper Award at HPCA 2019! Nov 2018: Our idea paper on accelerating virtual address translation via learned indexes will be presented at Workshop on ML for Systems at NIPS, 2018. Nov 2018: How does one reconcile throughput and quality-of-service on a multi-threaded core running colocated server workloads? Stretch shows the way! Appearing in HPCA 2019. Sep 2018: Amna Shahab, a 3rd-year PhD student in our lab, received the Best Poster award at this year's ARM Research Summit! Jul 2018: Our upcoming MICRO 2018 paper makes the case for all-private cache hierchies for server CPUs. Say good-bye to shared last-level caches! Jun 2018: Promoted to an Associate Professor (aka Reader in British academic parlance). Jan 2018: Aggressive replication, strong consistency guarantees and high throughput in a scale-out cluster are not mutually exclusive. Surpised? Read our EuroSys 2018 . June 2017: Shotgun revolutionizes control flow delivery for server processors. Appearing at ASPLOS 2018. Sep 2017: Oracle has generously supported our research on scale-up graph processing for future memory systems. June 2017: Our work on variability-aware cache management will appear at PACT 2017. May 2017: Leeway has been selected to compete in the Cache Replacement Championship in June. Apr 2017: The Mondrian Data Engine will engage at ISCA 2017. Oct 2016: Boris has been inducted into the MICRO Hall of Fame! Oct 2016: Boomerang accepted to HPCA 2017. It shows how to design a high-performance core front-end without costly metadata - great news for server processors! June 2016: Two papers accepted to MICRO 2016. May 2016: Our paper debunking the usefulness of dead block prediction for last-level caches will appear in WDDD. May 2016: Fat Caches for Scale-Out Servers has been accepted for publication in IEEE Micro. Mar 2016: The IEEE Micro Special Issue on Near-Data Processing , which I guest co-edited, is now available. Oct 2015: Asynchronous Memory Access Chaining will appear in VLDB 2016. Sep 2015: Confluence: Unified Instruction Supply for Scale-Out Servers will appear in MICRO 2015. May 2015: Sort vs. Hash Join Revisited for Near-Data Execution will appear in ASBD 2015. Mar 2015: Manycore Network Interfaces for In-Memory Rack-Scale Computing accepted to ISCA 2015! Dec 2014: Our Workshop on Near-Data Processing (WoNDP) was hugely successful at this year's MICRO with over 100 people in attendance! Sep 2014: Bulk Memory Access Prediction and Streaming accepted to MICRO 2014! Sep 2014: The IEEE Micro Special Issue on Big Data , which I guest co-edited, is now available. Aug 2014: Recipient of the Google Faculty Research Award. Apr 2014: Serving on the ASPLOS , MICRO , and HPCA program committees. Please consider submitting to these top venues in computer systems research. Dec 2013: Meet the Walkers gets Best Paper Runner-Up kudos at MICRO! 
