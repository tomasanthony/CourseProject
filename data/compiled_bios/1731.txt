 Toggle Navigation Home Publications Contact Blog Marc G. Bellemare Marc G. Bellemare Research Scientist, Google Brain Adjunct Professor, McGill University Canada CIFAR AI Chair, Mila Email Twitter GitHub As humans, we spend our daily lives interacting with the unknown, from ordering off a restaurant menu to learning the ropes at a new job. From an artificial intelligence perspective, we are generally competent agents . I am interested in understanding which algorithms may support this general competency in artificial agents and developing the right environments , simulated or otherwise, to foster and study general competency. I am a research scientist at Google Brain in Montral, Canada focusing on the reinforcement learning effort there. From 2013 to 2017 I was at DeepMind in the UK. I received my Ph.D. from the University of Alberta working with Michael Bowling and Joel Veness . My research lies at the intersection of reinforcement learning and probabilistic prediction. I'm also interested in deep learning, generative modelling, online learning, and information theory. Current students: Marlos C. Machado (Ph.D.), with Michael Bowling, University of Alberta. Adrien Ali Taga (Ph.D.), with Aaron Courville, MILA. Liam Fedus (Ph.D.), with Hugo Larochelle and Yoshua Bengio, MILA. Vishal Jain (M.Sc.), with Doina Precup, McGill University. Philip Amortila (M.Sc.), with Doina Precup and Prakash Panangaden, McGill University. News February 4th, 2019. We've open-sourced the Hanabi Learning Environment , a platform for multiagent AI research on the board game Hanabi . This has been a fantastic collaboration with researchers from DeepMind. More details in this blog post . February 4th, 2019. Last August we released the first version of the Dopamine framework for reinforcement learning research. This week version 2.0 comes out. Dopamine 2.0 provides support for most discrete action domains supported by OpenAI Gym. Huge thanks to Pablo Samuel Castro for getting this second release out the door, blog post here . December 1st, 2017. I am thrilled to finally announce the release of version 0.6.0 of the Arcade Learning Environment. This new version brings many exciting new features and bug fixes, most importantly the ability to play games in different modes and difficulties, in some cases giving rise to dozens of different configurations! This will be of particular interest to researchers interested in transfer, lifelong, and continual learning. More details here. October 13th, 2017. I recently gave two talks on distributional reinforcement learning: a short one at the fantastic Montreal AI Symposium and a longer one at my alma mater, the University of Alberta. September 5th, 2017. I am pleased to announce that I have joined the Google Brain team in Montral, Canada. Along with a continuation of my reinforcement learning research, I am also quite excited to have the opportunity to take part in helping grow Montral in its role as an "AI Silicon Valley". In the coming months, I will also be developing a short online course in reinforcement learning, in the form of a series of blog posts stay tuned! The Arcade Learning Environment The Arcade Learning Environment (ALE). The ALE is the highly-successful interface to Atari 2600 games, designed at the University of Alberta from 2008 onwards and first released in 2010 (see our 2013 paper in the Journal of Artificial Intelligence Research). It was popularized by our 2015 Nature paper at DeepMind, and has since supported countless research projects. ALE on GitHub ALE via OpenAI Gym Distributional Reinforcement Learning Most of reinforcement learning only models the expected return from a state (i.e., the average sum of encountered rewards, typically called expected value ). In a recent paper we showed that reasoning about and modelling the full distribution, even in the absence of risk considerations, can lead to significant performance gains. Beyond immediate results, the distributional framework lets us revisit almost every aspect of reinforcement learning I'm quite excited to see what comes next. Blog post (DeepMind) Reinforcement Learning With Density Models Incredibly enough, most of the recent successes of deep reinforcement learning have been achieved with the most naive forms of exploration, where an agent discovers its environment through random noise in its actions. This approach is extremely wasteful; for example, the first Deep Q-Networks agent took over 38 days of continuous play to obtain human-level performance. Over the last few years I'ev become interested in the role that density models (i.e., generative models whose probability distribution can be explicitly queried) can play in improving exploration. At the time of writing, our pseudo-count approach remains state-of-the-art for Atari 2600 games where exploration is difficult ( original paper ; follow-up work with Georg Ostrovski). Powered by Mezzanine and Django | Theme by Bootstrap 
