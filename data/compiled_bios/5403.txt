News Intro Prospective Students Team Research CompuCast Teaching Publications Contact HUGH LEATHER News 1 (22nd Nov 2018) I have been interviewed by the IoT Podcast. Listen here . (12th Jul 2018) DISTINGUISHED PAPER AWARD at ISSTA 2018 For paper "Compiler Fuzzing through Deep Learning". (24th Sep 2017) BEST PAPER AWARD at PACT 2017 For paper "Synthesizing Benchmarks for Predictive Modeling". (8th Feb 2017) BEST PAPER AWARD at CGO 2017 For paper "End-to-end Deep Learning of Optimization Heuristics". (20th Jan 2017) PhD Studentships available I have one fully funded PhD position for either overseas or EU/UK. There is more info here . Intro 10 I am a reader (associate professor) in the School of Informatics at the University of Edinburgh. I am a member of the CARD group and of the Edinburgh ARM Center of Excellence . I am a Royal Academy of Engineering / EPSRC Research Fellow. I am a Chancellor's Fellow of the University of Edinburgh. Previously, I worked for a while in industry, for Microsoft in Seattle and Trilogy in Austin. Prospective Students 11 Do you want to do a PhD? Being an advisor to students is by far the best part of my job, I love it. Have a look at my research areas below. If any of those grab you, then shoot me an email. It is nearly always possible to find money for good students. Make sure you have a look at the advice below. NEWS: I have one fully funded PhD position for either overseas or EU/UK. Tips on applying and having a successful PhD A PhD is a long degree, probably it will be 4 years of your life. And, different from your first, taught degree, it is extremely specialised on one or two areas. That is a long time to spend on something if it does not excite you. So, the first thing is to ask yourself why you want a PhD. There are lots of reasons people have, but the best reason, in my opinion, is that you really, really want to explore some area or question in computer science. Are you fascinated by some problem and just wish someone would give you the time to try out all your ideas on it? That's an excellent way to begin! Let me tell you what I see my role as. I see myself as your advisor, not your supervisor. By that I mean that I am not your boss. You will make your own decisions and are quite at liberty not to take my advice. In fact, I expect you to argue with me - if you haven't told me I'm wrong about at least one thing by the end you can't have become the expert that a PhD is supposed to qualify you to be. I think it is important that students get that freedom (I could not have stomached my PhD if my advisor had been the supervisor type). But, I am aware that some people prefer to be told what to do step by step and treat a PhD more like a job or a taught degree than the creative self driven experience I want for you. That's okay, we can do that too, but if you know this ahead of time, consider a different academic - there are lots who do the boss thing. If you do want to do a PhD with me, here's how it will go: After you first contact me, I'll give you a short 'do at home' programming test. This serves two purposes for me. The first is that it quickly filters out the time wasters. "You're joking!" I hear you cry? No really, there are lots of people who send out form letters to hundreds of academics. Often, they have copied and pasted my research interests into their letter, and probably not even bothered to make the fonts consistent with the rest of their letter. These people tend not to reply to a programming test. The second thing is that it gives me a quick idea of your programming ability. I assume that you already know how to program, and unless you are amazing in some other ways, I would probably recommend you not do a PhD with me if you can't program. This test doesn't have a deadline, but how long you take to complete it will be taken into account. We have a phone/Skype/Hangout/etc interview. For me, I want to hear about why you want to do a PhD, what your ideas are, and find out if you're someone I could work with. I'll also give you a short 'live' programming test. "Another one, really?" Yes, some people have actually cheated on the at home test before! Often I invite another academic to the call to get a second opinion. You should use this call to find out at least how a PhD would work with me and whether you think you could work with me. You should probably talk to some of my students (or the students of any academic you intend to study with). Send them an email and maybe arrange a chat. Do they like working with me? What are my worst faults? Would they rather they had done a PhD with someone else? And on and on. You never know what things they might tell you and they could save you from making a horrible mistake! Now, you apply with all the forms here . One important part here is that you write a research proposal. This is just a few sides of A4, structured a bit like a research paper (abstract, background, related work, etc). Even if I think you are great and want to accept you, students have to make it through selection panels where things like your CV and research proposal will be discussed and must be approved by several academics. While we wait for that, we (well probably I) find you funding. You might need to fill out more forms to apply for various scholarships. Normally the funding pays your fees and gives you a living allowance. Sometimes it might also include travel money and a small equipment budget. The fees for overseas students are about twice that of EU/UK students (although Brexit might soon mean that will include EU students, too!). There are also fewer pots of money that can be used for overseas students. These things mean overseas students are harder to fund and typically have to be that much better to make it worthwhile. Please don't let that discourage you, funding is often found for great students! Assuming we're all good up to this point you should get an acceptance letter from the University. It could still be several months before your start date. You have to work out your travel and accommodation (sorry the University doesn't give much help and gives no financial help beyond your stipend). Probably during this time, if you are up for it, we will meet online occasionally (say once a month) to discuss papers and ideas, etc. But, if you are busy working or doing whatever, don't feel you have to have these meetings. Great! Hopefully you are now in Edinburgh starting your PhD! This should be fantastic! I can't wait to meet you in person! Team 100 Rodrigo Caetano Rocha PhD Student Auto paralellisation Chris Cummins PhD Student Deep Learning Over Programs Ouz Ergin Post Doc - Finished 2016 Vertical Data Centre Integration Associate Professor Stephen Kyle PhD - Graduated 2015 Virtual Machines Researcher at ARM Ltd. Paschalis Mpeis PhD Student Interactive Iterative Compilation William Ogilvie PhD Student Active Learning for Compilers Pavlos Petoumenos Post Doc Energy Accounting Volker Seeker PhD Student Low Energy Mobile Systems Chad Verbowski PhD Student Self Optimising Data Centres Research 101 My research is about improving the energy consumption and performance of computers, ranging from mobile systems to data centres. The energy used by the World's computers is staggering and growing exponentially. If we don't do something to fix it we are going to need a nuclear power station on every street corner just so we can all stalk each other on Facebook. My work often uses machine learning and deep learning to discover new ways to optimise programs and systems. Mobile Systems Compiler Optimisation Power and Energy Optimisation Runtime Adaptation and Dynamic Runtime Optimisation Heterogeneous Parallelism Optimisation GPGPU Optimisation Auto-tuning and Machine Learning Techniques A list of some ongoing projects is below. ALEA - Abstraction-Level Energy Accounting for Many-core Programming Languages Project supported by: Pavlos Petoumenos Performance profiling has been around for a long time, but there is nothing similar for energy. This project builds mechanisms to determine how much energy each line of a program's source code consumes. And, it determines how data structures contribute to energy consumption. Energy consumption will be matched against programming language abstractions, from basic-blocks to functions, loops, and parallel constructs, and from variables to data structures, providing developers with the information that they need. Without tools like this, developers cannot optimise their programs for energy. Built on top of this, we apply machine learning techniques to automatically optimise programs for energy consumption. Website DIVIDEND - Distributed Heterogeneous Vertically IntegrateD ENergy Efficient Data Centres Project supported by: Ouz Ergin Volker Seeker Our world is in the midst of a big data revolution, driven by the ubiquitous ability to gather, analyse, and query datasets of unprecedented variety and size. The sheer storage volume and processing capacity required to manage these datasets has resulted in a transition away from desktop processing and toward warehouse-scale computing inside data centres. State-of-the-art data centres, employed by the likes of Google and Facebook, draw 20-30 MW of power, equivalent to 20,000 homes, with these companies needing many data centres each. The global data centre energy footprint is estimated at around 2% of the worlds energy consumption and doubles every five years. Contemporary data centres have an average overhead of 90%, meaning that they consume up to 1.9 MW to deliver 1 MW of IT support; this is not cost-effective or environmentally sound. If the exponential data growth and processing capacity are to scale in the way that both the public and industry have come to rely upon, we must tackle the data centre energy crisis or face the reality of stagnated progress. With the semiconductor industrys inability to further lower operating voltages in processor and memory chips, the challenge is in developing technologies for large-scale data-centric computation with energy as a first-order design constraint. The DIVIDEND project attacks the data centre energy efficiency bottleneck through vertical integration, specialisation, and cross-layer optimisation. Our vision is to present heterogeneous data centres, combining CPUs, GPUs, and task-specific accelerators, as a unified entity to the application developer and let the runtime optimise the utilisation of the system resources during task execution. DIVIDEND embraces heterogeneity to dramatically lower the energy per task through extensive hardware specialisation while maintaining the ease of programmability of a homogeneous architecture. To lower communication latency and energy, DIVIDEND leverages SoC integration and prefers a lean point-to-point messaging fabric over complex connection-oriented network protocols. DIVIDEND addresses the programmability challenge by adapting and extending the industry-led heterogeneous systems architecture programming language and runtime initiative to account for energy awareness and data movement. DIVIDEND provides for a cross-layer energy optimisation framework via a set of APIs for energy accounting and feedback between hardware, compilation, runtime, and application layers. The DIVIDEND project will usher in a new class of vertically integrated data centres and will take a first stab at resolving the energy crisis by improving the power usage effectiveness of data centres by at least 50%. Website SUMMER - SchedUling on heterogeneous Mobile Multicores based on quality of ExpeRience Project supported by: Pavlos Petoumenos Volker Seeker Users want mobile devices that appear fast and responsive, but at the same time have long lasting batteries and do not overheat. Achieving both of these at once is difficult. The workloads employed to evaluate mobile optimisations are rarely representative of real mobile applications and are oblivious to user perception, focussing only on performance. As a result hardware and software designers' decisions do not respect the user's Quality of Experience (QoE). The device either runs faster than necessary for optimal QoE, wasting energy, or the device runs too slowly, spoiling QoE. SUMMER will develop the first framework to record, replay, and analyse mobile workloads that represent and measure real user experience. Our work will expose for the first time the real Pareto trade-off between the user's QoE and energy consumption. The results of this project will permit others, from computer architects up to library developers, to make their design decisions with QoE as their optimisation target. To show the power of this new approach, we will design the first energy efficient operating system scheduler for heterogeneous mobile processors which takes QoE into account. With heterogeneous mobile processors just now entering the market, a scheduler able to use them optimally is urgently needed. We expect our scheduler to be at least 50% more energy efficient on average than the standard Linux scheduler on an ARM BIG.LITTLE system. SelfOptimisation of Internet Services Project supported by: Chad Verbowski Modern Internet services such as search, social networking, online shopping, media services, gaming, and email can span as many as 1,000,000 servers, requiring multiple geographically distributed instances to serve customers around the world. The financial cost of building and operating each of these various services can be more than US$1 billion annually. In addition, it is critical that the performance of these services are highly optimised across all servers and datacenter locations. Experiments slowing user responses by as little as 100ms have caused a measurable decrease in user engagement. Given the large scale and impact of these systems, optimising them presents a significant resource saving opportunity and a chance to improve overall service quality. This project presents a novel idea for making significant improvements to the capacity utilisation of these servers thus potentially saving many millions of dollars annually, and in addition will improve the end-user perceived availability of these services and reduce the latency they experience when using them. User Experience Driven CPU Frequency Scaling On Mobile Devices Towards Better Energy Efficiency Project supported by: Pavlos Petoumenos Volker Seeker Mobile computing devices such as smartphones and tablets have become tightly integrated with many peoples life, both at work and at home. Users spend large amounts of time interacting with their mobile device and demand an excellent user experience in terms of responsiveness, whilst simultaneously expecting a long battery life between charging cycles. Frequency governors, responsible for increasing or decreasing the CPU clock frequency depending on the current workload and external events, try to balance the two contrasting goals of high performance and low energy consumption. However, despite their critical role in providing energy efficiency it is difficult to measure the effectiveness of frequency governors in an interactive environment. In this paper we develop a novel methodology for creating repeatable, fully automated, realistic, workloads that can accurately measure time lag in interactive applications resulting from non-optimally selected operating frequencies. We also introduce a new metric capturing the user experience for different ANDROID frequency governors. We evaluate interactive workloads to demonstrate how our approach enables us to automatically record and replay sequences of user interactions for different system configurations. We demonstrate that none of the available ANDROID frequency governors performs particularly well, but leave substantial room for improvement. We show that energy savings of up to 27% are possible, whilst delivering a user experience that is better than that provided by the standard ANDROID frequency governor. We also show that it is possible to save 47% energy with performance that is indistinguishable from permanently running the CPU at the highest frequency. Applications of Information Sharing for Code Generation in Process Virtual Machines Project supported by: Stephen Kyle This project looks at improving VM performance. It considers aspects of parallelisation, program fuzzing, and crowd sourced optimisation. CompuCast 110 Compucast CompuCast is a podcast for computer scientists and anyone with a taste for delving deeper into understanding computers and their related technologies. Podcasts RSS A few years ago I couldn't find any podcasts for computer scientists. There were lots for the other sciences and a few for engineers, but none for us. Honestly it was getting to the stage that if I had to listen to another story about global warming I think I was going to set fire to a polar bear! So, myself and a few friends decided to do something about it - Compucast was born! We set out to produce an interesting and varied show aimed specifically at those with a reasonable level of understanding of computer science: we don't want to dance around the detail, we want to embrace it and really learn something. In each show correspondents with some expertise in the field will present several recently published and thoroughly researched computer-science related features. The idea is then to intertwine the nitty gritty detail of these with some more light-hearted topics (jokes and a quiz) so that we don't lose anyone as the show progresses. We will also present a news section that will guide you through the innovative, the surreal and the absurd of the previous few months. If this sounds like something you can be entertained by, then listen to our latest podcast, at compucast.io . By the way, that is the new swanky website made by Zheng Wang. The old, ugly one, made by me is still around at compucast.inf.ed.ac.uk . Compucast has changed a bit over the years. For a couple of years after my daughter was born I didn't have time for it, so it languished for a bit. Now we're back with a new, enthusiastic team. The show is also now a joint collaboration between three universities, the University of Edinburgh , the University of Lancaster , and the University of St Andrews . Teaching 111 UG4 and MSc - Compiler Optimisation - 2019 Course page This course introduces students to modern techniques in efficient implementation of programming languages. Modern processors and systems are designed based on the assumption that a compiler will be able to effectively exploit architectural resources. This course will examine in detail techniques to exploit instruction level parallelism, memory hierarchy and higher level parallelism. It will examine classic static analysis approaches to these problems and introduce newer feedback directed and dynamic approaches to optimisation. The course work will require students to implement selected optimisations in a research compiler. Timetable Semester 2 Day Start Finish Building Room Monday 10:00 10:50 Appleton Tower AT2.11 Thursday 10:00 10:50 Appleton Tower AT2.11 Coursework Deadline Thursday 4pm Feb 21st 2019 Feedback Thursday 4pm Mar 7th 2019 Lecture Notes 1 2 3 4 4-from-ssa 5 6 7 8 9 10 11 12 13 14 Video lectures on YouTube Tools and Resources Ayrton Massey's Data-Flow Tutor . Ayrton was a student of mine who built this rather excellent teaching tool for you. BTW, you can fork his project for this on GitHub . Engineering A Compiler (pdf) . This is the pdf of the first text book. I assume this is legit, but haven't checked, or downloaded it myself. Download at your own risk! Programming Club It is quite possible to finish an undergraduate CS degree at Edinburgh without having done a huge amount of programming. The purpose of the of the course is not to turn students into programmers, but rather to give them the very important theoretical skills that make decent programmers into extraordinary programmers. We leave learning to be an expert in at least one programming language to the students to do in their own time. After all, if we were determined to teach that to students, there wouldn't be any time left over for the theoretical things. Unfortunately, this can lead to a rather rude awakening for students who haven't learned programming on their own time when they go out to try and get a job. So, I run a completely voluntary, no credits course/club for students to get more practice. Once a week during term time we have a couple of hours in an informal, relaxed lab where we offer different challenges for students to attack. We have programming competitions, short projects, talks, and guest speakers. There should be something for everyone at whatever stage they are at. The hope is that students will find something in the class to inspire them and then take it further during their free time. So far, feedback has been that students find the course fun, interesting, and useful - phew! GitHub Repository Mailing List Publications 1000 Full BibTex Papers 1 Compiler Fuzzing through Deep Learning Distinguished Paper Award Chris Cummins, Pavlos Petoumenos, Hugh Leather, Alastair Murray Proceedings of the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018) July 2018 Amsterdam, Netherlands bibtex | abstract | paper(pdf) Random program generation fuzzing is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03 less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing. @inproceedings{ leather_compilerfuzzingdeeplearning_issta2018, author = {Chris Cummins and Pavlos Petoumenos and Hugh Leather and Alastair Murray}, location = {Amsterdam, Netherlands}, booktitle = {Proceedings of the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018)}, month = {July}, year = {2018}, title = {Compiler Fuzzing through Deep Learning} } 2 Right-sizing Server Capacity Headroom for Global Online Services Chad Verbowski, Ed Thayer, Paolo Costa, Hugh Leather, Bjoern Franke Proceedings of the 38th IEEE International Conference on Distributed Computing Systems (ICDCS 2018) July 2018 Vienna, Austria bibtex | abstract | paper(pdf) We present a capacity planning case study showing a significant opportunity for improving the utilization of a large, low-latency, highly available online service containing 100K+ servers spanning 9 geographic regions. Analyzing 30 PB of traces over 90 days we devised a new iterative black-box capacity planning model using the discovered relationships between workload, utilization, and quality. We verified the model on 1,000s of servers showing capacity reductions between 20% and 40% with effectively no impact on workload latency, availability, or the capacity required for disaster recovery. These results are confirmed experimentally by shrinking production server pools to cause the remaining servers to run at higher utilization, and using data from real-world large scale unplanned failures. Finally, we show examples of using our model for offline regression analysis to detect critical issues before their deployment. @inproceedings{ leather_rightsizingservercapacity_icdcs2018, author = {Chad Verbowski and Ed Thayer and Paolo Costa and Hugh Leather and Bjoern Franke}, location = {Vienna, Austria}, booktitle = {Proceedings of the 38th IEEE International Conference on Distributed Computing Systems (ICDCS 2018)}, month = {July}, year = {2018}, title = {Right-sizing Server Capacity Headroom for Global Online Services} } 3 End-to-end Deep Learning of Optimization Heuristics Best Paper Award Christopher Cummins, Pavlos Petoumenos, Zheng Wang, Hugh Leather Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT 2017) September 2017 Portland, Oregon, US bibtex | abstract | paper(pdf) Accurate automatic optimization heuristics are necessary for dealing with the complexity and diversity of modern hardware and software. Machine learning is a proven technique for learning such heuristics, but its success is bound by the quality of the features used. These features must be hand crafted by developers through a combination of expert domain knowledge and trial and error. This makes the quality of the final model directly dependent on the skill and available time of the system architect. Our work introduces a better way for building heuristics. We develop a deep neural network that learns heuristics over raw code, entirely without using code features. The neural network simultaneously constructs appropriate representations of the code and learns how best to optimize, removing the need for manual feature creation. Further, we show that our neural nets can transfer learning from one optimization problem to another, improving the accuracy of new models, without the help of human experts. We compare the effectiveness of our automatically generated heuristics against ones with features hand-picked by experts. We examine two challenging tasks: predicting optimal mapping for heterogeneous parallelism and GPU thread coarsening factors. In 89% of the cases, the quality of our fully automatic heuristics matches or surpasses that of state-of-the-art predictive models using hand-crafted features, providing on average 14% and 12% more performance with no human effort expended on designing features. @inproceedings{ leather_deeplearning_pact2017, author = {Christopher Cummins and Pavlos Petoumenos and Zheng Wang and Hugh Leather}, location = {Portland, Oregon, US}, booktitle = {Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT 2017)}, month = {September}, year = {2017}, title = {End-to-end Deep Learning of Optimization Heuristics} } 4 Synthesizing Benchmarks for Predictive Modeling Best Paper Award Chris Cummins, Pavlos Petoumenos, Zheng Wang, Hugh Leather Proceedings of the International Symposium on Code Generation and Optimization (CGO'17) 4 February 2017 Austin, Texas USA bibtex | abstract | paper(pdf) Predictive modeling using machine learning is an effective method for building compiler heuristics, but there is a short- age of benchmarks. Typical machine learning experiments outside of the compilation field train over thousands or mil- lions of examples. In machine learning for compilers, how- ever, there are typically only a few dozen common bench- marks available. This limits the quality of learned models, as they have very sparse training data for what are often high-dimensional feature spaces. What is needed is a way to generate an unbounded number of training programs that finely cover the feature space. At the same time the generated programs must be similar to the types of programs that human developers actually write, otherwise the learning will target the wrong parts of the feature space. We mine open source repositories for program fragments and apply deep learning techniques to automatically con- struct models for how humans write programs. We sample these models to generate an unbounded number of runnable training programs. The quality of the programs is such that even human developers struggle to distinguish our generated programs from hand-written code. We use our generator for OpenCL programs, CLgen, to automatically synthesize thousands of programs and show that learning over these improves the performance of a state of the art predictive model by 1.27. In addition, the fine covering of the feature space automatically exposes weaknesses in the feature design which are invisible with the sparse training examples from existing benchmark suites. Correcting these weaknesses further increases performance by 4.30. @inproceedings{ leather_dnnprogramgen_cgo17, author = {Chris Cummins and Pavlos Petoumenos and Zheng Wang and Hugh Leather}, day = {4}, location = {Austin, Texas USA}, month = {February}, year = {2017}, booktitle = {Proceedings of the International Symposium on Code Generation and Optimization (CGO'17)}, title = {Synthesizing Benchmarks for Predictive Modeling} } 5 Minimizing the cost of iterative compilation with active learning William Ogilvie, Pavlos Petoumenos, Zheng Wang, Hugh Leather Proceedings of the International Symposium on Code Generation and Optimization (CGO'17) 4 February 2017 Austin, Texas USA bibtex | abstract | paper(pdf) Since performance is not portable between platforms, engineers must fine-tune heuristics for each processor in turn. This is such a laborious task that high-profile compilers, supporting many architectures, cannot keep up with hardware innovation and are actually out-of-date. Iterative compilation driven by machine learning has been shown to be efficient at generating portable optimization models automatically. However, good quality models require costly, repetitive, and extensive training which greatly hinders the wide adoption of this powerful technique. In this work, we show that much of this cost is spent collecting training data, runtime measurements for different optimization decisions, which contribute little to the fi- nal heuristic. Current implementations evaluate randomly chosen, often redundant, training examples a pre-configured, almost always excessive, number of times a large source of wasted effort. Our approach optimizes not only the selection of training examples but also the number of samples per example, independently. To evaluate, we construct 11 high-quality models which use a combination of optimization settings to predict the runtime of benchmarks from the SPAPT suite. Our novel, broadly applicable, methodology is able to reduce the training overhead by up to 26x compared to an approach with a fixed number of sample runs, transforming what is potentially months of work into days. @inproceedings{ leather_minimiseitercomp_cgo17, author = {William Ogilvie and Pavlos Petoumenos and Zheng Wang and Hugh Leather}, day = {4}, location = {Austin, Texas USA}, month = {February}, year = {2017}, booktitle = {Proceedings of the International Symposium on Code Generation and Optimization (CGO'17)}, title = {Minimizing the cost of iterative compilation with active learning} } 6 ALEA: A Fine-grained Energy Profiling Tool Lev Mukhanov, Pavlos Petoumenos, Zheng Wang, Nikos Parasyris, Dimitrios Nikolopoulos, Bronis de Supinski, Hugh Leather Journal of ACM Transactions on Architecture and Code Optimization (TACO) bibtex @article{ leather_alea_taco17, author = {Lev Mukhanov and Pavlos Petoumenos and Zheng Wang and Nikos Parasyris and Dimitrios Nikolopoulos and Bronis de Supinski and Hugh Leather}, booktitle = {Journal of ACM Transactions on Architecture and Code Optimization (TACO)}, title = {ALEA: A Fine-grained Energy Profiling Tool} } 7 Predicting and Optimizing Image Compression Alexander Murashko, Hugh Leather, John Thomson Proceedings of the 24th ACM International Conference on Multimedia MM '16 15 October 2016 Amsterdam, The Netherlands bibtex | abstract | paper(pdf) Image compression is a core task for mobile devices, social media and cloud storage backend services. Key evaluation criteria for compression are: the quality of the output, the compression ratio achieved and the computational time (and energy) expended. Predicting the effectiveness of standard compressors like libjpeg and WebP on a novel image is challenging, and often leads to non-optimal compression. This paper presents a machine learning-based technique to accurately model the outcome of image compression for arbitrary new images in terms of quality and compression ratio, without requiring significant additional computational time and energy. Using this model, we can actively adapt the aggressiveness of compression on a per image basis to accurately fit user requirements, leading to a more optimal compression. @inproceedings{ leather_imagecompression_mm16, author = {Alexander Murashko and Hugh Leather and John Thomson}, publisher = {ACM}, month = {October}, year = {2016}, address = {New York, NY, USA}, location = {Amsterdam, The Netherlands}, day = {15}, series = {MM '16}, booktitle = {Proceedings of the 24th ACM International Conference on Multimedia}, title = {Predicting and Optimizing Image Compression} } 8 Parallel Computing: Accelerating Computational Science and Engineering (CSE) Hugh Leather, Mark Sawyer, Mark Parsons, Gerhard Joubert, Frans Peters Advances in Parallel Computing 25 April 2014 bibtex @proceedings{ leather_parco2016, editor = {Hugh Leather and Mark Sawyer and Mark Parsons and Gerhard Joubert and Frans Peters}, publisher = {IOS Press}, year = {2014}, volume = {25}, series = {Advances in Parallel Computing}, title = {Parallel Computing: Accelerating Computational Science and Engineering (CSE)}, month = {April} } 9 The Lambda Calculus: Practice and Principle Hugh Leather, Janne Irgens A List of Successes That Can Change the World: Essays Dedicated to Philip Wadler on the Occasion of His 60th Birthday 201--206 25 March 2016 bibtex | abstract | paper(pdf) The Lambda Calculus has perplexed students of computer science for millennia, rendering many incapable of understanding even the most basic precepts of functional programming. This paper gently introduces the core concepts to the lay reader, assuming only a minimum of background knowledge in category theory, quantum chromodynamics, and paleomagnetism. In addition, this paper goes on to its main results, showing how the Lambda Calculus can be used to easily prove the termination of Leibniz Hailstone numbers for all n > 0, to show that matrix multiplication is possible in linear time, and to guarantee Scottish independence. @inbook{ leather_lambdacalulus_wf16, author = {Hugh Leather and Janne Irgens}, url = {http://dx.doi.org/10.1007/978-3-319-30936-1_11}, month = {March}, editor = {Lindley, Sam and McBride, Conor and Trinder, Phil and Sannella, Don}, address = {Cham}, doi = {10.1007/978-3-319-30936-1_11}, publisher = {Springer International Publishing}, title = {The Lambda Calculus: Practice and Principle }, isbn = {978-3-319-30936-1}, pages = {201--206}, year = {2016}, day = {25}, booktitle = {A List of Successes That Can Change the World: Essays Dedicated to Philip Wadler on the Occasion of His 60th Birthday} } 10 Towards Collaborative Performance Tuning of Algorithmic Skeletons Christopher Cummins, Pavlos Petoumenos, Michel Stewer, Hugh Leather Proceedings of the International Workshop on High-Level Programming for Heterogeneous and Hierarchical Parallel Systems (HLPGPU 2016) January 2016 Prague, Czech Republic bibtex | abstract | paper(pdf) The physical limitations of microprocessor design have forced the industry towards increasingly heterogeneous designs to extract performance. This trend has not been matched with adequate software tools, leading to a growing disparity between the availability of parallelism and the ability for application developers to exploit it. Algorithmic skeletons simplify parallel programming by providing high-level, reusable patterns of computation. Achieving performant skeleton implementations is a difficult task; skeleton authors must attempt to anticipate and tune for a wide range of architectures and use cases. This results in implementations that target the general case and cannot provide the performance advantages that are gained from tuning low level optimization parameters. Autotuning combined with machine learning offers promising performance benefits in these situations, but the high cost of training and lack of available tools limits the practicality of autotuning for real world programming. We believe that performing autotuning at the level of the skeleton library can overcome these issues. In this work, we present OmniTune an extensible and distributed framework for dynamic autotuning of optimization parameters at runtime. OmniTune uses a client-server model with a flexible API to support machine learning enabled autotuning. Training data is shared across a network of cooperating systems, using a collective approach to performance tuning. We demonstrate the practicality of OmniTune in a case study using the algorithmic skeleton library SkelCL. By automatically tuning the workgroup size of OpenCL Stencil skeleton kernels, we show that that static tuning across a range of GPUs and programs can achieve only 26% of the optimal performance, while OmniTune achieves 92% of this maximum, equating to an average 5.65 speedup. OmniTune achieves this without introducing a significant runtime overhead, and enables portable, cross-device and cross-program tuning. @inproceedings{ leather_collaborativetuningskeletons_adapt2016, author = {Christopher Cummins and Pavlos Petoumenos and Michel Stewer and Hugh Leather}, title = {Towards Collaborative Performance Tuning of Algorithmic Skeletons}, location = {Prague, Czech Republic}, booktitle = {Proceedings of the International Workshop on High-Level Programming for Heterogeneous and Hierarchical Parallel Systems (HLPGPU 2016)}, month = {January}, year = {2016} } 11 Iterative Compilation on Mobile Devices Paschalis Mpeis, Pavlos Petoumenos, Hugh Leather Proceedings of the 6th International Workshop on Adaptive Self-tuning Computing Systems (ADAPT 2016) January 2016 Prague, Czech Republic bibtex | abstract | paper(pdf) The abundance of poorly optimized mobile applications coupled with their increasing centrality in our digital lives make a framework for mobile app optimization an imperative. While tuning strategies for desktop and server applications have a long history, it is difficult to adapt them for use on mobile devices. Reference inputs which trigger behavior similar to a mobile applications typical are hard to construct. For many classes of applications the very concept of typical behavior is nonexistent, each user interacting with the application in very different ways. In contexts like this, optimization strategies need to evaluate their effectiveness against real user input, but doing so online runs the risk of user dissatisfaction when suboptimal optimizations are evaluated. In this paper we present an iterative compiler which employs a novel capture and replay technique in order to collect real user test cases and use it later to evaluate different transformations offline. The proposed mechanism identifies and stores only the set of memory pages needed to replay the most heavily used functions of the application. At idle and charging periods, this minimal state is combined with different binaries of the application, each one build with different optimizations enabled. Replaying the targeted functions allows us to evaluate the effectiveness of each set of optimizations for the actual way the user interacts with the application. For the BEEBS benchmark suite, our approach was able to improve performance of hot functions by up to 57%, while keeping the slowdown experienced by the user on average at 0.8%. By focusing only on heavily used functions, we are able to conserve storage space by between two and three orders of magnitude compared to typical capture and replay implementations. @inproceedings{ leather_iterativecompilationmobile_adapt2016, author = {Paschalis Mpeis and Pavlos Petoumenos and Hugh Leather}, title = {Iterative Compilation on Mobile Devices}, location = {Prague, Czech Republic}, booktitle = {Proceedings of the 6th International Workshop on Adaptive Self-tuning Computing Systems (ADAPT 2016)}, month = {January}, year = {2016} } 12 Autotuning OpenCL Workgroup Size for Stencil Patterns Christopher Cummins, Pavlos Petoumenos, Michel Stewer, Hugh Leather Proceedings of the 6th International Workshop on Adaptive Self-tuning Computing Systems (ADAPT 2016) January 2016 Prague, Czech Republic bibtex | abstract | paper(pdf) Selecting an appropriate workgroup size is critical for the performance of OpenCL kernels, and requires knowledge of the underlying hardware, the data being operated on, and the implementation of the kernel. This makes portable performance of OpenCL programs a challenging goal, since simple heuristics and statically chosen values fail to exploit the available performance. To address this, we propose the use of machine learning enabled autotuning to automatically predict workgroup sizes for stencil patterns on CPUs and multi-GPUs. We present three methodologies for predicting workgroup sizes. The first, using classifiers to select the optimal workgroup size. The second and third proposed methodologies employ the novel use of regressors for performing classification by predicting the runtime of kernels and the relative performance of different workgroup sizes, respectively. We evaluate the effectiveness of each technique in an empirical study of 429 combinations of architecture, kernel, and dataset, comparing an average of 629 different workgroup sizes for each. We find that autotuning provides a median 3.79 speedup over the best possible fixed workgroup size, achieving 94% of the maximum performance. @inproceedings{ leather_autotuneopenclstencil_adapt2016, author = {Christopher Cummins and Pavlos Petoumenos and Michel Stewer and Hugh Leather}, title = {Autotuning OpenCL Workgroup Size for Stencil Patterns}, location = {Prague, Czech Republic}, booktitle = {Proceedings of the 6th International Workshop on Adaptive Self-tuning Computing Systems (ADAPT 2016)}, month = {January}, year = {2016} } 13 On the Inference of User Paths from Anonymized Mobility Data Galini Tsoukaneri, George Theodorakopoulos, Hugh Leather, Mahesh K. Marina Proceedings of the 1st IEEE European Symposium on Security and Privacy (EuroS&P 2016) March 2016 Saarbrcken, Germany bibtex | abstract | paper(pdf) Using the plethora of apps on smartphones and tablets entails giving them access to different types of privacy sensitive information, including the devices location. This can potentially compromise user privacy when app providers share user data with third parties (e.g., advertisers) for monetization purposes. In this paper, we focus on the interface for data sharing between app providers and third parties, and devise an attack that can break the strongest form of the commonly used anonymization method for protecting the privacy of users. More specifically, we develop a mechanism called Comber that given completely anonymized mobility data (without any pseudonyms) as input is able to identify different users and their respective paths in the data. Comber exploits the observation that the distribution of speeds is typically similar among different users and incorporates a generic, empirically derived histogram of user speeds to identify the users and disentangle their paths. Comber also benefits from two optimizations that allow it to reduce the path inference time for large datasets. We use two real datasets with mobile user location traces (Mobile Data Challenge and GeoLife) for evaluating the effectiveness of Comber and show that it can infer paths with greater than 90% accuracy with both these datasets. @inproceedings{ leather_deanonymise_eurosp2016, author = {Galini Tsoukaneri and George Theodorakopoulos and Hugh Leather and Mahesh K. Marina}, title = {On the Inference of User Paths from Anonymized Mobility Data}, location = {Saarbrcken, Germany}, booktitle = {Proceedings of the 1st IEEE European Symposium on Security and Privacy (EuroS&P 2016)}, month = {March}, year = {2016} } 14 Power Capping: What Works, What Does Not Pavlos Petoumenos, Lev Mukhanov, Zheng Wang, Hugh Leather, Dimitrios Nikolopoulos Proceedings of the 21st IEEE International Conference on Parallel and Distributed Systems (ICPADS), 2015 December 2015 Melbourne, Australia bibtex | abstract | paper(pdf) Peak power consumption is the first order design constraint of data centers. Though peak power consumption is rarely, if ever, observed, the entire data center facility must prepare for it, leading to inefficient usage of its resources. The most prominent way for addressing this issue is to limit the power consumption of the data center IT facility far below its theoretical peak value. Many approaches have been proposed to achieve that, based on the same small set of enforcement mechanisms, but there has been no corresponding work on systematically examining the advantages and disadvantages of each such mechanism. In the absence of such a study, it is unclear what is the optimal mechanism for a given computing environment, which can lead to unnecessarily poor performance if an inappropriate scheme is used. This paper fills this gap by comparing for the first time five widely used power capping mechanisms under the same hardware/software setting. We also explore possible alternative power capping mechanisms beyond what has been previously proposed and evaluate them under the same setup. We systematically analyze the strengths and weaknesses of each mechanism, in terms of energy efficiency, overhead, and predictable behavior. We show how these mechanisms can be combined in order to implement an optimal power capping mechanism which reduces the slowdown compared to the most widely used mechanism by up to 88%. Our results provide interesting insights regarding the different trade-offs of power capping techniques, which will be useful for designing and implementing highly efficient power capping in the future. @inproceedings{ leather_powercapping_icpads2015, author = {Pavlos Petoumenos and Lev Mukhanov and Zheng Wang and Hugh Leather and Dimitrios Nikolopoulos}, title = {Power Capping: What Works, What Does Not}, location = {Melbourne, Australia}, booktitle = {Proceedings of the 21st IEEE International Conference on Parallel and Distributed Systems (ICPADS), 2015}, month = {December}, year = {2015} } 15 Application of Domain-aware Binary Fuzzing to Aid Android Virtual Machine Testing Stephen Kyle, Hugh Leather, Bjorn Franke, Dave Butcher, Stuart Monteith Proceedings of the 2015 International Conference on Virtual Execution Environments (VEE'15) March 2015 Istanbul, Turkey bibtex | abstract | paper(pdf) The development of a new application virtual machine (VM), like the creation of any complex piece of software, is a bug-prone process. In version 5.0, the widely-used Android operating system has changed from the Dalvik VM to the newly-developed ART VM to execute Android applications. As new iterations of this VM are released, how can the developers aim to reduce the number of potentially security-threatening bugs that make it into the final product? In this paper we combine domain-aware binary fuzzing and differential testing to produce DEXFUZZ, a tool that exploits the presence of multiple modes of execution within a VM to test for defects. These modes of execution include the interpreter and a runtime that executes ahead-of-time compiled code. We find and present a number of bugs in the in-development version of ART in the Android Open Source Project. We also assess DEXFUZZs ability to highlight defects in the experimental version of ART released in the previous version of Android, 4.4, finding 189 crashing programs and 15 divergent programs that indicate defects after only 5,000 attempts. @inproceedings{ leather_dalvikfuzzing_vee2015, author = {Stephen Kyle and Hugh Leather and Bjorn Franke and Dave Butcher and Stuart Monteith}, title = {Application of Domain-aware Binary Fuzzing to Aid Android Virtual Machine Testing}, location = {Istanbul, Turkey}, booktitle = {Proceedings of the 2015 International Conference on Virtual Execution Environments (VEE'15)}, month = {March}, year = {2015} } 16 Intelligent Heuristic Construction with Active Learning William F Ogilvie, Pavlos Petoumenos, Zheng Wang, Hugh Leather Proceedings of Compilers for Parallel Computing (CPC) 2015 January 2015 London, England bibtex @inproceedings{ leather_activelearning_cpc2015, author = {William F Ogilvie and Pavlos Petoumenos and Zheng Wang and Hugh Leather}, location = {London, England}, booktitle = {Proceedings of Compilers for Parallel Computing (CPC) 2015}, title = {Intelligent Heuristic Construction with Active Learning}, month = {January}, year = {2015} } 17 Measuring QoE of Interactive Workloads and Characterising Frequency Governors on Mobile Devices. Best Paper Award Volker Seeker, Pavlos Petoumenos, Hugh Leather, Bjorn Franke IISWC '14: Proceedings of the 2014 IEEE International Symposium on Workload Characterization (BEST PAPER!) October 2014 Raleigh, North Carolina, USA bibtex | abstract | paper (pdf) | presentation (pptx) | video for presentation - sample workload | video for presentation - auto replay Mobile computing devices such as smartphones and tablets have become tightly integrated with many peoples life, both at work and at home. Users spend large amounts of time interacting with their mobile device and demand an excellent user experience in terms of responsiveness, whilst simultaneously expecting a long battery life between charging cycles. Frequency governors, responsible for increasing or decreasing the CPU clock frequency depending on the current workload and external events, try to balance the two contrasting goals of high performance and low energy consumption. However, despite their critical role in providing energy efficiency it is difficult to measure the effectiveness of frequency governors in an interactive environment. In this paper we develop a novel methodology for creating repeatable, fully automated, realistic, workloads that can accurately measure time lag in interactive applications resulting from non-optimally selected operating frequencies. We also introduce a new metric capturing the user experience for different ANDROID frequency governors. We evaluate interactive workloads to demonstrate how our approach enables us to automatically record and replay sequences of user interactions for different system configurations. We demonstrate that none of the available ANDROID frequency governors performs particularly well, but leave substantial room for improvement. We show that energy savings of up to 27% are possible, whilst delivering a user experience that is better than that provided by the standard ANDROID frequency governor. We also show that it is possible to save 47% energy with performance that is indistinguishable from permanently running the CPU at the highest frequency. @inproceedings{ leather_interactiveworkloads_iiswc2014, author = {Volker Seeker and Pavlos Petoumenos and Hugh Leather and Bjorn Franke}, title = {Measuring QoE of Interactive Workloads and Characterising Frequency Governors on Mobile Devices.}, location = {Raleigh, North Carolina, USA}, booktitle = {IISWC '14: Proceedings of the 2014 IEEE International Symposium on Workload Characterization (BEST PAPER!)}, month = {October}, year = {2014} } 18 Fast Automatic Heuristic Construction Using Active Learning William F. Ogilvie, Pavlos Petoumenos, Zheng Wang, Hugh Leather LCPC '14: Proceedings of the Workshop on Languages and Compilers for Parallel Computing 15 September 2014 Hillsboro, Oregan, USA bibtex | abstract | paper (pdf) Building effective optimization heuristics is a challenging task which often takes developers several months if not years to complete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data. However, obtaining this data can take months per platform. This is becoming an ever more critical problem and if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines. In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which significantly reduces this training overhead. Typically in supervised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry. This wastes effort on parts of the space that contribute little to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples. We demonstrate this technique by automatically constructing a model to determine on which device to execute four parallel programs at differing problem dimensions for a representative CpuGpu based heterogeneous system. Our methodology is remarkably simple and yet effective, making it a strong candidate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state-of-the-art. @inproceedings{ leather_activelearning_lcpc2014, author = {William F. Ogilvie and Pavlos Petoumenos and Zheng Wang and Hugh Leather}, title = {Fast Automatic Heuristic Construction Using Active Learning}, booktitle = {LCPC '14: Proceedings of the Workshop on Languages and Compilers for Parallel Computing}, location = {Hillsboro, Oregan, USA}, day = {15}, month = {September}, year = {2014} } 19 Automatic Feature Generation for Machine Learning--based Optimising Compilation Hugh Leather, Edwin Bonilla, Michael O'Boyle ACM Trans. Archit. Code Optim. 11 14:1--14:32 Feb 2014 bibtex | abstract | paper(acm) Recent work has shown that machine learning can automate and in some cases outperform handcrafted compiler optimisations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This article develops a novel mechanism to automatically find those features that most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modelling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCCs hard-coded heuristic achieves only 3% of the maximum performance available, whereas a state-of-the-art machine learning approach with hand-coded features obtains 59%. Our feature generation technique is able to achieve 76% of the maximum available speedup, outperforming existing approaches. @article{ leather_autofeatgen_taco14, author = {Hugh Leather and Edwin Bonilla and Michael O'Boyle}, keywords = {Feature generation, genetic programming, program optimisation}, acmid = {2536688}, volume = {11}, numpages = {32}, month = {Feb}, address = {New York, NY, USA}, doi = {10.1145/2536688}, issue_date = {February 2014}, title = {Automatic Feature Generation for Machine Learning--based Optimising Compilation}, articleno = {14}, number = {1}, publisher = {ACM}, url = {https://dl.acm.org/citation.cfm?id=2536688}, pages = {14:1--14:32}, journal = {ACM Trans. Archit. Code Optim.}, year = {2014}, issn = {1544-3566} } 20 Active learning accelerated automatic heuristic construction for parallel program mapping Proceedings of the 23rd international conference on Parallel architectures and compilation (PACT'14) 481--482 24 August 2014 Edmonton, Alberta, Canada bibtex @inproceedings{ leather_activeheur_pact14, organization = {ACM}, author = {table: 0x7fd79550ffa0}, booktitle = {Proceedings of the 23rd international conference on Parallel architectures and compilation (PACT'14)}, location = {Edmonton, Alberta, Canada}, pages = {481--482}, year = {2014}, day = {24}, month = {August}, title = {Active learning accelerated automatic heuristic construction for parallel program mapping} } 21 Auto-tuning Parallel Skeletons Alexander Collins, Christian Fensch, Hugh Leather Parallel Processing Letters (PPL) 22 1240005-1--16 June 2012 bibtex | abstract | paper (pdf) Parallel skeletons are a structured parallel programming abstraction that provide pro- grammers with a predefined set of algorithmic templates that can be combined, nested and parameterized with sequential code to produce complex programs. The implemen- tation of these skeletons is currently a manual process, requiring human expertise to choose suitable implementation parameters that provide good performance. This paper presents an empirical exploration of the optimization space of the FastFlow parallel skele- ton framework. We performed this using a Monte Carlo search of a random subset of the space, for a representative set of platforms and programs. The results show that the space is program and platform dependent, non-linear, and that automatic search achieves a significant average speedup in program execution time of 1:6x over a human expert. An exploratory data analysis of the results shows a linear dependence between two of the parameters, and that another two parameters have little effect on performance. These properties are then used to reduce the size of the space by a factor of 6, reducing the cost of the search. This provides a starting point for automatically optimizing parallel skeleton programs without the need for human expertise, and with a large improvement in execution time compared to that achievable using human expert tuning. @article{ leather_autotuneparallelskeletons_ppl12, author = {Alexander Collins and Christian Fensch and Hugh Leather}, title = {Auto-tuning Parallel Skeletons}, volume = {22}, month = {June}, number = {2}, pages = {1240005-1--16}, journal = {Parallel Processing Letters (PPL)}, year = {2012} } 22 Efficiently Parallelizing Instruction Set Simulation of Embedded Multi-Core Processors Using Region-based Just-in-Time Dynamic Binary Translation Stephen Kyle, Igor Bohm, Bjorn Franke, Hugh Leather, Nigel Topham LCTES '12: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on Languages, Compilers, and Tools for Embedded Systems 12 June 2012 Beijing, China bibtex | abstract | paper (pdf) Embedded systems, as typified by modern mobile phones, are already seeing a drive toward using multi-core processors. The number of cores will likely increase rapidly in the future. Engineers and researchers need to be able to simulate systems, as they are expected to be in a few generations time, running simulations of many-core devices on today's multi-core machines. These requirements place heavy demands on the scalability of simulation engines, the fastest of which have typically evolved from just-in-time (Jit) dynamic binary translators (Dbt). Existing work aimed at parallelizing Dbt simulators has focused exclusively on trace-based Dbt, wherein linear execution traces or perhaps trees thereof are the units of translation. Region-based Dbt simulators have not received the same attention and require different techniques than their trace-based cousins. In this paper we develop an innovative approach to scaling multi-core, embedded simulation through region-based Dbt. We initially modify the Jit code generator of such a simulator to emit code that does not depend on a particular thread with its thread-specific context and is, therefore, thread-agnostic. We then demonstrate that this thread-agnostic code generation is comparable to thread-specific code with respect to performance, but also enables the sharing of Jit-compiled regions between different threads. This sharing optimisation, in turn, leads to significant performance improvements for multi-threaded applications. In fact, our results confirm that an average of 76\% of all Jit-compiled regions can be shared between 128 threads in representative, parallel workloads. We demonstrate that this translates into an overall performance improvement by 1.44x on average and up to 2.40x across 12 multi-threaded benchmarks taken from the Splash-2 benchmark suite, targeting our high-performance multi-core Dbt simulator for embedded Arc processors running on a 4-core Intel host machine. @inproceedings{ leather_paralleljit_lctes12, author = {Stephen Kyle and Igor Bohm and Bjorn Franke and Hugh Leather and Nigel Topham}, title = {Efficiently Parallelizing Instruction Set Simulation of Embedded Multi-Core Processors Using Region-based Just-in-Time Dynamic Binary Translation}, booktitle = {LCTES '12: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on Languages, Compilers, and Tools for Embedded Systems}, location = {Beijing, China}, day = {12}, month = {June}, year = {2012} } 23 MaSiF: Machine Learning Guided Auto-tuning of Parallel Skeletons Alexander Collins, Christian Fensch, Hugh Leather Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT'12) PACT '12 437--438 2012 Minneapolis, Minnesota, USA bibtex @inproceedings{ leather_masif_pact12, author = {Alexander Collins and Christian Fensch and Hugh Leather}, publisher = {ACM}, location = {Minneapolis, Minnesota, USA}, pages = {437--438}, year = {2012}, series = {PACT '12}, booktitle = {Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT'12)}, title = {MaSiF: Machine Learning Guided Auto-tuning of Parallel Skeletons} } 24 Optimization Space Exploration of the FastFlow Parallel Skeleton Framework Alexander Collins, Christian Fensch, Hugh Leather HLPGPU '12: Proceedings of High-Level Programming for Heterogeneous and Hierarchical Parallel Systems 23 January 2012 Paris, France bibtex | abstract | paper (pdf) Parallel skeletons are a structured parallel programming abstrac- tion that provide programmers with a predefined set of algorithmic templates that can be combined, nested and parametrized with se- quential code to produce complex programs. The implementation of these skeletons is currently a manual process, requiring human expertise to choose suitable implementation parameters that pro- vide good performance. This paper presents an empirical explo- ration of the optimization space of the FastFlow parallel skeleton framework. We performed this using a Monte Carlo search of a ran- dom subset of the space, for a representative set of platforms and programs. The results show that the space is program and platform dependent, non-linear, and that automatic search achieves a signif- icant average speedup in program execution time of 1.6 over a human expert. An exploratory data analysis of the results shows a linear dependence between two of the parameters, and that another two parameters have little effect on performance. These properties are then used to reduce the size of the space by a factor of 6, re- ducing the cost of the search. This provides a starting point for au- tomatically optimizing parallel skeleton programs without the need for human expertise, and with a large improvement in execution time compared to that achievable using human expert tuning. @inproceedings{ leather_opt-expl-skeltons_hlpgpu12, author = {Alexander Collins and Christian Fensch and Hugh Leather}, title = {Optimization Space Exploration of the FastFlow Parallel Skeleton Framework}, booktitle = {HLPGPU '12: Proceedings of High-Level Programming for Heterogeneous and Hierarchical Parallel Systems}, location = {Paris, France}, day = {23}, month = {January}, year = {2012} } 25 Raced Profiles: Efficient Selection of Competing Compiler Optimizations Hugh Leather, Michael O'Boyle, Bruce Worton LCTES '09: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on Languages, Compilers, and Tools for Embedded Systems 25-28 June 2009 Dublin, Ireland bibtex | abstract | paper (pdf) | slides (open office) Many problems in embedded compilation require one set of optimizations to be selected over another based on run time performance. Self-tuned libraries, iterative compilation and machine learning techniques all compare multiple compiled program versions. In each, program versions are timed to determine which has the best performance. The program needs to be run multiple times for each version because there is noise inherent in most performance measurements. The number of runs must be enough to compare different versions, despite the noise, but executing more than this will waste time and energy. The compiler writer must either risk taking too few runs, potentially getting incorrect results, or taking too many runs increasing the time for their experiments or reducing the number of program versions evaluated. Prior works choose constant size sampling plans where each compiled version is executed a fixed number of times without regard to the level of noise. In this paper we develop a sequential sampling plan which can automatically adapt to the experiment so that the compiler writer can have both confidence in the results and also be sure that no more runs were taken than were needed. We show that our system is able to correctly determine the best optimization settings with between 76% and 87% fewer runs than needed by a brute force, constant sampling size approach.We also compare our approach to JavaSTATS(10); we needed 77% to 89% fewer runs than it needed. @inproceedings{ leather_racedprofiles_lctes09, author = {Hugh Leather and Michael O'Boyle and Bruce Worton}, title = {Raced Profiles: Efficient Selection of Competing Compiler Optimizations}, booktitle = {LCTES '09: Proceedings of the ACM SIGPLAN/SIGBED 2009 Conference on Languages, Compilers, and Tools for Embedded Systems}, location = {Dublin, Ireland}, day = {25-28}, month = {June}, year = {2009} } 26 Automatic Feature Generation for Machine Learning Based Optimizing Compilation Best Presentation Award Hugh Leather, Edwin Bonilla, Michael O'Boyle CGO '09: Proceedings of the International Symposium on Code Generation and Optimization March 2009 Seattle, United States of America bibtex | abstract | paper (pdf) | slides (open office) | slides no animation (pdf) Recent work has shown that machine learning can automate and in some cases outperform hand crafted compiler optimizations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This paper develops a novel mechanism to automatically find those features which most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modeling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCC's hard-coded heuristic achieves only 3% of the maximum performance available, while a state of the art machine learning approach with hand-coded features obtains 59%. Our feature generation technique is able to achieve 76% of the maximum available speedup, outperforming existing approaches. @inproceedings{ leather_autofeatgen_cgo09, author = {Hugh Leather and Edwin Bonilla and Michael O'Boyle}, title = {Automatic Feature Generation for Machine Learning Based Optimizing Compilation}, location = {Seattle, United States of America}, booktitle = {CGO '09: Proceedings of the International Symposium on Code Generation and Optimization}, month = {March}, year = {2009} } 27 MILEPOST GCC: machine learning based research compiler Grigori Fursin, Cupertino Miranda, Olivier Temam, Mircea Namolaru, Elad Yom-Tov, Ayal Zaks, Bilha Mendelson, Phil Barnard, Elton Ashton, Eric Courtois, Francois Bodin, Edwin Bonilla, John Thomson, Hugh Leather, Chris Williams, Michael O'Boyle Proceedings of the GCC Developers' Summit June 2008 Ottawa, Canada bibtex | abstract | paper (pdf) Tuning hardwired compiler optimizations for rapidly evolving hardware makes porting an optimizing compiler for each new platform extremely challenging. Our radical approach is to develop a modular, extensible, self-optimizing compiler that automatically learns the best optimization heuristics based on the behavior of the platform. In this paper we describe MILEPOST GCC, a machine-learning-based compiler that automatically adjusts its optimization heuristics to improve the execution time, code size, or compilation time of specific programs on different architectures. Our preliminary experimental results show that it is possible to considerably reduce execution time of the MiBench benchmark suite on a range of platforms entirely automatically. @inproceedings{ leather_milepostgcc_gccsum08, author = {Grigori Fursin and Cupertino Miranda and Olivier Temam and Mircea Namolaru and Elad Yom-Tov and Ayal Zaks and Bilha Mendelson and Phil Barnard and Elton Ashton and Eric Courtois and Francois Bodin and Edwin Bonilla and John Thomson and Hugh Leather and Chris Williams and Michael O'Boyle}, title = {MILEPOST GCC: machine learning based research compiler}, location = {Ottawa, Canada}, month = {June}, year = {2008}, booktitle = {Proceedings of the GCC Developers' Summit} } 28 Automatic Feature Generation for Setting Compilers Heuristics Hugh Leather, Elad Yom-Tov, Mircea Namolaru, Ari Freund SMART'08: 2nd Workshop on Statistical and Machine learning approaches to ARchitectures and compilaTion January 2008 Gteborg, Sweden bibtex | abstract | paper (pdf) | slides (open office) Heuristics in compilers are often designed by manually analyzing sample programs. Recent advances have successfully applied machine learning to automatically generate heuristics. The typical format of these approaches reduces the input loops, functions or programs to a finite vector of features. A machine learning algorithm then learns a mapping from these features to the desired heuristic parameters. Choosing the right features is important and requires expert knowledge since no machine learning tool will work well with poorly chosen features. This paper introduces a novel mechanism to generate features. Grammars describing languages of features are defined and from these grammars sentences are randomly produced. The features are then evaluated over input data and computed values are given to machine learning tools. We propose the construction of domain specific feature languages for different purposes in different parts of the compiler. Using these feature languages, complex, machine generated features are extracted from program code. Using our observation that some functions can benefit from setting different compiler options, while others cannot, we demonstrate the use of a decision tree classifier to automatically identify the former using the automatically generated features. We show that our method outperform human generated features on problems of loop unrolling and phase ordering, achieving a statistically significant decrease in run-time compared to programs compiled using GCCs heuristics. @inproceedings{ leather_autofeatgen_smart08, author = {Hugh Leather and Elad Yom-Tov and Mircea Namolaru and Ari Freund}, title = {Automatic Feature Generation for Setting Compilers Heuristics}, location = {Gteborg, Sweden}, booktitle = {SMART'08: 2nd Workshop on Statistical and Machine learning approaches to ARchitectures and compilaTion}, month = {January}, year = {2008} } 29 Emergency Evacuation using Wireless Sensor Networks Barnes M, Leather H, Arvind D K Proceedings of the 32nd IEEE Conference on Local Computer Networks (LCN 2007) - Volume 00 October 2007 Dublin, Ireland bibtex | abstract | paper (pdf) This paper presents a distributed algorithm to direct evacuees to exits through arbitrarily complex building layouts in emergency situations. The algorithm finds the safest paths for evacuees taking into account predictions of the relative movements of hazards, such as fires, and evacuees. The algorithm is demonstrated on a 64 node wireless sensor network test platform and in simulation. The results of simulations are shown to demonstrate the navigation paths found by the algorithm. @inproceedings{ leather_evac_lcn07, author = {Barnes M and Leather H and Arvind D K}, title = {Emergency Evacuation using Wireless Sensor Networks}, location = {Dublin, Ireland}, month = {October}, year = {2007}, booktitle = {Proceedings of the 32nd IEEE Conference on Local Computer Networks (LCN 2007) - Volume 00} } Theses 1 Machine Learning in Compilers Hugh Leather School of Informatics, University of Edinburgh 2010 bibtex | thesis (pdf) @phdthesis{ leather_phdthesis_10, author = {Hugh Leather}, school = {School of Informatics, University of Edinburgh}, year = {2010}, title = {Machine Learning in Compilers} } Invited Talks 1 Program Generation with Deep Learning Seoul National University 18 September 2016 Seoul, South Korea 2 Reducing the High Cost of Low Latency ARM Ltd 1 February 2016 Manchester, England 3 Energy Accounting for Program Optimisation Lancaster University 28 January 2015 Lancaster, England 4 Measuring QoE of Interactive Workloads Intel Corporation 15 September 2014 Hillsboro, Oregan, USA 5 Fast Automatic Heuristic Construction Iiswc 12 October 2014 Raleigh, North Carolina, USA 6 Active learning accelerated automatic heuristic construction Pact 24 August 2014 Edmonton, Alberta, Canada 7 Automatic Feature Generation for Machine Learning Based Optimizing Compilation Compusoc 26 February 2014 Edinburgh, Scotland 8 Machine Learning in Compilers Peking University 23 November 2013 Beijing, China 9 Mobile System Optimisation Qualcomm Ltd 18 August 2013 San Jose, California, USA 10 Operating System Optimisation Freescale Ltd 24 March 2013 East Kilbride, Scotland 11 Machine Learning in Compilers Beihang University 13 March 2013 Beijing, China 12 Accelerating Programs with Iterative Compilation Baidu Ltd 12 March 2013 Beijing, China 13 Optimising Compilers with Machine Learning Hong Kong University of Science and Technology (HKUST) 07 March 2013 Hong Kong, China 14 Mobile Device Optimisation with Machine Learning SCONE (SCOttish Networking Event) 08 February 2013 Edinburgh Scotland 15 Machine Learning Compilers and Mobile Systems University of St Andrews 11 April 201 St Andrews, Scotland 16 Optimising the Mobile Net ARM Ltd 17 September 2012 Cambridge, England 17 How to Give a Research Presentation After the previous, good reception of my talk at the 2011 SICSA conference, I have been asked to reprise the rle at this year's conference, providing advice on how to present research at conferences. SICSA Conference 2012 21 June 2012 Glasgow, Scotland slides (open office) | slides (pdf) 18 Machine Learning, Compilers and Mobile I will present my research and give an overview of the state-of-the-art in this field. Department of Computer Science 15 May 2012 St.Andrews, Scotland slides (open office) | slides (pdf) 19 Automatic Feature Generation for Machine Learning in Compilers I have been invited to present my PhD research. Department of Computer Science 29 February 2012 Manchester, Scotland slides (open office) | slides (pdf) 20 Optimising the Mobile Net Samsung 17 May 2011 Staines, England 21 How to Give a Research Presentation I was asked to give a talk about how to do conference presentations. PhD Induction 2011 21 October 2011 Firbush, Scotland slides (open office) | slides (pdf) 22 Collaborative Machine Learning Compiler and OS Optimisation for Android Devices ARM Ltd 31 May 2011 Cambridge, England 23 How to Give a Research Presentation I was asked to give a talk about how to do conference presentations. The seminar was the highest rated talk at the conference and, as a result, I have been asked to repeat the talk at this year's conference. SICSA Conference 2011 24 May 2011 Edinburgh, Scotland slides (open office) | slides (pdf) 24 Machine Learning in Compilers for Parallelisation Presented to Keshav Pingali's group which produces the Galois parallel system. ACES, University of Texas 27 May 2009 Austin, Texas, USA slides (open office) 25 Optimising Compilation with Machine Learning Keith Cooper asked me to present a lecture to his group, explaining the state of the art in machine learning for compilers. Rice University 26 May 2009 Houston, Texas, USA slides (open office) 26 Machine Learning in Compilers Google invited me to present an overview of machine learning in compilers. I was asked to show the cutting edge and give particular emphasis to the developments at Edinburgh. Google Technical Talks 21 May 2009 Mountain View, California, USA slides (open office) 27 Efficient Selection of Competing Compiler Optimizations Trinity College Dublin 19 June 2009 Dublin, Ireland 28 Automatic Feature Generation for Setting Compiler Heuristics University of Gothenburg 27 January 2008 Goteborg, Sweden 29 Learning Compilers for Configurable Processors I was invited by the European Commission, DG INFSO of Embedded Systems, to present our research at a conference aimed at cementing collaboration opportunities between the EU and Korea. I was asked because the EU felt that ours was an example of outstanding work. Korea-EU Cooperation Forum on ICT 16-17 June 2008 Seoul, S. Korea slides (pdf) 30 Machine Learning for Configurable Processors I gave a talk demonstrating how machine learning could be used to improve the company's compilers which targeted configurable processors. The talk explained the basics of ML techniques with particular reference to work that combined architectural and code features. ARC International 2008 St. Albans, UK 31 Extensibility for GCC I was asked to give a presentation on my work making GCC extensible. Making the compiler extensible means allowing heuristics to be driven from the outside and is essential for performing machine learning tasks without hacking the compiler. INRIA 2007 Paris, France 32 libPlugin: a Plugin Library for GCC I presented my tool chain, libPlugin, which provides an elegant plug-in framework for the GCC compiler. This presentation was given to members of the compiler team in IBM research. IBM Research 2007 Haifa, Israel 33 Scripting Compilers This company was beginning to look at machine learning experiments in GCC. During a week long visit to help solve this problem, I also gave a talk on how scripting languages can enable iterative compilation by making the compiler extensible. As a result of this the company began using my ECMA Scripting extensions for GCC. CAPS Enterprise 2006 Rennes, France Full BibTex Contact 1001 HUGH LEATHER School of Informatics Room 1.18a, Informatics Forum 10 Crichton Street Edinburgh EH8 9AB United Kingdom +44 (0)131 650 2707 hleather at inf dot ed dot ac dot uk This website has been designed by my very good friend, Ed Brooke. If you like a website built for you, he's your guy! He can be found at www.oddsok.com . Thanks so much for the Website, Ed! Copyright 2016. All rights reserved. Design by oddsok .
